<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yang Pei</title>
    <description>The effort a coding monkey paid when he managed to become a coding machine.</description>
    <link>https://pyemma.github.io</link>
    <atom:link href="https://pyemma.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Reading Notes 08/19</title>
        <description>&lt;h4 id=&quot;model-interpreting&quot;&gt;Model Interpreting&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning&quot;&gt;Ideas on interpreting machine learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Residual analysis can help understand which part of the data model is doing wrongly and thus find solution to improve it. However it’s straight forward to apply this method if the problem is a pure regression problem, which is actually not a common practice in industry. How to do this with for logistic regression? (devariance?)&lt;/li&gt;
  &lt;li&gt;A bunch of other method: surrogate model (train a simpler model on the output of the complexity model, which is similar to distill info of a neural network into a BDT), &lt;a href=&quot;https://arxiv.org/pdf/1602.04938v1.pdf&quot;&gt;LIME&lt;/a&gt;, maximum activation analysis (pair with LIME is better), variance importance (different way to compute the importance, pervade in tree model), sensitivity analysis, leave one covariance out, tree interpreter.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 19 Aug 2018 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Reading-Notes/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Reading-Notes/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 5</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://medium.com/intuitionmachine/google-and-ubers-best-practices-for-deep-learning-58488a8899b6&quot;&gt;Google and Uber’s Best Practices for Deep Learning&lt;/a&gt;
A good post introducing how &lt;strong&gt;Uber&lt;/strong&gt; and &lt;strong&gt;Google&lt;/strong&gt; apply deep learning in real world (not in academic). The post illustrates some highlight of the two machine learning platform build by these two giants: &lt;a href=&quot;https://eng.uber.com/michelangelo/&quot;&gt;Michelangelo&lt;/a&gt; and &lt;a href=&quot;http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform&quot;&gt;TFX&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For these two platform, there share some common idea: &lt;em&gt;enforce the share of knowledge across the company&lt;/em&gt;. For example, the all provide feature store, which is shared across the company and every team can reuse the feature easily. Also, the meta-data about model is also stored so that everyone can learn how model is trained, how’s the performance and might apply similar model design to their own problem.&lt;/p&gt;

&lt;p&gt;The TFX is relatively more complex than Michelangelo (because Google is larger in scale right :) ). Some unique traits included in TFX that impresses me a lot:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TFX emphasize a lot on data management, it provides a mechanism to monitor the distribution and statistic of the data. This will help engineer understand the data as well as detecting some abnormality in the data.&lt;/li&gt;
  &lt;li&gt;TFX also provide transfer-learning by making warm-up from model trained on common features pretty easy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/programmers_guide/eager&quot;&gt;TensforFlow Eager Execution&lt;/a&gt;
Helpful for model debugging.&lt;/p&gt;
</description>
        <pubDate>Sat, 31 Mar 2018 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//What-I-Read-This-Week-5/</link>
        <guid isPermaLink="true">https://pyemma.github.io//What-I-Read-This-Week-5/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 4</title>
        <description>&lt;h3 id=&quot;a-visual-proof-that-neural-nets-can-compute-any-function&quot;&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html&quot;&gt;A visual proof that neural nets can compute any function&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A really good writing on helping understand why neural network can approximate any functions, without too much illustrate on complex mathematic theory.&lt;/p&gt;

&lt;p&gt;The main idea the author used is that, for any activation function used in neuron, we can make it has a pretty large weight and bias and it would be like a function that, before some value it would be 0 and after it would be 1. We can have two neurons, manipulate the weight and bias to get a step function. The range of this step function could be pretty small, together with the idea of calculus, we can use millions of such step function to approximate almost all functions. Here is an example from the original blog to show this idea. The parameter \(s \) is get by \( s = -b/w \), and \( h \) is a parameter used to control the hight of the step function. For example, the neuron \(s = 0.4 \) and \( s = 0.6 \) is a pair, the \( h \) for the first one is \( -1.2 \) and for the second one is \( 1.2 \). Thus, they worked together, we get a step function that starts at \( 0.4 \), jump to \( -1.2 \), and back at \( 0.6 \).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/nn_approx.png&quot; alt=&quot;Neural Network Approximate Any Function&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;understanding-capsule-networksais-alluring-new-architecture&quot;&gt;&lt;a href=&quot;https://medium.freecodecamp.org/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc&quot;&gt;Understanding Capsule Networks — AI’s Alluring New Architecture&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Another very good tutorial blog on capsule neural network, the author also provide a visualization tool that we can play with.&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Mar 2018 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//What-I-Read-This-Week-4/</link>
        <guid isPermaLink="true">https://pyemma.github.io//What-I-Read-This-Week-4/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 3</title>
        <description>&lt;h3 id=&quot;explicit-vs-implicit-recommenders&quot;&gt;&lt;a href=&quot;https://medium.com/the-graph/insights-from-an-evening-with-recommender-systems-experts-ab44d677dc5e&quot;&gt;Explicit vs. Implicit Recommenders&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Never use RMSE (or other metrics only take explicit feedback (e.g. rating)) as measurement for your recommendation system&lt;/li&gt;
  &lt;li&gt;Implicit feedback (e.g. click/view) is far more valuable than explicit one&lt;/li&gt;
  &lt;li&gt;Netflix is not using &lt;strong&gt;star&lt;/strong&gt; anymore as an effort to remove explicit feedback&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a pretty interesting argument and I would like to put a question mark on it. Plan to do some experiment to justify this argument.&lt;/p&gt;

&lt;h3 id=&quot;introduction-to-learning-to-trade-with-reinforcement-learning&quot;&gt;&lt;a href=&quot;http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/&quot;&gt;Introduction to Learning to Trade with Reinforcement Learning&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A good post on introducing using reinforcement learning techniques to do trading. The author opens with some background and basic concept in trading, then comes with the limitation of using supervised learning method and current approach. Explained how we can model the trading into reinforcement learning, what kinds of environment, state, actions, rewards, etc. And then stated the beneficial to use reinforcement learning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It’s pretty hard to make money by predicting the price only through supervised learning. Even though we could accurately predict the price next time, we still need to face the problem of &lt;strong&gt;liquidity available&lt;/strong&gt;, &lt;strong&gt;network latency&lt;/strong&gt; and &lt;strong&gt;fees&lt;/strong&gt;. To make money by supervised learning, we need to accurate predict the large volume of price change or a long time period, or smartly manage our orders and fee, which all is very challenge.&lt;/li&gt;
  &lt;li&gt;Another problem with supervised learning is that its not indicate a policy we should execute. You have to come up with a rule based policy.&lt;/li&gt;
  &lt;li&gt;For the reinforcement learning based strategy, we have several beneficial points:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Learned Policy&lt;/em&gt;: we won’t do a job like “first have a model to predict price and then compose a rule based policy”&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Trained in a simulated environment&lt;/em&gt;: We can add all the factors that would affect our model into the environment, and let the model learn to response to these factors (e.g. network latency)&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Learning to adopt to market conditions&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Ability to model other agents&lt;/em&gt;, this stems from the fact that we can incorporate other agent into our environment and let our agent to game with them&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;titanic-data-science-solutions&quot;&gt;&lt;a href=&quot;https://www.kaggle.com/startupsci/titanic-data-science-solutions&quot;&gt;Titanic Data Science Solutions&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A good notebook introduce a basic workflow of how to solve a competition on kaggle. The most valuable part in my opinion is how to use pandas to do lots of data investigation and feature visualization.&lt;/p&gt;

&lt;h3 id=&quot;exploratory-data-analysis-with-pandas&quot;&gt;&lt;a href=&quot;https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68&quot;&gt;Exploratory data analysis with Pandas&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A good tutorial on explaining how to use pandas to do data analysis. The assignment is well-written and pretty helpful for mastering all kinds of functions in pandas.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf&quot;&gt;Pandas Cheat Sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 25 Feb 2018 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//What-I-Read-This-Week-3/</link>
        <guid isPermaLink="true">https://pyemma.github.io//What-I-Read-This-Week-3/</guid>
      </item>
    
      <item>
        <title>DQN In Practice</title>
        <description>&lt;p&gt;Recently I have been working on Deep-Q-Learning and apply it to some interesting AI games. In this post, I would like to give a brief introduction to how I implemented the Deep-Q-Learning, as well as lots of learning along the way.&lt;/p&gt;

&lt;h3 id=&quot;what-is-dqn&quot;&gt;What is DQN&lt;/h3&gt;
&lt;p&gt;To understand DQN, we need first know is prototype, Q-Leanring. Here is a pervious post about &lt;a href=&quot;https://pyemma.github.io/Reinforcement-Learning-Lesson-4/&quot;&gt;Q-Learning&lt;/a&gt;. Some core elements are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have a \( Q(s, a) \) to record for each state and action pair, what is the expected reward we can get from them&lt;/li&gt;
  &lt;li&gt;We update this estimation by finding what is the &lt;strong&gt;max&lt;/strong&gt; reward we can get from the next state leaded by our current state and action, update it by &lt;script type=&quot;math/tex&quot;&gt;Q(S, A) = Q(S, A) + \alpha(R + \gamma max_a Q(S^\prime, a) - Q(S, A))&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we have limited number of state and action, we can hold these information into a simple lookup table. However, in reality we usually deal with unlimited number of state and action. In this case, a lookup table is not scalable, we use a model to simulate this part: describe the state with some features, tell the model and the model will tell us what \( Q(s, a) \) would be, the model would be trained and updated along the way with the examples we have.&lt;/p&gt;

&lt;p&gt;Deep-Q-Leanring basically is a combination of the above two ideas. Apply the logic of Q-Learning, with a model measuring the \( Q(s, a) \). Here the &lt;em&gt;Deep&lt;/em&gt; comes from the fact that we usually use &lt;em&gt;Deep-Neural-Network&lt;/em&gt; as our model. However, there is another two important thing to stabilize the training of DQN:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Experience Replay&lt;/strong&gt;: Instead of directly using the most recent example, we keep a pool of past experience and sample a batch from this pool to update our model&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q-Target Network&lt;/strong&gt;: Instead of the max value output by our current model, we use the version of several steps ago. This is called the Q-Target model and this model will be frozen and not updated, but occasionally copied from our main model.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dqn-implementation&quot;&gt;DQN Implementation&lt;/h3&gt;
&lt;p&gt;Cool, as we have some highlight idea on what DQN is, let’s see how it is implemented. The code is &lt;a href=&quot;https://github.com/pyemma/tensorflow/blob/master/util/dqn.py&quot;&gt;here&lt;/a&gt;. Please not that this code is currently not generalized yet and only suitable for training &lt;em&gt;Cartpole&lt;/em&gt; game due to how we parsing the state. Making it generalized is WIP. However, that does not prevent us from understanding the main idea of DQN. Let me now illustrate some important component:&lt;/p&gt;

&lt;p&gt;Let’s first take a look at the main training logic:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsiode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Train the model
    Args:
        epsiode:        Number of epsiode to train
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_start&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsiode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_remember&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_to_copy_graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_copy_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_decay&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For each episode, we first initialize the state, and before the game terminate, we take a action based on our model and policy, then get the reward and next state for that action. We then put this as an experience into our memory pool. After the game is terminated, we update our model, and check if we should update q-target network. We also decrease the epsilon as we play. Here we are using \( \epsilon \)-greedy policy, and this parameter is the tradeoff between explore and exploit.&lt;/p&gt;

&lt;p&gt;Now lets take a look at how we train the model:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Use Experience Replay and Target Value Network to learn
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;q_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here, we sample a batch of experience from our memory pool. Then prepare it into the right format. Our goal is to train our model’s prediction (in this case, the prediction is the value of each action) is the same as the actual reward + q-target. In the code, we first get the model prediction for all actions. We also get q-target prediction for each action. We then update the value for the action we take to the target value. Then we train our model using this updated value. Since we only updated the value of action we took, the model will only learn from these updated value, all other is the same as before and model would not learn from them.&lt;/p&gt;

&lt;h3 id=&quot;dqn-in-practice&quot;&gt;DQN In Practice&lt;/h3&gt;
&lt;p&gt;During the implementation of this feature, I encountered lots of problem and would like to notice them down for further discussion:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Initially I updated the model &lt;strong&gt;after we take each action&lt;/strong&gt; instead of &lt;strong&gt;after each game&lt;/strong&gt;. This will dramatically increase the number of training we have and impact on the training time. However, getting more number of training is not always a good thing. I noticed that in my case, the training would be not stable.&lt;/li&gt;
  &lt;li&gt;Parameter tuning is really challenging. I tried different combination of batch size, memory pool size, learning rate, and model arch. I found that usually have a moderate memory pool size with a larger learning rate is beneficial.&lt;/li&gt;
  &lt;li&gt;The step to copy the q-target network is also hard to set. If we set is too small, then the training is less stabile; if too large, the training does not get improved.&lt;/li&gt;
  &lt;li&gt;I feel like the usage of the memory is not good enough, as there is not difference in terms of success experience and failure experience. From our common sense, we know that we learn more from our bad experience, maybe we should skew more onto the bad experience?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//DQN-In-Practice/</link>
        <guid isPermaLink="true">https://pyemma.github.io//DQN-In-Practice/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 2</title>
        <description>&lt;h3 id=&quot;evolving-search-recommendations-on-pinterest&quot;&gt;&lt;a href=&quot;https://medium.com/@Pinterest_Engineering/evolving-search-recommendations-on-pinterest-136e26e0468a&quot;&gt;Evolving search recommendations on Pinterest&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A post introducing the search work done in Pinterest.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initially they use a &lt;em&gt;Term-Query graph&lt;/em&gt; to generate candidates. In this graph, each term (a single word) is represent a node, as well as the query. Each term node is connected to the query, weighted by the reciprocal of the number of queries that term shows up in. Each query node is also connected to query node, weighted by the relativeness. Most visited queries are recommended.&lt;/li&gt;
  &lt;li&gt;They later changed to &lt;em&gt;Pixie&lt;/em&gt;, a graph based recommendation platform. The graph is build using query and pins. Compared with pervious solution, this solution will not break the query and thus keep the semantic information. &lt;strong&gt;To give better recommendation, semantic information is important.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;They have further work to utilize embeddings for queries. Is embedding based candidate generation works better than random walk based candidate generation?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;an-overview-of-multi-task-learning-in-deep-neural-networks&quot;&gt;&lt;a href=&quot;http://ruder.io/multi-task/index.html#introduction&quot;&gt;An Overview of Multi-Task Learning in Deep Neural Networks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A pretty good blog introducing what MTL is, who is works and some recent works. Here are some points I think most beneficial:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Using one sentence to explain MTL: “By sharing representation among related tasks(leveraging different domain-specific knowledge), the generalization of model is improved.”&lt;/li&gt;
  &lt;li&gt;Why MTL work:
    &lt;ul&gt;
      &lt;li&gt;Our training data will always contains some noise. Training a single goal on the data will easily get overfit. However, training on multiple tasks simultaneously will help cancel out this noise (&lt;strong&gt;Data Augmentation&lt;/strong&gt;).&lt;/li&gt;
      &lt;li&gt;By training on multiple tasks and sharing the same representation at the same time, the model will try to find a more general hypothesis that would work for all tasks (&lt;strong&gt;Regularization&lt;/strong&gt;).&lt;/li&gt;
      &lt;li&gt;Some feature combination might be pretty complex in one task and hard to let the model to capture, but easy in the other one. MTL will help sharing this info between tasks(&lt;strong&gt;Feature Engineering&lt;/strong&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are mainly two form of MTL:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Hard Parameter Sharing&lt;/strong&gt;: Different tasks will have several same layers at the lower level, and have their own layers at higher level. A common use-case is that, when can use the bottom layers in VGG, and then train our own layer on our task.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Soft Parameter Sharing&lt;/strong&gt;: Different tasks will have their own model, but each model can constraint each other to not differ too much.&lt;/li&gt;
      &lt;li&gt;Currently, &lt;strong&gt;Hard Parameter Sharing&lt;/strong&gt; is still very popular, but &lt;strong&gt;Soft Parameter Sharing&lt;/strong&gt; is more promising as it let the model to learn what to share.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;welcoming-the-era-of-deep-neuroevolution&quot;&gt;&lt;a href=&quot;https://eng.uber.com/deep-neuroevolution/&quot;&gt;Welcoming the Era of Deep Neuroevolution&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Uber AI Lab’s work on using Genetic Algorithm instead of SGD to optimize DNN on reinforcement learning tasks.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GA can produce comparatively similar result as SGD.&lt;/li&gt;
  &lt;li&gt;They purposed a method to smartly guide the mutation to put more attention on the sensitive feature, to solve the problem GA has when dealing with large networks.&lt;/li&gt;
  &lt;li&gt;They also purposed a method to enforce the exploration, which they try to have a population of candidates that act differently from each other as much as possible (unlikely to trapped in local minima).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;effective-modern-c&quot;&gt;[Effective Modern C++]&lt;/h3&gt;
&lt;p&gt;Mainly read the smart pointer part.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;unique_ptr&lt;/code&gt; performs similar to the old fashion raw pointer. It indicates a exclusive ownership to the object it manages, thus it can only be moved not be copied. We can specify a custom deleter to a unique pointer, and the deleter would become part of the unique pointer’s type. It is very convenient to covert a unique pointer to a shared pointer.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt; performs similar to the garbage collection in Java. It indicates a shared ownership to the object. The underlay mechanism is that each shared pointer will create a control block that would keep the reference count and other data. Since there is a separate object holding all extra info, the deleter we passed to shared pointer will not become a part of its type. Remember not using the raw pointer to initialize a shared pointer, as it is pretty dangerous and we might result in free the object multiple times. This is extremely the case when we are working with the &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; pointer. To be able to safely create a shared pointer from &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;enable_shared_from_this&lt;/code&gt; template.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;enable_shared_from_this&lt;/code&gt; uses &lt;em&gt;Curiously Recurring Template Pattern (CRTP)&lt;/em&gt; (to be add more details later).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;weak_ptr&lt;/code&gt; is like &lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;, but it does not effect the reference count on the object, and the object it is pointing to might be destroyed. The use case for &lt;code class=&quot;highlighter-rouge&quot;&gt;weak_ptr&lt;/code&gt; can be &lt;strong&gt;cacheing&lt;/strong&gt;, &lt;strong&gt;observer lists&lt;/strong&gt; and the prevention of &lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt; cycles.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 20 Jan 2018 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//What-I-Read-This-Week-2/</link>
        <guid isPermaLink="true">https://pyemma.github.io//What-I-Read-This-Week-2/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 1</title>
        <description>&lt;h3 id=&quot;the-3-tricks-that-made-alphago-zero-work&quot;&gt;&lt;a href=&quot;https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef&quot;&gt;The 3 Tricks That Made AlphaGo Zero Work&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using &lt;strong&gt;better training sample&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;Use a single neural network to predict which &lt;strong&gt;move&lt;/strong&gt; to recommend &lt;em&gt;and&lt;/em&gt; which &lt;strong&gt;move&lt;/strong&gt; are likely to win the game (This is the idea of using &lt;a href=&quot;http://ruder.io/multi-task/index.html#introduction&quot;&gt;&lt;strong&gt;Multitask Learning&lt;/strong&gt;&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Use a upgrade version of neural network (from convolutional neural network to &lt;strong&gt;residual neural network&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intuitive-rl-intro-to-advantage-actor-critic-a2c&quot;&gt;&lt;a href=&quot;https://medium.com/@rudygilman/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752&quot;&gt;Intuitive RL: Intro to Advantage-Actor-Critic (A2C)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very vivid introduction of &lt;strong&gt;Advantage-Actor-Critic&lt;/strong&gt; reinforcement learning.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;First we should keep in mind that &lt;strong&gt;Actor-Critic&lt;/strong&gt; is a blend of both value estimation and policy estimation reinforcement learning method, a.k.a we will try to learn value function, as well as policy from game play (This is different from pure value function based method and policy based method).&lt;/li&gt;
  &lt;li&gt;In &lt;strong&gt;Actor-Critic&lt;/strong&gt;, the &lt;strong&gt;Actor&lt;/strong&gt; will tries to optimize the parameter for policy and &lt;strong&gt;Critic&lt;/strong&gt; will tries to optimize the parameter for the value function of a state. This can be done by having a single model outputting both the value of the state, as will as the probability of action.&lt;/li&gt;
  &lt;li&gt;By jump into one state, taking action and get reward. We will get the training examples for our &lt;strong&gt;Critic&lt;/strong&gt;. The estimate for each state will become more and more accurate. In this way, we don’t need to wait until the end of the game to get the value of each state, which is high in variance.&lt;/li&gt;
  &lt;li&gt;In stead of simply policy gradient update the policy (which tries to avoid the action that lead to a state with low value), we use &lt;strong&gt;Advantage&lt;/strong&gt;, which is the relative improvement of the action take (e.g. current state is -100, and take action A we arrive in a state with -20, the improvement of the action is 80!). The idea behind this is that the action might be the result that result in a low value.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ai-and-deep-learning-in-2017--a-year-in-review&quot;&gt;&lt;a href=&quot;http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/&quot;&gt;AI and Deep Learning in 2017 – A Year in Review&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A really awesome post that summarize what is going on in deep learning in 2017. Some points that I enjoy most:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evolution Algorithm (e.g. Genetic Algorithm) is coming back again.&lt;/li&gt;
  &lt;li&gt;Lots of deep learning framework is available right now: PyTorch is pretty popular in academic, but personally I thing TensorFlow is still the bests to try out (It’s also my plan to be more familiar with TensorFlow and work on some side project).&lt;/li&gt;
  &lt;li&gt;A good online reinforcement learning algorithm to read: &lt;a href=&quot;https://github.com/openai/baselines&quot;&gt;&lt;em&gt;OpenAI Baseline&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A good online courses: https://stats385.github.io/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;effective-modern-c&quot;&gt;Effective Modern C++&lt;/h3&gt;
&lt;p&gt;Mainly read the chapter about lambda function, some take away is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Avoid using default capture&lt;/li&gt;
  &lt;li&gt;In C++14, we can use init capture to move data we would like to use into the closure class, or use some expression to initialize the data member&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)](){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_unique&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()](){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Use decltype on auto&amp;amp;&amp;amp; parameters to forward them&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[](&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;decltype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Another thing to remember is to prefer using &lt;strong&gt;alias&lt;/strong&gt; declarations than &lt;strong&gt;typedefs&lt;/strong&gt;, because &lt;strong&gt;alias&lt;/strong&gt; supports templates better:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyAllocList&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyAlloc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sun, 14 Jan 2018 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//What-I-Read-This-Week-I/</link>
        <guid isPermaLink="true">https://pyemma.github.io//What-I-Read-This-Week-I/</guid>
      </item>
    
      <item>
        <title>Deep Work Reading Note</title>
        <description>&lt;p&gt;“Deep Work” is the first book I read this year. I was pretty impressed by the idea and methods the author purposed to help you gain the ability to do “deep work”, which means how to be concentrate on the work that can really generate value. In this note, I would like to summarize the main points of author, together with my personal experience.&lt;/p&gt;

&lt;p&gt;Deep work is the work which can really generate value, compared with shallow work, which is more like some routines that don’t need to think about too much to finish. Only by doing deep work, can we really improve ourselves, broaden our vision and shape our skills. As my personal experience, learning a new machine learning algorithm, or build a new APP counts to deep work, but repeatedly doing some interview algorithm exercise is shallow work.&lt;/p&gt;

&lt;p&gt;There are mainly 4 different strategies for deep work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Monastic: This is the most crucial strategies, which means that you are going to disconnected with all around and only focusing on the work you are doing all the time. This strategy does not apply to me, as I have work to do everyday and could not totally block others.&lt;/li&gt;
  &lt;li&gt;Bimodal: This is less crucial then the above one. For this one, you are not going to disconnected with the world all the time, but periodically. For example, for each year, you might choose two or three month to disconnect; or for each month, you choose one week to disconnect. This also does not apply for me, if I’m absent for too long, my manager would go crazy about me.&lt;/li&gt;
  &lt;li&gt;Rhythmic: This maybe the most common way to do deep work, which is to schedule some time each day dedicatedly. For example, every morning from 7:00 am to 9:00 am do deep work. This is the most suitable strategy for me, as well as for most of us. Although the “deepness” is not as powerful as the above two, but it still works pretty well. My plan is to change my schedule, and leave some time before go to work and go to bed, do some deep work.&lt;/li&gt;
  &lt;li&gt;Journalistic: This is the most flexible version, which is just to do deep work occasionally across the day, whenever you have some time. But this really need have strong deep work ability, so that you can switch to deep work easily. Because the switch will consume lots of power-will, which is limited resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides these strategies, there are also some tips to help do deep work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Come up with some rules when you do deep work, e.g. where to work (red rock cafe?), how to work (ban internet, no wechat) and how to support you deep work (relax music and good coffee)&lt;/li&gt;
  &lt;li&gt;Go to some really really really expensive places :)&lt;/li&gt;
  &lt;li&gt;Collaboration with others to help inspire your idea, but still work alone to do deep work&lt;/li&gt;
  &lt;li&gt;Execute like a business:
    &lt;ul&gt;
      &lt;li&gt;Have a small number of clear ambitious. I usually fail this one, as I was always trying to do lots of thing together to maximums my parallelism.&lt;/li&gt;
      &lt;li&gt;Record to number of deep hour you spend on deep work&lt;/li&gt;
      &lt;li&gt;Keep a compelling scoreboard to track your progress&lt;/li&gt;
      &lt;li&gt;Spend sometime to introspect how well you are doing deep work, what you success and what you fail to do&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Take good rest, e.g. stop working after 8:00 pm&lt;/li&gt;
  &lt;li&gt;Divide the working hour into some large block (deep work block and shallow work block). This would be extreme helpful for me, as I was easily distracted when I was writing code. What I plan to do is to have three long (more than 1hr) deep working blocks to focusing on coding, reading/writing important document/posts. Also schedule every minutes of the work time.&lt;/li&gt;
  &lt;li&gt;Do some meditate productively. What I plan is to spend some time walking around the campus
    &lt;ul&gt;
      &lt;li&gt;Be wary of distraction and looping. When you start to distract from your original problem, remind yourself and come back. Also you need to avoid the looping question you have already thought about&lt;/li&gt;
      &lt;li&gt;Structure deep thinking&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do some memory exercise to help you improve your concentration&lt;/li&gt;
  &lt;li&gt;Quit social media (no Facebook, wechat :) )&lt;/li&gt;
  &lt;li&gt;Know how to tell what is deep work and what is shallow work, and prioritize deep work.&lt;/li&gt;
  &lt;li&gt;Talk with your manager about the percentage of shallow work spend on each day.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//Deep-Work-Reading-Note/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Deep-Work-Reading-Note/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 8</title>
        <description>&lt;p&gt;This is the last lesson for the entire reinforcement learning, and in this lesson we will learn something related to exploit and explore. In machine learning service, like recommendation service, there is always a trade off between exploit and explore. Exploit means we are always choosing the best given the current information we have, while explore means try something new we haven’t tried yet. An example is if you go to restaurant, you can always go to the one you enjoy most(exploit), while you can also try a new one(explore).&lt;/p&gt;

&lt;p&gt;This problem is usually formularized as multi bandit problem, which can be represented as \(&amp;lt;A, R&amp;gt; \). Here \( A \) is a set of action we can take, and \( R^a(r) = P[R=r, A=a] \) is an &lt;strong&gt;unknown&lt;/strong&gt; probability distribution over rewards. At each time, our agent is going to pick an action, and the environment will generate a reward. The goal is to maximize the cumulative reward.&lt;/p&gt;

&lt;h4 id=&quot;regret&quot;&gt;Regret&lt;/h4&gt;
&lt;p&gt;We can measure the goodness of our action use &lt;strong&gt;regret&lt;/strong&gt;. Suppose the action value is the mean reward for an action \( a \)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(a) = E[r|a]&lt;/script&gt;

&lt;p&gt;and the optimal value \( V^\star \) is the max mean reward we can get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^\star = Q(a^\star) = max_{a\in A}Q(a)&lt;/script&gt;

&lt;p&gt;Then maximize the cumulative reward is equivalent to minimize the total regret, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_t = E[\sum_{i=1}^t (V^\star - Q(a_i))]&lt;/script&gt;

&lt;h4 id=&quot;upper-confidence-bound&quot;&gt;Upper Confidence Bound&lt;/h4&gt;
&lt;p&gt;We can try to solve this problem in the face of uncertainty. The best action we should try is the one that would on one hand has a high mean reward, and on the other hand have a high uncertainty. We might get a higher reward, which is good. While we can also get a worse reward, but that does not matter, since we can reduce our uncertainty about that action, and prefer other action which might have higher reward. A more formal description is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Estimate an upper confidence \( \hat{U_t}(a) \) for each action value, which depends on the number of times \( a \) has been selected, the larger the times, the smaller the upper confidence&lt;/li&gt;
  &lt;li&gt;Such that \( Q(a) \le \hat{Q_t}(a) + \hat{U_t}(a) \) with high probability&lt;/li&gt;
  &lt;li&gt;Select action maximize Upper Confidence Bound (UCB)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} \hat{Q_t}(a) + \hat{U_t}(a)&lt;/script&gt;

&lt;p&gt;We need to come up with some method to calculate the upper bound. Here, we bring &lt;em&gt;Hoeffding’s Inequality&lt;/em&gt; for help&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Let \( X_1,…, X_t \) be i.i.d. random variables in \( [0, 1] \), and let \( \bar{X_t} = \frac{1}{i} \sum_{i=1}^t X_i \) be the sample mean. Then&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[E[X] &gt; \bar{X}_t + u] \le e^{-2tu^2}&lt;/script&gt;

&lt;p&gt;With this we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[Q(a) &gt; \hat{Q_t}(a) + \hat{U_t}] \le e^{-2N_t(a)U_t(a)^2}&lt;/script&gt;

&lt;p&gt;where \( N_t(a) \) is the expected number of \( a \) is selected. We then can pick a probability \( p \) that true value exceeds UCB, and reduce $p$ as we observer more rewards, e.g. \( p = t^{-4} \). Then we could obtain the upper bound as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_t(a) = \sqrt{\frac{2logt}{N_t(a)}}&lt;/script&gt;

&lt;p&gt;And finally we have the UCB1 algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} (Q(a) + \sqrt{\frac{2logt}{N_t(a)}})&lt;/script&gt;
</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Reinforcement-Learning-Lesson-8/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Reinforcement-Learning-Lesson-8/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 7</title>
        <description>&lt;p&gt;In the pervious notes, we are all using &lt;strong&gt;model-free&lt;/strong&gt; reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1&quot;&gt;Lesson 1&lt;/a&gt; we know that a MDP can be represent by \( &amp;lt;S, A, P, R&amp;gt; \), and our model is going to understand and simulate this. We will only introduce the simple version here, in which we assume that the \( S \) and \( A \) is known, and thus we only need to model \( P \) and \( R \). We can formulate it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{t+1} ~ P_\eta(S_{t+1}|S_t, A_t) \\
R_{t+1} = R_\eta(R_{t+1}|S_t, A_t)&lt;/script&gt;

&lt;p&gt;where the prediction of next state is a density estimation problem and the reward is a regression problem.&lt;/p&gt;

&lt;h2 id=&quot;integrated-architecture&quot;&gt;Integrated Architecture&lt;/h2&gt;
&lt;p&gt;In this architecture, we are going to consider two types of experience. &lt;strong&gt;Real experience&lt;/strong&gt; which is sampled from the environment, and &lt;strong&gt;Simulated experience&lt;/strong&gt; which is sampled from our model. In the past, we only use the real experience to learn value function/policy. Now, we are going to learn our model from real experience, then plan and learn value function/policy from both real and simulated experience. This is thus called integrated architecture (integration of real and fake), the &lt;strong&gt;Dyna Architecture&lt;/strong&gt;. Here is an picture to illustrate what the logic flow of Dyna is like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dyna.png&quot; alt=&quot;Dyna Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to the Dyna architecture, we can design many algorithm, here is an example of &lt;strong&gt;Dyna-Q Algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize \( Q(s, a) \) and \( Model(s, a) \) for all \( s \) and \( a \)&lt;/li&gt;
  &lt;li&gt;Do forever:
    &lt;ul&gt;
      &lt;li&gt;\( S = \) current (nonterminal) state&lt;/li&gt;
      &lt;li&gt;\( A = \epsilon - \text{greedy}(S, Q) \)&lt;/li&gt;
      &lt;li&gt;Execute action \( A \); observe result reward \( R \), and state \( S’ \)&lt;/li&gt;
      &lt;li&gt;\( Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)] \) (This is using real experience)&lt;/li&gt;
      &lt;li&gt;Update \( Model(S, A) \) using \( R, S’ \)&lt;/li&gt;
      &lt;li&gt;Repeat \( n \) times: (This is using simulated experience to learn value function)
        &lt;ul&gt;
          &lt;li&gt;\( S = \) random previously observed state&lt;/li&gt;
          &lt;li&gt;\( A = \) random action previously taken in \( S \)&lt;/li&gt;
          &lt;li&gt;Sample \( R, S’ \) from \( Model(S, A) \)&lt;/li&gt;
          &lt;li&gt;\( Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)] \)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;monte-carlo-tree-search&quot;&gt;Monte-Carlo Tree Search&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Monte-Carlo Tree Search&lt;/strong&gt; is a very efficient algorithm to plan once we have a model.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model \( M_v \)&lt;/li&gt;
  &lt;li&gt;Simulate $K$ episodes from current state $s_t$ using current simulation policy \( \pi \)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{s_t, A_t^k, R_{t+1}^k, S_{t+1}^k, ..., S_T^k} ~ M_v, \pi&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Build a search tree containing visited states and actions&lt;/li&gt;
  &lt;li&gt;Evaluate state \( Q(s, a) \) by mean return of episodes from \( s, a \)&lt;/li&gt;
  &lt;li&gt;After search is finished, select current (real) action with maximum value in search tree&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In MCMT, the simulation policy \( \pi \) improves. Each simulation consists of two phases (in-tree, out-of-tree):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree policy (improves): pick action to maximize \( Q(S, A) \)&lt;/li&gt;
  &lt;li&gt;Default policy (fixed): pick action randomly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Repeat (each simulation):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evaluate states \( Q(S, A) \) by Mento-Carlo evaluation&lt;/li&gt;
  &lt;li&gt;Improve tree policy, e.g. by \( \epsilon-\text{greedy}(Q) \)s&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are several advantages of MCMT:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Highly selective best-first search&lt;/li&gt;
  &lt;li&gt;Evaluates states dynamically&lt;/li&gt;
  &lt;li&gt;Uses sampling to break curse of dimensionality&lt;/li&gt;
  &lt;li&gt;Works for “black-box” models (only requires samples)&lt;/li&gt;
  &lt;li&gt;Computationally efficient, anytime&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 11 Sep 2017 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Reinforcement-Learning-Lesson-7/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Reinforcement-Learning-Lesson-7/</guid>
      </item>
    
  </channel>
</rss>
