<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Coding Monkey</title>
    <description>A coding monkey, who is passionate in algorithm and machine learning, hoping one day to become a coding machine.</description>
    <link>pyemma.github.io</link>
    <atom:link href="pyemma.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>2017 年终终结</title>
        <description>&lt;p&gt;2017年一转眼就过去了，自己也从一个刚刚毕业的学生成了一个快上了两年班的上班族了。趁着年末好好回顾一下这一整年发生的事情，一方面是给自己留个纪念，另一方面也是好好总结一下这一年的得失，找到改进的方向。&lt;/p&gt;

&lt;h3 id=&quot;工作&quot;&gt;工作&lt;/h3&gt;
&lt;p&gt;前半年工作比较顺利，但是后半年工作比较艰辛。首先最开心的事情就是年初得升职了，然后上半年的表现的评级也还不错，算是对自己能力的一个肯定。但是下半年由于种种原因，主要负责的项目一直进展非常不顺利，而且最后没能发布出去，很郁闷。辛辛苦苦干了大半年，每天基本从早上9点工作到晚上10点才回家，每个周末也去公司加班，但是还是事与愿违。而且由于一直专注于这个项目，导致疏忽了其他的方面工作，例如roadmap啊，mentor新人啊之类的，和上半年比起来差不少。&lt;/p&gt;

&lt;p&gt;反思了一下，感觉还是太焦躁心急，不够沉稳。心里总是想着早点升职多赚钱什么的，然而往往事与愿违。尤其是自己的计划被打乱之后，整个人就会变得非常的消沉，郁闷。一个很好的例子就是今年回国H1B签证被check，导致在国内待了两个月。当时的心情是非常的郁闷，还经常自己吓唬自己万一拿不到签证回不去美国咋办，好好的工作没了，自己怎么就这么倒霉。现在回头看感觉自己当时真是好笑，本来两个月的时间完全可以出去逛逛玩玩当个长假来休息，但是由于自己太重视这份工作，使自己一直纠结于被check的这一无法改变的事实，在家愁了两个月。不过经历过这一出之后，自己的心态也有所改善，脾气基本上是给磨没了。现在遇到点事情能比当时更沉得住气一些了。然而离自己理想的境界还有很长的距离， 希望在今后的日子里能在“不以物喜，不以己悲”的修炼上更进一步。&lt;/p&gt;

&lt;p&gt;另一个就是要想办法挑战自己。最近在工作上一直处于自己的舒适区，只是在做一些自己已经很熟练的工作而不去做一些自己没有尝试过的更具有挑战性的事情。很大的程度上这是因为担心脱离舒适区的后果，比如想换组，但是万一换到别的组表现不如现在的好影响了自己的评级了咋办？风险自然是有的，磨难肯定也是不少，但是只有经历过这些，才能成长，才能有进一步的提高，才能有更广的可能性。套用钢之炼金术师里面的台词，“在经历过这些磨难之后，就能获得钢铁般的心”。最近刚刚看过了吴军老师对于职业发展的一个观点：“稀缺性”。现在自己的工作，换一个人来干一样能干，都是一些简单的写写代码，调调参数什么的。这说明自己的稀缺性还不够，想要增加自己的稀缺性，那么就要敢于去尝试一些别人不能做的事情，这种事情往往风险比较大，存在于舒适区之外。另一方面，就是要增加自己的见识，对自己的领域要有一些比较深入的看法和见解，因为这个东西，别人是不可能轻易学去的，当然培养这方面也很困难，需要大量的经验积累，而这种经验，如果一直处于舒适区，是很难获得的。&lt;/p&gt;

&lt;h3 id=&quot;学习&quot;&gt;学习&lt;/h3&gt;
&lt;p&gt;年初定下来的好多计划最终都没有坚持下来：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;想要学口琴，但是一点都没有练过&lt;/li&gt;
  &lt;li&gt;想要学习日语，但是没能坚持下来&lt;/li&gt;
  &lt;li&gt;想要学习GO语言，但是根本就没有开始&lt;/li&gt;
  &lt;li&gt;想要看一些开源的代码，结果没看过&lt;/li&gt;
  &lt;li&gt;想要做一些side project，但是一个也没做出来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是还是有一些坚持下来了：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;每个月读一本书&lt;/li&gt;
  &lt;li&gt;学习了TensorFlow并且写了些简单的算法&lt;/li&gt;
  &lt;li&gt;自学了Reinforcement Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;感觉自己在增长新技能这方面有点不够专注，什么都想沾一手，但是最终的结果是什么都没有学到，典型的“想得太多，做得太少”。而且我还另一个特点，开始干一件事情的阈值很高，但是只要开始了，我就会坚持下去，尽量做到完美，这也表现出我不是一个半途而废的人，那么关键就是怎么开始了。所以在2018年，打算缩减自己的计划，把精力集中在两三个事情上，强迫自己开始并坚持下去。&lt;/p&gt;

&lt;p&gt;另一个就是自己不太擅长记录，比如读过的书，看过就扔了，没有提炼出一些个性化的东西；学过的一些技术，也是没有好好的总结，过一阵子也就忘记了。之前尝试过写读书笔记以及技术博客，但是都没有坚持下来。在新的一年里要坚持更新自己的博客。&lt;/p&gt;

&lt;h3 id=&quot;感情&quot;&gt;感情&lt;/h3&gt;
&lt;p&gt;感情这一年就是很扎心了。先是跟前女友分手了，之后被一个妹子表白第二天就又被发卡，现在联系一个新认识的妹子结果一直不怎么受待见。这很可能跟我负能量比较多，不够自信有关。我这个人看待事物的时候有点消极，遇到什么事情总是喜欢把消极面放大而不会从积极的焦虑去考虑。然后自信也比较低，总感觉自己比不上别人。感觉这是自己性格方面的两个很重要的缺陷。为此我还买了本书叫《正能量》，想看看从心理学的角度怎么能改正一下。总而言之一句话就是表现的乐观就会感觉乐观，表现的自信就会感觉自信，即行为影响情绪。所以平时说话多用积极的字眼，经常保持微笑，不整天愁眉苦脸的，来帮助自己树立正能量和自信。除此之外，也更加深入的明白了一个道理，找女朋友这件事绝对不能将就，要找就找自己喜欢的，想一辈子过日子的。不能抱着说，“先谈着，慢慢培养感情”，这种思想。培养感情是要建立在一定的冲动之上，如果连一点喜欢的感觉都没有，强迫自己培养感情喜欢上对方，对方痛苦，你自己也痛苦，还浪费时间跟精力，实在不是明智之举。&lt;/p&gt;

&lt;p&gt;当然啦，平时跟基友也总吐槽自己单身，埋怨自己怎么找不到女朋友啥的，但是其实单身的生活还是比较享受的。每天想在公司加班就加班，想回家打游戏就打游戏，周末想去吃个日料就去吃个日料，想到咖啡店看书就去咖啡店看书。时间充分的自由支配也挺好的。&lt;/p&gt;

&lt;h3 id=&quot;生活&quot;&gt;生活&lt;/h3&gt;
&lt;p&gt;生活方面感觉是这一年取得的成就最大的，那就是健身整整坚持了一整年，而且小有成效，再也不用为买什么衣服而发愁了哈哈。从上半年的每天坚持跑步，到下半年每天和同事去健身房做力量训练，虽然过程很痛苦，但是心里充满了成就感。年初的时候参加了公司组织的10km的长跑比赛，跑进了1小时；卧推现在能举起200lb以上；体重曾经一度控制在70kg，这些都是曾经我不敢想象的，然而通过自己的努力都做到了，非常的欣慰。&lt;/p&gt;

&lt;p&gt;另一个感觉很欣慰的事情就是成了一个狂热的咖啡和日料的爱好者。每个周末都会抽个半天去各种各样的咖啡店品尝咖啡看看书，体验一下不同的咖啡店的环境和氛围；晚上也会去各种日料店吃日料，顺便发个美食攻略贴。这些算是给自己单调的生活填上了一些乐趣。&lt;/p&gt;

&lt;p&gt;俗话说得好，“读万卷书，行万里路”。新的一年里想多出去走走，领略一下不同城市的风景。&lt;/p&gt;

&lt;h3 id=&quot;结语&quot;&gt;结语&lt;/h3&gt;
&lt;p&gt;不管2017年怎么样，都已经成为过去，不必太纠结于这一整年的失落和痛苦，让那些成为以后酒桌上的笑谈。昂首挺胸，迎接崭新的一年！&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Dec 2017 00:00:00 -0800</pubDate>
        <link>pyemma.github.io//posts/2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93</link>
        <guid isPermaLink="true">pyemma.github.io//posts/2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 8</title>
        <description>&lt;p&gt;This is the last lesson for the entire reinforcement learning, and in this lesson we will learn something related to exploit and explore. In machine learning service, like recommendation service, there is always a trade off between exploit and explore. Exploit means we are always choosing the best given the current information we have, while explore means try something new we haven’t tried yet. An example is if you go to restaurant, you can always go to the one you enjoy most(exploit), while you can also try a new one(explore).&lt;/p&gt;

&lt;p&gt;This problem is usually formularized as multi bandit problem, which can be represented as $&amp;lt;A, R&amp;gt;$. Here $A$ is a set of action we can take, and $R^a(r) = P[R=r, A=a]$ is an &lt;strong&gt;unknown&lt;/strong&gt; probability distribution over rewards. At each time, our agent is going to pick an action, and the environment will generate a reward. The goal is to maximize the cumulative reward.&lt;/p&gt;

&lt;h4 id=&quot;regret&quot;&gt;Regret&lt;/h4&gt;
&lt;p&gt;We can measure the goodness of our action use &lt;strong&gt;regret&lt;/strong&gt;. Suppose the action value is the mean reward for an action $a$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(a) = E[r|a]&lt;/script&gt;

&lt;p&gt;and the optimal value $V^\star$ is the max mean reward we can get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^\star = Q(a^\star) = max_{a\in A}Q(a)&lt;/script&gt;

&lt;p&gt;Then maximize the cumulative reward is equivalent to minimize the total regret, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_t = E[\sum_{i=1}^t (V^\star - Q(a_i))]&lt;/script&gt;

&lt;h4 id=&quot;upper-confidence-bound&quot;&gt;Upper Confidence Bound&lt;/h4&gt;
&lt;p&gt;We can try to solve this problem in the face of uncertainty. The best action we should try is the one that would on one hand has a high mean reward, and on the other hand have a high uncertainty. We might get a higher reward, which is good. While we can also get a worse reward, but that does not matter, since we can reduce our uncertainty about that action, and prefer other action which might have higher reward. A more formal description is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Estimate an upper confidence $\hat{U_t}(a)$ for each action value, which depends on the number of times $a$ has been selected, the larger the times, the smaller the upper confidence&lt;/li&gt;
  &lt;li&gt;Such that $Q(a) \le \hat{Q_t}(a) + \hat{U_t}(a)$ with high probability&lt;/li&gt;
  &lt;li&gt;Select action maximize Upper Confidence Bound (UCB)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} \hat{Q_t}(a) + \hat{U_t}(a)&lt;/script&gt;

&lt;p&gt;We need to come up with some method to calculate the upper bound. Here, we bring &lt;em&gt;Hoeffding’s Inequality&lt;/em&gt; for help&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Let $X_1,…, X_t$ be i.i.d. random variables in $[0, 1]$, and let $\bar{X_t} = \frac{1}{i} \sum_{i=1}^t X_i$ be the sample mean. Then&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[E[X] &gt; \bar{X}_t + u] \le e^{-2tu^2}&lt;/script&gt;

&lt;p&gt;With this we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[Q(a) &gt; \hat{Q_t}(a) + \hat{U_t}] \le e^{-2N_t(a)U_t(a)^2}&lt;/script&gt;

&lt;p&gt;where $N_t(a)$ is the expected number of $a$ is selected. We then can pick a probability $p$ that true value exceeds UCB, and reduce $p$ as we observer more rewards, e.g. $p = t^{-4}$. Then we could obtain the upper bound as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_t(a) = \sqrt{\frac{2logt}{N_t(a)}}&lt;/script&gt;

&lt;p&gt;And finally we have the UCB1 algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} (Q(a) + \sqrt{\frac{2logt}{N_t(a)}})&lt;/script&gt;
</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lesson-8</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lesson-8</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 7</title>
        <description>&lt;p&gt;In the pervious notes, we are all using &lt;strong&gt;model-free&lt;/strong&gt; reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1&quot;&gt;Lesson 1&lt;/a&gt; we know that a MDP can be represent by $&amp;lt;S, A, P, R&amp;gt;$, and our model is going to understand and simulate this. We will only introduce the simple version here, in which we assume that the $S$ and $A$ is known, and thus we only need to model $P$ and $R$. We can formulate it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{t+1} ~ P_\eta(S_{t+1}|S_t, A_t) \\
R_{t+1} = R_\eta(R_{t+1}|S_t, A_t)&lt;/script&gt;

&lt;p&gt;where the prediction of next state is a density estimation problem and the reward is a regression problem.&lt;/p&gt;

&lt;h4 id=&quot;integrated-architecture&quot;&gt;Integrated Architecture&lt;/h4&gt;
&lt;p&gt;In this architecture, we are going to consider two types of experience. &lt;strong&gt;Real experience&lt;/strong&gt; which is sampled from the environment, and &lt;strong&gt;Simulated experience&lt;/strong&gt; which is sampled from our model. In the past, we only use the real experience to learn value function/policy. Now, we are going to learn our model from real experience, then plan and learn value function/policy from both real and simulated experience. This is thus called integrated architecture (integration of real and fake), the &lt;strong&gt;Dyna Architecture&lt;/strong&gt;. Here is an picture to illustrate what the logic flow of Dyna is like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dyna.png&quot; alt=&quot;Dyna Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to the Dyna architecture, we can design many algorithm, here is an example of &lt;strong&gt;Dyna-Q Algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q(s, a)$ and $Model(s, a)$ for all $s$ and $a$&lt;/li&gt;
  &lt;li&gt;Do forever:
    &lt;ul&gt;
      &lt;li&gt;$S =$ current (nonterminal) state&lt;/li&gt;
      &lt;li&gt;$A = \epsilon - \text{greedy}(S, Q)$&lt;/li&gt;
      &lt;li&gt;Execute action $A$; observe result reward $R$, and state $S’$&lt;/li&gt;
      &lt;li&gt;$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$ (This is using real experience)&lt;/li&gt;
      &lt;li&gt;Update $Model(S, A)$ using $R, S’$&lt;/li&gt;
      &lt;li&gt;Repeat $n$ times: (This is using simulated experience to learn value function)
        &lt;ul&gt;
          &lt;li&gt;$S =$ random previously observed state&lt;/li&gt;
          &lt;li&gt;$A =$ random action previously taken in $S$&lt;/li&gt;
          &lt;li&gt;Sample $R, S’$ from $Model(S, A)$&lt;/li&gt;
          &lt;li&gt;$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;monte-carlo-tree-search&quot;&gt;Monte-Carlo Tree Search&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Monte-Carlo Tree Search&lt;/strong&gt; is a very efficient algorithm to plan once we have a model.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model $M_v$&lt;/li&gt;
  &lt;li&gt;Simulate $K$ episodes from current state $s_t$ using current simulation policy $\pi$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{s_t, A_t^k, R_{t+1}^k, S_{t+1}^k, ..., S_T^k} ~ M_v, \pi&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Build a search tree containing visited states and actions&lt;/li&gt;
  &lt;li&gt;Evaluate state $Q(s, a)$ by mean return of episodes from $s, a$&lt;/li&gt;
  &lt;li&gt;After search is finished, select current (real) action with maximum value in search tree&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In MCMT, the simulation policy $\pi$ improves. Each simulation consists of two phases (in-tree, out-of-tree):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree policy (improves): pick action to maximize $Q(S, A)$&lt;/li&gt;
  &lt;li&gt;Default policy (fixed): pick action randomly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Repeat (each simulation):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evaluate states $Q(S, A)$ by Mento-Carlo evaluation&lt;/li&gt;
  &lt;li&gt;Improve tree policy, e.g. by $\epsilon-\text{greedy}(Q)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are several advantages of MCMT:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Highly selective best-first search&lt;/li&gt;
  &lt;li&gt;Evaluates states dynamically&lt;/li&gt;
  &lt;li&gt;Uses sampling to break curse of dimensionality&lt;/li&gt;
  &lt;li&gt;Works for “black-box” models (only requires samples)&lt;/li&gt;
  &lt;li&gt;Computationally efficient, anytime&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 11 Sep 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lesson-7</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lesson-7</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 6</title>
        <description>&lt;p&gt;In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_{\theta}(s ,a) = P[a|s, \theta]&lt;/script&gt;

&lt;p&gt;In this case, we are not going to have any value function. A slight variance of this method is called &lt;strong&gt;Actor-Critic&lt;/strong&gt;, in which both value function and policy are modeled and learnt.&lt;/p&gt;

&lt;p&gt;The advantage of &lt;strong&gt;Policy based RL&lt;/strong&gt; is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Better convergence properties&lt;/li&gt;
  &lt;li&gt;Effective in high-dimensional or continuous action spaces&lt;/li&gt;
  &lt;li&gt;Can learn stochastic policies&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h4&gt;
&lt;p&gt;Since we are going to learn $\pi_\theta (s, a)$ and find the best $\theta$, we need to first find a way to measure the quality of our policy. These are called &lt;strong&gt;policy objective function&lt;/strong&gt; and some we can use are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In episode environment we can use the start value&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_1(\theta) = V^{\pi_\theta}(s_1) = E_{\pi_\theta}[v_1]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;In continuous environment we can use average value or average reward pre time-step&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_{avV}(\theta) = \sum_{s}d^{\pi_\theta}(s)V^{\pi_\theta}(s) \\
J_{avR}(\theta) = \sum_{s}d^{\pi_\theta}(s)\sum_{a}\pi_\theta(s, a)R_s^a&lt;/script&gt;

&lt;p&gt;where $d^{\pi_\theta}(s)$ is stationary distribution of Markov chain for $\pi_\theta$.&lt;/p&gt;

&lt;p&gt;After we have the measurement of the policy quality, we are going to find the best parameter which gives us the best quality and this becomes an optimization problem. Actually, similar to the last post, we can also use stochastic gradient to help use here. Since we are trying to find the maximum value, we are going to use what is called gradient ascent to find the steepest direction to update our parameter (very similar to gradient decrease).&lt;/p&gt;

&lt;h4 id=&quot;score-function&quot;&gt;Score Function&lt;/h4&gt;
&lt;p&gt;In order to compute the policy gradient analytically, we introduced the &lt;strong&gt;score function&lt;/strong&gt;. Assume policy $\pi_{\theta}$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta (s, a)$. Then using some tricky we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_\theta \pi_\theta (s, a) &amp; = \pi_\theta (s, a) \frac{\nabla_\theta \pi_\theta (s, a) }{\pi_\theta (s, a)} \\
&amp; = \pi_\theta (s, a) \nabla_\theta log \pi_\theta (s, a)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here, $\nabla_\theta log \pi_\theta (s, a)$ is the &lt;strong&gt;score function&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;For any differentiable policy $\pi_\theta (s, a)$, for any of the policy objective functions $J_1, J_{avV}, J_{avR}$, the policy gradient is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log \pi_\theta (s, a) Q^{\pi_\theta} (s, a)]&lt;/script&gt;

&lt;h4 id=&quot;monte-carlo-policy-gradient&quot;&gt;Monte-Carlo Policy Gradient&lt;/h4&gt;
&lt;p&gt;Use return as an unbiased sample of $Q^{\pi_\theta} (s, a)$, the algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $\theta$ arbitrarily
    &lt;ul&gt;
      &lt;li&gt;for each episode ${s_1, a_1, r_2, …, s_{T-1}, a_{T-1}, r_T} ~ \pi_\theta$ do
        &lt;ul&gt;
          &lt;li&gt;for $t = 1$ to $T - 1$ do
            &lt;ul&gt;
              &lt;li&gt;$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) v_t$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;end for&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;end for&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $\theta$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;actor-critic-policy-gradient&quot;&gt;Actor Critic Policy Gradient&lt;/h4&gt;
&lt;p&gt;The problem with Monte-Carlo Policy Gradient is that is has a very high variance. In order to reduce the variance, we can use a &lt;strong&gt;critic&lt;/strong&gt; to estimate the action value function. Thus in &lt;strong&gt;Actor Critic Policy Gradient&lt;/strong&gt;, we have two components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Critic&lt;/em&gt; updates action value function parameters $w$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Actor&lt;/em&gt; updates policy parameters $\theta$, in direction suggested by critic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example when we use linear value function approximation for the critic:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $s$, $\theta$&lt;/li&gt;
  &lt;li&gt;Sample $a ~ \pi_\theta$&lt;/li&gt;
  &lt;li&gt;for each step do
    &lt;ul&gt;
      &lt;li&gt;Sample reward $r$, sample next state $s’$&lt;/li&gt;
      &lt;li&gt;Sample action $a’ ~ \pi_\theta (s’, a’)$&lt;/li&gt;
      &lt;li&gt;$\delta = r + \gamma Q_w(s’, a’) - Q_w(s, a)$ (This is the TD error)&lt;/li&gt;
      &lt;li&gt;$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) Q_w(s, a)$ (We replace with the approximation)&lt;/li&gt;
      &lt;li&gt;$w = w + \beta \delta \phi(s, a)$ (Update value function approximation model parameter)&lt;/li&gt;
      &lt;li&gt;$a = a’$, $s = s’$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 10 Sep 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcment-Learning-Lesson-6</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcment-Learning-Lesson-6</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 5</title>
        <description>&lt;p&gt;In this post, we are going to look into how can we solve the real world problem with a practical way. Think of the state value function $v(s)$ or the action value function $q(s, a)$ we mentioned before. If the problem has a really large state space, then it would take a lot of memory to store each value function. Instead of recording each value function, we can actually use a model to approximate the actual value function, which means given the current state, we want to predict the value of the state. There are three types of value function approximation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input current state, output the state value&lt;/li&gt;
  &lt;li&gt;Input current state and an action, out put the action value&lt;/li&gt;
  &lt;li&gt;Input current state, output all possible action’s action value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This can be reviewed as a classical supervised learning problem if we &lt;strong&gt;know the actual value function&lt;/strong&gt;, and more accurately speaking, its a regression problem. In the regression problem, we are trying to fit a model which will output some real number that matches the our input label as much as possible. In the regression problem, the loss is defined using mean-square error. In order to get a model, we need first to do some feature engineering and represent each state using the &lt;strong&gt;feature vector&lt;/strong&gt; $x(S)$, this is going to be the input into our model. And then we try to minimize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w) = E_{\pi}[(v_{\pi}(S) - v(S, w))^2]&lt;/script&gt;

&lt;p&gt;Here $w$ is our model’s parameter and is what we are going to improve. $v_{\pi}(S)$ is the actual value (label) and $v(S, w)$ is the output from our model (predict). In order to minimize this loss, we use stochastic gradient decrease to update $w$, which we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w = \alpha (v_{\pi}(S) - v(S, w)) \nabla_w v(S, w)&lt;/script&gt;

&lt;p&gt;Here $\alpha$ is a learning rate controlling how fast we improve $w$, and $\nabla_w v(S, w)$ is the derivate of our model toward the parameter, for example, if we choose a linear model, where $v(S, w) = x(S)^T * w$, then we would have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w = \alpha (v_{\pi}(S) - v(S, w))x(S)&lt;/script&gt;

&lt;p&gt;However, we could only obtain this update when we really &lt;strong&gt;know the actual value function&lt;/strong&gt;, which is the case of supervised learning. However, in reinforcement learning, we are lack of such information. So we have to use some target to replacement them. We can actually combining it with the algorithm we have introduced before. For example the MC algorithm, In each episode, we will get a series of the state and corresponding return $&amp;lt;S_t, G_t&amp;gt;$, we can actually use this return as our target and train our model on it. The process would be like use our model to compute the state value, and use some policy to go through the process, then we would have $&amp;lt;S_1, G_1&amp;gt;, &amp;lt;S_2, G_2&amp;gt;, …, &amp;lt;S_T, G_T&amp;gt;$. Then use these as our training data and update our model. This training is &lt;strong&gt;on-policy&lt;/strong&gt; (because we are learning as well as behaving) and &lt;strong&gt;incremental&lt;/strong&gt; (episode by episode). Similar things can be applied to TD(0) and TD($\lambda$), where we use TD target and $G_t^\lambda$. Good news to use TD target is that is needs less steps for model to converge (since TD target is less variance), but it might not converge in some cases, for example, if we choose Neural Network as our model, then the model will blow up.&lt;/p&gt;

&lt;p&gt;Besides the incremental method, there is also &lt;strong&gt;batch&lt;/strong&gt; method, which we record all experience of the agent in $D$, and sample from it to get the training sample, then we update our model parameter using the same method above. &lt;strong&gt;Batch&lt;/strong&gt; method is more sample efficient and tries to find the best fit of all value functions available. While in the &lt;strong&gt;incremental&lt;/strong&gt; one, we are generate training sample one by one which is not very efficient, and we only use it once after update the parameter. A more detailed example is Deep Q-Networks (DQN), you can think of it as using NN model along with Q learning method. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Take action $a_t$ according to $\epsilon$-greedy policy&lt;/li&gt;
  &lt;li&gt;Store transition $(s_t, a_t, r_{t+1}, s_{t+1})$ in replay memory $D$&lt;/li&gt;
  &lt;li&gt;Sample random mini-batch of transitions $(s, a, r, s^,)$ from $D$&lt;/li&gt;
  &lt;li&gt;Compute Q learning target with an old, fixed parameter $w^-$&lt;/li&gt;
  &lt;li&gt;Optimize MSE between Q target and Q learning Network&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_i(w_i) = E_{s,a,r,s^, ~ D}[(r + \gamma max_{a^,}Q(s^,,a^,; w^-) - Q(s, a; w_i))^2]&lt;/script&gt;

&lt;p&gt;The key method that stabilize the model is experience reply and Q target. For the experience reply, it helps decouple the relationship between each step since we are randomly sampling. For the Q target, we are using the model several steps ago, not the model we just updated. You can think of this as avoid oscillation.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Sep 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lesson-5</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lesson-5</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 4</title>
        <description>&lt;p&gt;In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.&lt;/p&gt;

&lt;h4 id=&quot;mento-calro-policy-iteration&quot;&gt;Mento Calro Policy Iteration&lt;/h4&gt;
&lt;p&gt;In the &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-2&quot;&gt;Lesson 2&lt;/a&gt;, we mentioned how to solve a MDP when we have full information about the MDP. One method is called &lt;strong&gt;Policy Iteration&lt;/strong&gt;. It can be divided into two components: &lt;em&gt;policy iterative evaluation&lt;/em&gt; and &lt;em&gt;policy improvement&lt;/em&gt;. For the evaluation part, we can use the methods in &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-3&quot;&gt;last lesson&lt;/a&gt;, nominally MC and TD. However, we could not directly use the state value function, cause in the policy improvement step (e.g. greedy), we need to know the $R$ and $P$ to find the best action (recall the &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1&quot;&gt;Bellman Optimality Function&lt;/a&gt;). However, action value function does not need the model of the MDP while in greedy policy improvement:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_*(s) = argmax_a q(s, a)&lt;/script&gt;

&lt;p&gt;For the policy improvement part. If we stick to the greedy method, it will not be good for us to explore all possible states. So we use another method which is called $\epsilon$-greedy. We will have $1-\epsilon$ probability to perform greedily (choose the current best action), and have $\epsilon$ probability to random choose an action:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\pi(s|a) = \begin{cases}
\frac{\epsilon}{m} + 1 - \epsilon, &amp; \text{if $a^\star = argmax_a Q(s, a)$} \\
\frac{\epsilon}{m}, &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have the final Mento Calro Policy Iteration as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sample the kth episode $S_1, A_1, …, S_T$ from policy $\pi$&lt;/li&gt;
  &lt;li&gt;For each state $S_t$ and $A_t$ in the episode&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N(S_t, A_t) = N(S_t, A_t) + 1 \\
Q(S_t, A_t) = Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}((G_t - Q(S_t, A_t))) \\&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Update the $\epsilon$ and policy:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon = 1/k \\
\pi = \epsilon\text{-greedy}(Q)&lt;/script&gt;

&lt;h4 id=&quot;sarsa-algorithm&quot;&gt;Sarsa Algorithm&lt;/h4&gt;
&lt;p&gt;If we use the logic in TD for the evaluation part, then we would have the sarsa algorithm. The main difference is that, in original TD, we use the value state function of the successor state, however, we need the action value function right now. We can obtain that by run our current policy again (remember, TD does not need the complete sequence of experience, we can generate the state and action along the way). Following is the algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0&lt;/li&gt;
  &lt;li&gt;Repeat for each episode
    &lt;ul&gt;
      &lt;li&gt;Initialize $S$, choose $A$ from the current policy derived from $Q$&lt;/li&gt;
      &lt;li&gt;Repeat for each step in the episode until we hit terminal
        &lt;ul&gt;
          &lt;li&gt;Take action $A$, observe $R$ and $S^\prime$&lt;/li&gt;
          &lt;li&gt;Choose $A^\prime$ from $S^\prime$ from the current policy derived from $Q$&lt;/li&gt;
          &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;Q(S, A) = Q(S, A) + \alpha(R + \gamma Q(S^\prime, A^\prime) - Q(S, A))&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Update $S = S^\prime, A = A^\prime$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, we can also use Eligibility Trace for the sarsa algorithm and result in sarsa($\lambda$) algorithm. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0
    &lt;ul&gt;
      &lt;li&gt;Repeat for each episode&lt;/li&gt;
      &lt;li&gt;Initialize $E$ for each $s, a$ pair to 0&lt;/li&gt;
      &lt;li&gt;Initialize $S$, choose $A$ from the current policy derived from Q&lt;/li&gt;
      &lt;li&gt;Repeat for each step in the episode until we hit terminal
        &lt;ul&gt;
          &lt;li&gt;Take action $A$, observe $R$ and $S^\prime$&lt;/li&gt;
          &lt;li&gt;Choose $A^\prime$ from $S^\prime$ from the current policy derived from Q&lt;/li&gt;
          &lt;li&gt;Calculate $\delta = R + \gamma Q(S^\prime, A^\prime) - Q(S, A)$&lt;/li&gt;
          &lt;li&gt;Update $E(S, A) = E(S, A) + 1$&lt;/li&gt;
          &lt;li&gt;For each $s$ and $a$ pair&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(s, a) = Q(s, a) + \alpha\delta E(s, a) \\
E(s, a) = \gamma\lambda E(s, a)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Update $S = S^\prime, A = A^\prime$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;q-learning&quot;&gt;Q Learning&lt;/h4&gt;
&lt;p&gt;Both MC policy iteration and sarsa algorithm are &lt;strong&gt;online learning&lt;/strong&gt; method, which means that they are observing there own policy, learning along the process. There is another category which is called &lt;strong&gt;offline learning&lt;/strong&gt;, in which we learn from other policy, not the policy we are trying to improving. Example is that a robots learns walking by observing human. Q learning falls in this category. It is pretty similar to the sarsa algorithm, the only difference is that when we get the action for successor state, we replace the $\epsilon$-greedy to greedy policy. The Q learning method is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize Q for each state and action pair arbitrarily, set Q(terminate, *) to 0&lt;/li&gt;
  &lt;li&gt;Repeat for each episode
    &lt;ul&gt;
      &lt;li&gt;Initialize $S$, choose $A$ from the current policy derived from Q&lt;/li&gt;
      &lt;li&gt;Repeat for each step in the episode until we hit terminal
        &lt;ul&gt;
          &lt;li&gt;Take action $A$, observe $R$ and $S^\prime$&lt;/li&gt;
          &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;Q(S, A) = Q(S, A) + \alpha(R + \gamma max_a Q(S^\prime, a) - Q(S, A))&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Update $S = S^\prime$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 19 Aug 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lesson-4</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lesson-4</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 3</title>
        <description>&lt;p&gt;In this lesson, we will learn about what to do when we have no knowledge about the MDP. In the last lesson, we learnt about how to solve a MDP when we have full information about it (e.g. $P$, $R$). When we don’t have enough information, the Bellman Equation won’t work. The only way is to learn from experience, where we run the process once, and obtain a $S_1, R_1, …, S_T$ sequence and improve our value function with it. This is called model free. In this lesson, we learn about when given a policy $\pi$, how do we calculate the state value function (which is called model free predicting). And in the next one, we will learn how to come up with the policy (which is called model free control).&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-reinforcement-learning&quot;&gt;Monte-Carlo Reinforcement Learning&lt;/h4&gt;
&lt;p&gt;The first method is called Mento-Carlo Reinforcement Learning. The idea behind this method is to use empirical mean to measure the value. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $N(s)$ to all zero, copying the value function from last one&lt;/li&gt;
  &lt;li&gt;Given an episode $S_1, R_1, …, S_T$&lt;/li&gt;
  &lt;li&gt;For each $S_t$ with return $G_t$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N(S_t) = N(S_t) + 1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S_t) = V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))&lt;/script&gt;

&lt;p&gt;Here $N(S_t)$ counts the number of our visit to $S_t$. The update function is using running mean to update the value of the current state, by moving it towards the return in this episode (G_t) a little bit. Here we can replace $\frac{1}{N(S_t)}$ to a small number $\alpha$, this is functioning as a learning rate to control how quick we update our value function. When we increase the counter, we can increase it either by first visit within the episode or every visit within the episode.&lt;/p&gt;

&lt;p&gt;Mento-Carlo Reinforcement Learning can only works with episode experience, which means the MDP must has a terminate state and the experience must be complete.&lt;/p&gt;

&lt;p&gt;In this method, we are &lt;strong&gt;sampling&lt;/strong&gt; from the policy distribution because for each state, we are only considering one possible successor state. The learning method in last lesson is using dynamic programming, and it is not based on sampling, it actually takes all possible successor states into consideration.&lt;/p&gt;

&lt;h4 id=&quot;td0-learning&quot;&gt;TD(0) Learning&lt;/h4&gt;
&lt;p&gt;The second method is called Temporal Difference Learning. As its naming suggested, in this method we are not using the actual return in the episode but using an temporal estimation to update the value function. The algorithm is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For each $S_t$ within the episode&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S_t) = V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))&lt;/script&gt;

&lt;p&gt;Here, $R_{t+1} + \gamma V(S_{t+1})$ is called TD target, and $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ is called the TD error. The main logic here is bootstrapping, which means we are not directly making each value function to the most accurate value it should be given this episode. We are making it slightly better based on our current estimate on the successor state. The benefit of the doing so is that we can learn from incomplete experience, and MDP without a terminal state.&lt;/p&gt;

&lt;p&gt;In this method, we are also &lt;strong&gt;sampling&lt;/strong&gt; from the policy distribution, as well as bootstrapping. Dynamic programming also uses bootstrapping similar to TD(0) learning (recall the Bellman Equation).&lt;/p&gt;

&lt;h4 id=&quot;tdlambda-learning&quot;&gt;TD($\lambda$) Learning&lt;/h4&gt;
&lt;p&gt;In both MC and TD(0) Learning, we are looking forward to the future rewards. In MC, we are looking until we reach the end, while in TD(0) we only look at next step. Instead of looking forward, we can also looking backward. However, this involves how to assign the current timestamp rewards to pervious states. This is called credit assignment problem. And the method we overcome it is to use &lt;strong&gt;Eligibility Traces&lt;/strong&gt;, which fusion both assigning credit to the most recent state and most frequent states. Here we introduce the TD($\lambda$) algorithm (back view version):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize Eligibility Traces $E_0(s) = 0$&lt;/li&gt;
  &lt;li&gt;Given an experience, for each state $s$:&lt;/li&gt;
  &lt;li&gt;Update the Eligibility Traces by: $E_t(s) = \gamma \lambda E_{t-1}(s) + 1(S_t = s)$&lt;/li&gt;
  &lt;li&gt;Calculate the update step by: $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$&lt;/li&gt;
  &lt;li&gt;Update &lt;strong&gt;each&lt;/strong&gt; state by: $V(s) = V(s) + \alpha \delta_t E_t(s)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we use $\lambda = 0$, then the Eligibility Traces will fall to $1(S_t = s)$ and replace it in the update function, we will see that its the exact same update function as TD(0). If we choose $\lambda = 1$, then it is actually equals to every visit MC. We can prove it as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Suppose in our experience, $s$ is visited at timestamp $k$, then the $E_t(s)$ will be like&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E_t(s) = \begin{cases}
0, &amp; \text{if $t &lt; k$} \\
\gamma^{t - k}, &amp; \text{if $t \ge k$} \\
\end{cases} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The accumulated online update for $s$ is&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\sum_{t=1}^{T-1}\alpha\delta_t E_t(s) &amp; = \sum_{t=k}^{T-1}\gamma^{t-k}\delta_t \\
&amp; = \delta_k + \gamma\delta_{k+1} + ... + \gamma^{T-1-k}\delta_{T-1} \\
&amp; = R_{k+1} + \gamma V(S_{k+1}) - V(S_k) + \gamma R_{k+2} + \gamma^2 V(S_{k+2}) - \gamma V(S_{k+1}) + ... \\
&amp; + \gamma^{T-1-k} R_{T-1} + \gamma^{T-k} V(S_T) - \gamma^{T-1-k} V(S_{T-1}) \\
&amp; = R_{k+1} + \gamma R_{k+2} + \gamma^2 R_{k+3} + ... + \gamma^{T-1-k} R_{T-1} - V(S_k) \\
&amp; = G_k - V(S_k)
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Thus the update function for TD(1) is the same as the one in every visit MC (where we use $\alpha$ as a learning rate instead of the original one).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The good thing for TD($lambda$) is that it can learn with incomplete experience. And the update is performed &lt;em&gt;online&lt;/em&gt;, &lt;em&gt;step by step&lt;/em&gt; within the episode. MC is updated via offline, cause it needs to wait until the end and calculate the update for each state and update them in batch.&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Aug 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lession-3</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lession-3</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 2</title>
        <description>&lt;p&gt;In the last post, we introduced the definition of Markov Decision Process and Bellman Equation. Now, if you are given the states $S$, action $A$, transition matrix $P$, rewards $R$ and discounting ratio $\gamma$, how would you come up with a solution for this MDP? i.e. how would you calculate the value function and come up with an optimal policy for it?&lt;/p&gt;

&lt;h4 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h4&gt;
&lt;p&gt;This first method is to apply the Bellman Optimality Equation repeatedly. The idea is that we continue update the best estimation for each state value function, and once all $s^\prime$ reachable from $s$ achieve its optimal value function, then $v(s)$ can also achieve the optimal value. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initiate $v(s)$ to 0 for all $s\in S$&lt;/li&gt;
  &lt;li&gt;Apply
&lt;script type=&quot;math/tex&quot;&gt;v(s) = max_a(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^av(s))&lt;/script&gt;
to update each state value function to a better estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why this algorithm guarantee to find the optimal state value function (thus optimal policy)? Its because the Bellman Optimality Equation can be regarded as a contraction. We can image in a value function space, where its dimension is $|S|$, each point in this space determine a value state function. A contraction is an operation that can make two points in this space closer.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;(Contraction Mapping Theory) For any metric space that is complete under an operator that is a contraction, the operator will converge to a unique fixed point.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;According to the Contraction Mapping Theory, we know that $v^\ast=Tv^\ast$ has a unique solution. And based on value iteration converge, we know that $v_{t}=Tv_{t-1}$. Then we could have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||v_t - v\ast||_\infty = ||Tv_{t-1} - Tv\ast||_\infty \le \gamma ||v_{t-1} - v\ast||_\infty&lt;/script&gt;

&lt;p&gt;By applying the operator repeatedly, we are bringing our estimated value function closer and closer to the fixed point, thus we are achieving the optimal value function gradually.&lt;/p&gt;

&lt;p&gt;To prove Bellman Optimality Operator is a contraction, we can have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
|Tv_1(s) - Tv_2(s)|
&amp; = |max_a(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^av_1(s)) - max_a(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^av_2(s))| \\
&amp; \le max_a|(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^av_1(s)) - (R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^av_2(s))| \\
&amp; = max_a|\gamma\sum_{s^\prime\in S}P_{ss^\prime}^av_1(s) - \gamma\sum_{s^\prime\in S}P_{ss^\prime}^av_2(s)| \\
&amp; \le \gamma max_s|v_1(s) - v_2(s)|\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h4&gt;
&lt;p&gt;Compared with value iteration which focus on computing the optimal value function. Policy iteration evaluate a policy and improve the policy gradually, and finally converge to the optimal policy, the algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize a random policy $\pi$&lt;/li&gt;
  &lt;li&gt;Apply Bellman Expectation Equation to all state $s$ to get the current value function $v^\pi$&lt;/li&gt;
  &lt;li&gt;Improve the current policy greedily by:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^\prime = argmax_a (R_s^a + \gamma \sum_{s^\prime\in S}P_{ss^\prime}^av^\pi(s))&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Repeat until the policy does not change&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why policy iteration guarantee to converge to optimal policy? First, we can also proof that the Bellman Expectation Operator(Equation) is also a contraction. Thus given a policy $\pi$, we know that the value function will converge to $v^\pi$. Then, we only need to prove the policy can be improved by our greedy selection.&lt;/p&gt;

&lt;p&gt;Suppose a deterministic policy $a = \pi(s)$. We can improve it by acting greedily by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^\prime(s) = argmax_aq_\pi(s, a)&lt;/script&gt;

&lt;p&gt;according to the current action value function(remember the relationship between action value function and state value function, they can be transformed each other). It can improve the value function for any state&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\pi(s, \pi^\prime(s)) = argmax_a q_\pi(s, a) \ge q_\pi(s, \pi(s)) = v_\pi(s)&lt;/script&gt;

&lt;p&gt;And thus we can improves the value function $v_{\pi^\prime}(s) \ge v_\pi(s)$ (this can be proved by expanding the return and recursively substitute the above function).&lt;/p&gt;

&lt;p&gt;Policy iteration is pretty similar to Expectation Maximization (EM). In EM, we first evaluate the data using the current parameters, and then update the parameters to maximize the quantity.&lt;/p&gt;

&lt;p&gt;More detailed proof is available &lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/academic/class/15780-s16/www/slides/mdps.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Aug 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lesson-2</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lesson-2</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 1</title>
        <description>&lt;p&gt;This is the first post for the series reinforcement learning. The main source for the entire series is &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot;&gt;here&lt;/a&gt;. The post mainly focus on summarizing the content introduced in the video and slides, as well as some of my own understanding. Any feedback is welcomed.&lt;/p&gt;

&lt;p&gt;In this post, we will talk about Markov Decision Process (MDP), which is a pretty fundamental model in many reinforcement learning cases.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Almost all RL problems can be formalized as MDP&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;markov-process&quot;&gt;Markov Process&lt;/h4&gt;
&lt;p&gt;In order to learn about MDP, we need to first know what is Markov Process (MP). This introduces the following two concept:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Markov Property&lt;/li&gt;
  &lt;li&gt;State Transition Matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the most simple word, &lt;strong&gt;Markov Property&lt;/strong&gt; means that the future state is independent on the history given the current state. It can be formalized using following statement:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{P}[S_{t+1}|S_{t}] = \mathbb{P}[S_{t+1}|S_1, ..., S_t]&lt;/script&gt;

&lt;p&gt;This means that the current state contains all they necessary information for the future, and we can discard all history information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;State Transition Matrix&lt;/strong&gt; contains the probability we go from on state to another one. Given a state $s$ and its successor state $s^\prime$, the probability from $s$ goes to $s^\prime$ is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{ss\prime} = \mathbb{P}[S_{t+1}=s\prime|S_{t}=s]&lt;/script&gt;

&lt;p&gt;And the State Transition Matrix is by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P =
\begin{Bmatrix}
P_{11} &amp; ... &amp; P_{1n} \\
\vdots &amp; ... &amp; \vdots \\
P_{n1} &amp; ... &amp; P_{nn}
\end{Bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;From the above two concept, we can notice two things and these are also the constraint for MDP:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The state is finite (otherwise the definition of State Transition Matrix is problematic)&lt;/li&gt;
  &lt;li&gt;The environment is fully observable, no hidden state exists&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can obtain a definition for MP as a tuple $&amp;lt;S, P&amp;gt;$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$S$ is a finite state set&lt;/li&gt;
  &lt;li&gt;$P$ is a state transition matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example of MP:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/mdp.png&quot; alt=&quot;Markov Process&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;markov-reward-process&quot;&gt;Markov Reward Process&lt;/h4&gt;
&lt;p&gt;Markov Process combined with values, then we have Markov Reward Process (MRP), defined by a tuple $&amp;lt;S, P, R, \gamma&amp;gt;$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$S$ is a finite state set&lt;/li&gt;
  &lt;li&gt;$P$ is a state transition matrix&lt;/li&gt;
  &lt;li&gt;$R$ is a reward function,
&lt;script type=&quot;math/tex&quot;&gt;R_{s} = \mathbb{E}[R_{t+1}|S_{t}=s]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;$\gamma$ is a discounting ratio&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we have introduced reward, we can measure how many rewards we can get in each state. We define return $G_t$ as the discounted rewards we can get from timestamp $t$, and state value function $v(s)$ the expected return we can get starting from state $s$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=1}^{\infty}\gamma^{k}R_{t+k+1}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v(s) = \mathbb{E}[G_t|S_t=s]&lt;/script&gt;

&lt;p&gt;Note that the return is accumulated with discounting. This is important is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Avoid infinity loop that might exist in MDP (e.g. self loop)&lt;/li&gt;
  &lt;li&gt;Model the uncertainty about the future&lt;/li&gt;
  &lt;li&gt;From common sense, human prefer immediate reward than long term ones&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can breakdown the state value function into two parts, immediate and long term. Using recurse, we have the Bellman Equation for MRP:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t=s]&lt;/script&gt;

&lt;p&gt;By expanding the above expectation and using sum to replace expectation operator, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v(s) = R_s + \gamma\sum_{s^\prime\in S}P_{ss^\prime}v(s^\prime)&lt;/script&gt;

&lt;h4 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h4&gt;
&lt;p&gt;Adding the actions we can make among the state, we finally have the definition for MDP, which is $&amp;lt;S, A, P, R, \gamma&amp;gt;$$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$S$ is a finite state set&lt;/li&gt;
  &lt;li&gt;$A$ is a finite action set&lt;/li&gt;
  &lt;li&gt;$P$ is a state transition matrix,
&lt;script type=&quot;math/tex&quot;&gt;P_{ss^\prime}^a = \mathbb{P}[S_{t+1}=s^\prime|S_{t}=s, A_t=a]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;$R$ is a reward function,
&lt;script type=&quot;math/tex&quot;&gt;R_{s}^a = \mathbb{E}[R_{t+1}|S_{t}=s, A_t=a]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;$\gamma$ is a discounting ratio&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notice the change on the state transition matrix, before we only have a single matrix, and now we have one for each action $a$ (we can think of in the pervious case, we have only single action). Under different action $a$, the transition probability can be different between the two state. We can now regard the new state transition matrix as a tensor with three dimension.&lt;/p&gt;

&lt;p&gt;As we have actions to now, we need to make decision how to take actions. A &lt;strong&gt;policy&lt;/strong&gt; is a distribution over actions given a state:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]&lt;/script&gt;

&lt;p&gt;A policy fully determines how an agent would act, and it does not depend on the history. Similar to MRP, we have state value function for MDP as the expected return starting from $s$, following the policy $\pi$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s]&lt;/script&gt;

&lt;p&gt;We can also define an action value function, which is the expected return we get starting from state $s$, taking action $a$ and following policy $\pi$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a]&lt;/script&gt;

&lt;p&gt;These two functions have relationship and can be transformed to each other easily, to get state value function, we can summation over all the action value function for the action that state we could get, weight by the policy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_\pi(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s, a)&lt;/script&gt;

&lt;p&gt;And the action value function can be obtained by summation overall all state we can transition to, weighted by the state transition matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\pi(s, a) = R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a v(s^\prime)&lt;/script&gt;

&lt;p&gt;Combine the above equations, we can obtain the Bellman Exception Equation as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_\pi(s) = \sum_{a\in A}\pi(a|s)(R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a v(s^\prime))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\pi(s, a) = R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a \sum_{a\in A}\pi(a|s^\prime)q_{\pi}(s^\prime, a)&lt;/script&gt;

&lt;p&gt;Given a MDP, if we want to solve it (to know what’s the best performance we can get, e.g. What’s the maximum rewards we can get in the terminate state), we need to find the optimal value function for it. As long as we have obtain the optimal value function, we can compose an optimal policy easily:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\pi_{*}(a|s) =
\begin{cases}
1,  &amp; \text{if $a = argmax_{a\in A} q(s, a)$} \\
0, &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;There exists an optimal policy $\pi_{*}$ that is better than or equal to all other policy for any MDP. All optimal policy achieve optimal state value function and optimal action value function&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Following this policy, we can change our Bellman Exception Equation to Bellman Optimality Equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_*(s)=max_{a}(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^a v_*(s^\prime))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_*(s, a)=R_s^a + \gamma\sum_{s^\prime\in S} argmax_{a} P_{ss^\prime}^a v_*(s^\prime)&lt;/script&gt;

&lt;p&gt;Bellman Optimality Equation is non-linear, there is no closed form solution for it. However, we can solve it by some iterative methods (will introduce in later lectures):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Policy Iteration&lt;/li&gt;
  &lt;li&gt;Value Iteration&lt;/li&gt;
  &lt;li&gt;Q-learning&lt;/li&gt;
  &lt;li&gt;Sarsa&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 13 Aug 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Reinforcement-Learning-Lesson-1</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Reinforcement-Learning-Lesson-1</guid>
      </item>
    
      <item>
        <title>Math Equation</title>
        <description>&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;$$&lt;/code&gt; to write math equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{A}\textbf{B} = \textbf{C}&lt;/script&gt;

&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;\begin{equation}&lt;/code&gt; to write math equation
\begin{equation}
\sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}
\end{equation}&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  &amp; \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  &amp; (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) &amp; \cdots &amp; \phi(e_1, e_n) \\
      \vdots &amp; \ddots &amp; \vdots \\
      \phi(e_n, e_1) &amp; \cdots &amp; \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(n) =
\begin{cases}
n/2,  &amp; \text{if $n$ is even} \\
3n+1, &amp; \text{if $n$ is odd}
\end{cases} %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Sat, 12 Aug 2017 00:00:00 -0700</pubDate>
        <link>pyemma.github.io//posts/Math-Equation</link>
        <guid isPermaLink="true">pyemma.github.io//posts/Math-Equation</guid>
      </item>
    
  </channel>
</rss>
