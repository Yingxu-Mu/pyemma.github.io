<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayarea Coding Monkey</title>
    <description>The effort a coding monkey paid when he managed to become a coding machine.</description>
    <link>https://pyemma.github.io</link>
    <atom:link href="https://pyemma.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>How Workflow Get Scheduled via Plugins in Flyte</title>
        <description>&lt;p&gt;Reading open source code has been a recommended approach for software engineers to learn. However, in my past 8 years career, I didn’t do a good job on that. After working in a startup for 1 year, I accidentally foster the habit to read open source code XD. In this post, I would like to share one open source project I have been learning recently, and hope you would enjoy this journey as well.&lt;/p&gt;

&lt;p&gt;I have been working in ML pipelining for a long time in Meta Ads. However, I didn’t have a comprehensive understanding across the entire stack, especially on how the underlying infra schedule the model training job and execute it over a fleet of machines. Recently, I have been exposed to an open source project: &lt;a href=&quot;https://github.com/flyteorg/flyte&quot;&gt;Flyte&lt;/a&gt;, which is a orchestrator for ML pipeline built on top of Kubernetes. I think this might be a good opportunity for me to gain some deep understanding in this area.&lt;/p&gt;

&lt;p&gt;I have always been a believer of “Learning by Doing”. My ultimate goal on learning this open source code is to implement a simplified version of ML pipeline orchestrator on my own. Next, let’s see what problem we are going to discuss in this post.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;In Flyte, we could use something called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plugin&lt;/code&gt; for distributed training, e.g. &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/master/flyteplugins/go/tasks/plugins/k8s/kfoperators/pytorch/pytorch.go&quot;&gt;PyTorch Plugin&lt;/a&gt;. In this post, we would discuss how these plugins are getting invoked, so that the distributed training job we defined could get executed. In this post, I would simplify the discussion and only laser eye on the main flow, for other important topics such as storage, pipeline definition and compilation, availability and scalability, I plan to defer it to later posts.&lt;/p&gt;

&lt;h2 id=&quot;high-level-architecture&quot;&gt;High level architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/flytepropeller_architecture.png&quot; alt=&quot;FlytePropeller Design&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The key component that is responsible for scheduling and monitoring the workflow in Flyte is called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlytePropeller&lt;/code&gt;. It tries to push &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlyteWorkflow&lt;/code&gt;, which is defined as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Custom Resource Definition&lt;/code&gt; in k8s, to the desired state leveraging k8s reconcile mechanism. The official document of Flyte has provided a pretty good &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/d2614d416cd2565dc6a91f0ccff63f7a0dbdf970/docs/concepts/component_architecture/flytepropeller_architecture.rst&quot;&gt;high level architecture&lt;/a&gt; on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlytePropeller&lt;/code&gt;’s design, here is a list of the core components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Controller&lt;/strong&gt;: overall brain of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlytePropeller&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;WorkQueue/WorkerPoll&lt;/strong&gt;: where worker lives and take jobs to do, a very classic design in job scheduling system&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;WorkflowExecutor&lt;/strong&gt;: responsible for high-level workflow operations, such as tracking the status of workflow&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NodeExecutor&lt;/strong&gt;: responsible for process the node within the workflow and decide the action need to take&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NodeHandler&lt;/strong&gt;: different type of handler to execute different type of node in the workflow, e.g. TaskHandler for execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plugins&lt;/code&gt; and WorkflowHandler to execute embedded sub-workflows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Knowing what to do is one thing, and knowing how to do is another thing! Next, let’s jump into the code and see how these components are working internally and see how the logic defined within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plugin&lt;/code&gt; could be invoked.&lt;/p&gt;

&lt;h2 id=&quot;components-deep-dive&quot;&gt;Components Deep Dive&lt;/h2&gt;

&lt;h3 id=&quot;controller&quot;&gt;Controller&lt;/h3&gt;

&lt;p&gt;Let’s get our journey starts with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controller&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Controller&lt;/code&gt; is the starter for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlytePropeller&lt;/code&gt;, it is responsible for initializing other components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New&lt;/code&gt; function of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controller&lt;/code&gt;, we would create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workqueue&lt;/code&gt; &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L422&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;And then, we would create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt; &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L461&quot;&gt;here&lt;/a&gt;. Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt; requires the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workqueue&lt;/code&gt; we have created before as part of its initialization (because worker needs to consume the jobs from the queue), and one &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PropellerHandler&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;notably, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PropellerHandler&lt;/code&gt; is &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L455&quot;&gt;initialized with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WorkflowExecutor&lt;/code&gt;&lt;/a&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WorkflowExecutor&lt;/code&gt; is &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L442-L455&quot;&gt;composed of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodeExecutor&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodeExecutor&lt;/code&gt; requires a &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L436&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeHandlerFactory&lt;/code&gt;&lt;/a&gt; as part of the construction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As of now, all the key components we have mentioned in the high level architecture is ready. We would go deeper into them to understand how are they getting invoked.&lt;/p&gt;

&lt;p&gt;Besides the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New&lt;/code&gt; function, there is also a &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L106&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt;&lt;/a&gt; function which plays a critical role on launching the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controller&lt;/code&gt;. It launches things such as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gc&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metrics monitors&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; function is called within another function &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/controller.go#L93-L103&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Run&lt;/code&gt;&lt;/a&gt;, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Run&lt;/code&gt;, one interesting part is that it is going to leverage the &lt;strong&gt;leader election&lt;/strong&gt; functionality provided by k8s and only let leader to trigger &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; function. We would discuss this topic more in details in a future post.&lt;/p&gt;

&lt;p&gt;As the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controller&lt;/code&gt; would launch &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt;, let’s then move our view to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workqueue&lt;/code&gt; to understand how these 2 components work.&lt;/p&gt;

&lt;h3 id=&quot;workerpoolworkqueue&quot;&gt;WorkerPool/WorkQueue&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt; essentially is composed of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workqueue&lt;/code&gt; and several &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workers&lt;/code&gt;, each are actually goroutines (this is also why Flyte could be pretty scalable on a single CPU, we would discuss this in the future). The &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workers.go#L128&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Run&lt;/code&gt; function&lt;/a&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workerpool&lt;/code&gt; is the most critical function, which is the one get invoked by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;controller&lt;/code&gt;. The main logic is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;for loop&lt;/code&gt; &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workers.go#L144-L153&quot;&gt;here&lt;/a&gt;, where we launch multiple goroutines and each goroutine would make a call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runWorker&lt;/code&gt; function. The &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workers.go#L113&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runWorker&lt;/code&gt;&lt;/a&gt; function is relatively simple, just an endless while loop to call &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workers.go#L42&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;processNextWorkItem&lt;/code&gt;&lt;/a&gt; function. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;processNextWorkItem&lt;/code&gt; function gets an item from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workqueue&lt;/code&gt; and then &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workers.go#L89&quot;&gt;invokes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PropellerHandler&lt;/code&gt;&lt;/a&gt; we perviously passed in during initialization. As we could see, the key processing logic resides within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PropellerHandler&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function, which is defined as part of the interface &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workers.go#L18-L23&quot;&gt;here&lt;/a&gt;, then let’s move on and see how this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; works.&lt;/p&gt;

&lt;h3 id=&quot;propellerhandler&quot;&gt;PropellerHandler&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/handler.go#L180&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function&lt;/a&gt; defined by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Propeller&lt;/code&gt; struct is the entry point of the reconcile process (Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Propeller&lt;/code&gt; has implemented the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; interface, thus it could be considered as type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handler&lt;/code&gt; although there is no explicit inherit, this is how interface implementation works in Golang). The key logic is within this &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/handler.go#L240-L249&quot;&gt;for loop&lt;/a&gt;, where we call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;streak&lt;/code&gt; function up to a max trial. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;streak&lt;/code&gt; function would try to do a single mutation to workflow, and return the mutated workflow upon succeed, otherwise no update made if failed. The workflow here is the CRD &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlyteWorkflow&lt;/code&gt; and the mutation operation is done via &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/handler.go#L298&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TryMutateWorkflow&lt;/code&gt;&lt;/a&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TryMutateWorkflow&lt;/code&gt;  makes calls to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workflowExecutor&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HandleFlyteWorkflow&lt;/code&gt; function to see if we could reconcile the workflow towards it desired status. We left out other details, such as how to handle failure, how to handle aborted workflow etc. From the code in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PropellerHandler&lt;/code&gt;, we could observer that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handler&lt;/code&gt; is just doing some high-level logic and the actual workflow processing logic is delegated to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workflowExecutor&lt;/code&gt;. Now, let’s move to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workflowExecutor&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;workflowexecutor&quot;&gt;WorkflowExecutor&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HandleFlyteWorkflow&lt;/code&gt; function called within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PropellerHandler&lt;/code&gt; is a router function. It invokes other actual logic function based on the status of the workflow. For example, if the workflow status is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WorkflowPhaseRunning&lt;/code&gt;, then it would invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleRunningWorkflow&lt;/code&gt; function. In these functions, a common pattern is that they would setup the context, invoke &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workflow/executor.go#L175&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeExecutor&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecursiveNodeHandler&lt;/code&gt; function to get the new status&lt;/a&gt; and then update the status. The new status is passed back and used to transit the workflow’s status (which is the reconcile process). Notice that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlyteWorkflow&lt;/code&gt; is passed as parameters for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;executors.DAGStructure&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;executors.NodeLookup&lt;/code&gt;, as well as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;startNode&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There is some different operation based on the new status &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecursiveNodeHandler&lt;/code&gt; passed back. For example, if the new status is partial completed, then the workflow would be enqueue again and return &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/workflow/executor.go#L194-L197&quot;&gt;running status&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WorkflowExecutor&lt;/code&gt; handles the operation of workflow and decided what action to take. In ML pipeline, we know that workflow is usually composed by several nodes, and these nodes encapsulate the actual computation. Let’s take a look at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeExecutor&lt;/code&gt;, which is responsible for handling this part.&lt;/p&gt;

&lt;h3 id=&quot;nodeexecutor&quot;&gt;NodeExecutor&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/nodes/executor.go#L177&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecursiveNodeHandler&lt;/code&gt; function&lt;/a&gt; is one of the most important function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodeExecutor&lt;/code&gt;. It is the entry point to execute a node within a workflow. It uses actor model and modified version of DFS to traverse the DAG and to execute non-blocked nodes. Based on different status queried based on the starter node passed from input, it applies different logic to proceed. For example, if the node status is already succeed, skipped or recovered, then it would invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleDownstream&lt;/code&gt; function; while if the node is in status that could be handled, then the key logic happens &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/nodes/executor.go#L229-L234&quot;&gt;here&lt;/a&gt;: first, based on the node’s kind, a dedicated handler is retrieved from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeHandlerFactory&lt;/code&gt;; then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HandleNode&lt;/code&gt; function would be invoked to execute the node.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/nodes/executor.go#L266&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleDownstream&lt;/code&gt;&lt;/a&gt; is where the aforementioned &lt;em&gt;modified DFS&lt;/em&gt; implemented. The &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/nodes/executor.go#L269-L300&quot;&gt;logic&lt;/a&gt; is relatively straightforward: starting from the input node, we retrieve all downstream nodes; then we iterate each node and invoke the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecursiveNodeHandler&lt;/code&gt; function on each of them, with self as the new input start node; keep the status to check if all downstream nodes have been processed, and return the status accordingly.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HandleNode&lt;/code&gt; function of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeExecutor&lt;/code&gt; is also a router function, where different processing function is invoked based on the status of the current node. The most important functions are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleQueuedOrRunningNode&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleNotYetStartedNode&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/96bbf7ed5331f7d629763a3721e90a1e35215da9/flytepropeller/pkg/controller/nodes/executor.go#L941&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleNotYetStartedNode&lt;/code&gt;&lt;/a&gt;, the most critic logic is the call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preExecute&lt;/code&gt;, where we check if the node could be queued to be further processed. The checking logic is relative simple, where we check the upstream nodes are all in succeed status or not&lt;/li&gt;
  &lt;li&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleQueuedOrRunningNode&lt;/code&gt;, we would first try to check if there are cached result given the current handler, and &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/executor.go#L1132&quot;&gt;trigger the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execute&lt;/code&gt; function&lt;/a&gt; if there is no cache hit. The core part of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execute&lt;/code&gt; function is to &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/executor.go#L828&quot;&gt;trigger the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt;&lt;/a&gt; function of the input &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodeHandler&lt;/code&gt;, which is obtained from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecursiveNodeHandler&lt;/code&gt; and passed along the stack &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/executor.go#L229&quot;&gt;here&lt;/a&gt;, what a long journey!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, we have hit the most underground part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FlytePropeller&lt;/code&gt;’s architecture. Next, we need to dive into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodeHandler&lt;/code&gt; to understand how the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function is implemented (here we would focus on how the handler used to fulfill the operations we need in distributed training).&lt;/p&gt;

&lt;h3 id=&quot;nodehandler&quot;&gt;NodeHandler&lt;/h3&gt;

&lt;p&gt;From the section above, we know that we retrieve node handler from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodeHandlerFactory&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecursiveNodeHandler&lt;/code&gt;, through the &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/factory/handler_factory.go#L43&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetHandler&lt;/code&gt; function&lt;/a&gt;. Here is a step by step explanation on how we trigger the logic defined within plugins:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetHandler&lt;/code&gt; function returns node handler based on the type of the node. Most of training job is defined via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@task&lt;/code&gt;, which is of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task&lt;/code&gt; type in Flyte&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/factory/handler_factory.go#L64&quot;&gt;Here&lt;/a&gt; is the setup of the node handler for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task&lt;/code&gt; type. In Flyte, all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task&lt;/code&gt; is treated as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamic&lt;/code&gt; node and handle through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamic&lt;/code&gt; node handler. However, we would still &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/factory/handler_factory.go#L52&quot;&gt;pass a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt; node handler into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamic&lt;/code&gt; node handler&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamic&lt;/code&gt; node handler’s &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/dynamic/handler.go#L175&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function&lt;/a&gt;, by default, we would make a call to &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/dynamic/handler.go#L62&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handleParentNode&lt;/code&gt;&lt;/a&gt;, and in this function, we would make a call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TaskNodeHandler&lt;/code&gt; interface’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function&lt;/li&gt;
  &lt;li&gt;The logic of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt; node handler’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function is pretty complex. First, it tries to &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/handler.go#L537&quot;&gt;find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plugin&lt;/code&gt; based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt; type&lt;/a&gt;; then if there is no cache hit on result, it would &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/handler.go#L583&quot;&gt;invoke plugin&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invokePlugin&lt;/code&gt; function, the core part is to &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/handler.go#L397&quot;&gt;invoke the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ResolvePlugin&lt;/code&gt; search plugins through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pluginsForType&lt;/code&gt;, where we initialized within the &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/handler.go#L280&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Setup&lt;/code&gt; function&lt;/a&gt;; the initialization is essentially sweeping the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enabledPlugins&lt;/code&gt;, and we get it from &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/handler.go#L233&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WranglePluginsAndGenerateFinalList&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WranglePluginsAndGenerateFinalList&lt;/code&gt; function, we get all plugins related to k8s through &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/plugin_config.go#L56&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginRegistryIface&lt;/code&gt; interface&lt;/a&gt;; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task&lt;/code&gt; node handler, there is a data member &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pluginRegistry&lt;/code&gt; of this type, and the construction is &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/handler.go#L912&quot;&gt;here&lt;/a&gt;, where we call the &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flyteplugins/go/tasks/pluginmachinery/registry.go#L23&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginRegistry&lt;/code&gt; function&lt;/a&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pluginMachinery&lt;/code&gt; module&lt;/li&gt;
  &lt;li&gt;For each k8s plugin, &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/plugin_config.go#L56-L72&quot;&gt;they would be wrapped within a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginEntry&lt;/code&gt;&lt;/a&gt;, which is further &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/plugin_config.go#L68&quot;&gt;wrapped in an object called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NewPluginManagerWithBackOff&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;All k8s plugin would use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RegisterK8sPlugin&lt;/code&gt; function within the module &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pluginmachinery.register&lt;/code&gt; to register them into the system. For example, the Pytorch Plugin is registered &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flyteplugins/go/tasks/plugins/k8s/kfoperators/pytorch/pytorch.go#L223&quot;&gt;here&lt;/a&gt;. However, all of these plugins actual do not provide a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function, which should be called by the node handler. What happened?&lt;/li&gt;
  &lt;li&gt;Actually, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function is implemented within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginManager&lt;/code&gt;. Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginManager&lt;/code&gt; &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flyteplugins/go/tasks/pluginmachinery/core/plugin.go#L45&quot;&gt;implement all interface defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pluginCore.plugin&lt;/code&gt;&lt;/a&gt;, we could treat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginManager&lt;/code&gt; as a plugin to invoke (this is a class &lt;strong&gt;Strategy&lt;/strong&gt; design pattern, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginManager&lt;/code&gt; defines the main logic and expressed via several step functions. And we could use composition to fulfill these step functions with different implementation)&lt;/li&gt;
  &lt;li&gt;Within the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Handle&lt;/code&gt; function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PluginManager&lt;/code&gt;, we would check the current status, if the status is not started, then we would call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launchResource&lt;/code&gt;, otherwise we would call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getResource&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkResourcePhase&lt;/code&gt; to obtain new transition information
    &lt;ul&gt;
      &lt;li&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launchResource&lt;/code&gt; function, we would call &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/k8s/plugin_manager.go#L199&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BuildResource&lt;/code&gt;&lt;/a&gt; function which is defined in the plugin. This function is used to &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flyteplugins/go/tasks/plugins/k8s/kfoperators/pytorch/pytorch.go#L133-L139&quot;&gt;construct a kubeflow job&lt;/a&gt;. Then it &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/k8s/plugin_manager.go#L220&quot;&gt;make a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create&lt;/code&gt; request via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubeClient&lt;/code&gt;&lt;/a&gt; to create this resource&lt;/li&gt;
      &lt;li&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkResourcePhase&lt;/code&gt;, we would &lt;a href=&quot;https://github.com/flyteorg/flyte/blob/7d59f106db997ab22686b1b414228fe323934c48/flytepropeller/pkg/controller/nodes/task/k8s/plugin_manager.go#L283&quot;&gt;call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GetTaskPhase&lt;/code&gt;&lt;/a&gt; to get the current status of the job&lt;/li&gt;
      &lt;li&gt;Here is the point where Flyte is leveraging kubeflow and k8s to request resource and start the training job; both kubeflow and k8s would be huge topics, and I plan to discuss more in details in separate blog&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, we reach the end of our journey and the remaining job is delegated to k8s. What a complex flow!&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this post, we focus our discussion on how Flyte would invoke the distributed training job which is defined through plugin, we could see some common practice that is adopted in the design, such as utilization of queue and multithreading for scalability; separation of workflow executor and node executor for single responsibility principle; factory design for extensibility, etc.&lt;/p&gt;

&lt;p&gt;In next topic, we would focus on the storage used in Flyte, which is also another critical component, as we need to store the status of the workflow, node and even intermediate result; as well as leveraging caching to speed up the execution by avoiding duplicated computation. Once we have a better understanding on the storage part, we could start to evaluate the availability, scalability and persistence of Flyte.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Sep 2024 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Flyte-How-Workflow-Get-Scheduled/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Flyte-How-Workflow-Get-Scheduled/</guid>
      </item>
    
      <item>
        <title>LLM Training 101</title>
        <description>&lt;p&gt;这个是读完这篇综述 &lt;a href=&quot;https://arxiv.org/pdf/2407.20018&quot;&gt;Efficient Training of Large Language Models on Distributed Infrastructures - A Survey&lt;/a&gt; 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;PS: 后续会不定期的更新这篇 blog 来争取与时俱进，同时会有专栏来介绍这篇 blog 里面打算深入研究的项目&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;概念性知识&quot;&gt;概念性知识&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;LLM 训练的一些特点
    &lt;ul&gt;
      &lt;li&gt;模型架构的一致性，基本都是堆的 transformer, 虽然现在有一些不一样的尝试比如 Mamba 和 TTT, 但是主流的模型还是 transformer&lt;/li&gt;
      &lt;li&gt;训练的规模和时间也是空前绝后的&lt;/li&gt;
      &lt;li&gt;Specialized software, 比如 Megatron (这个听说过，去了解一下)&lt;/li&gt;
      &lt;li&gt;LLM 训练的 pipeline 也发生了变化（这一点说的还蛮有道理，我在这个领域有比较多的经验，可以向这个 LLM 的方向研究一下看看有什么机会）。传统的机器学习都是针对某一个问题用对应的数据来训练（domain specific），但是现在 LLM 的主流是在大量的数据做自监督学习，然后再进行 fine-tuning, alignment 等&lt;/li&gt;
      &lt;li&gt;在 LLM 训练的各项因素之中，Communication overhead 是一个主要痛点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LLM 训练的 infrastructure 相关的内容
    &lt;ul&gt;
      &lt;li&gt;PCIe 由于 bandwidth 的问题导致其不是很合适 LLM 的训练，现在更多的是使用专用的链接比如 NVLink 等，同时能使用不同的网络连接拓扑结构来进行进一步的优化，比如 cube-mesh 或者 switch-based fullly-connected&lt;/li&gt;
      &lt;li&gt;The Clos network architecture, commonly known as a Fat-Tree topology, is widely used in LLM training clusters. In a Closbased cluster, each server, equipped with one or more NICs, is organized into racks connected to leaf switches. These leaf switches link to spine switches, providing inter-rack connectivity and forming a pod. The pods are further interconnected with core switches, facilitating any-to-any communication across servers within the cluster.&lt;/li&gt;
      &lt;li&gt;Parallel file systems such as Lustre, GPFS, and BeeGFS are frequently deployed on leading high performance computing systems to ensure efficient I/O, persistent storage, and scalable performance. 听说过 distributed file system, 但是这个 parallel file system 是啥&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;打算去学习了解的框架和技术&quot;&gt;打算去学习了解的框架和技术&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;RDMA&lt;/strong&gt;: 可以去学习了解一下 InfiniBand&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DeepSpeed-Chat&lt;/strong&gt;, parallel strategy
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat&quot;&gt;https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;uses Hybrid Engine to seamlessly switch model partitioning between training and inference, such as using tensor parallelism to improve throughput during inference and using ZeRO or LoRA to improve memory utilization during training, providing outstanding system efficiency for RLHF training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;HuggingFace TRL&lt;/strong&gt;, parallel strategy
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://huggingface.co/docs/trl/en/index&quot;&gt;https://huggingface.co/docs/trl/en/index&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;make full use of various parameter-efficient fine-tuning (PEFT) methods, such as LoRA or QLoRA, to save memory cost, and use a dedicated kernel designed by unsloth to increase the training speed of RLHF.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FlashAttention&lt;/strong&gt;, 内存优化
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;https://github.com/Dao-AILab/flash-attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;an IO-aware tiling algorithm is proposed to reduce the number of memory reads/writes between slow HBM and fast on-chip SRAM based on the online softmax. 看能不能自己实现一遍这个算法，网上应该有一些简化版的 kernel 教程，可以参考学习一下&lt;/li&gt;
      &lt;li&gt;Selective-checkpointing selectively discards the activations of memory-intensive attention modules. FlashAttention fuses the attention module into a single kernel, and also employs selective-checkpointing to reduce memory consumption. 这个看一下具体是怎么做的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FlashAttention 2&lt;/strong&gt;: 内存优化, efficiently handles variable-length inputs by parallelizing the sequence length dimension inseparably
    &lt;ul&gt;
      &lt;li&gt;这个是怎么实现的，去学习一下代码&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FlashAttention 3&lt;/strong&gt;: 内存优化, An interleaved block-wise GEMM and softmax algorithm is redesigned based on FlashAttention-2 to hide the non-GEMM operations in softmax with the asynchronous WGMMA instructions for GEMM. Besides, by leveraging the asynchrony of the Tensor Cores and Tensor Memory Accelerator (TMA), overall computation is overlapped with data movement via a warp-specialized software pipelining scheme. Blockwise Parallel Transformer (BPT) further reduces the substantial memory requirements by extending the tiling algorithm in FlashAttention to fuse the feedforward network
    &lt;ul&gt;
      &lt;li&gt;需要学习了解一下 WGMMA, Tensor Cores, Tensor Memory Accelerator, Blockwise Parallel Transformer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Triton&lt;/strong&gt;, 用来写 kernel, 计算优化，听说现在很多公司内部在大量的使用这个写 Kernel, 可以学习一下 #kernel #CUDA
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/triton-lang/triton&quot;&gt;https://github.com/triton-lang/triton&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ZERO&lt;/strong&gt;, 通过 fully sharding 来进行内存优化, ZERO1, 2, 3
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.02054&quot;&gt;https://arxiv.org/pdf/1910.02054&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;ZeRO-3 employs per-parameter sharding to shard the full model and utilizes All-Gather and ReduceScatter for unsharding and sharding communication, respectively&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/&quot;&gt;&lt;strong&gt;ZERO++&lt;/strong&gt;&lt;/a&gt; 感觉也算是 ZERO 家族的一员，但是是一种 partial sharding 的办法，在 ZERO3 的基础之上, further introduces a secondary shard of parameters within subgroups of GPUs and uses quantization to compress parameters and gradients, effectively diminishing communication volume with a trade-off in accuracy&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2101.06840&quot;&gt;&lt;strong&gt;ZeRO-Offload&lt;/strong&gt;&lt;/a&gt; concentrates on multi-GPU training. It holds model parameters on GPU, and stores optimizer states and gradients on CPU memory. In addition, it offloads optimizer update computation to the CPU.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ring AllReduce&lt;/strong&gt; 算法: &lt;a href=&quot;https://github.com/baidu-research/baidu-allreduce&quot;&gt;https://github.com/baidu-research/baidu-allreduce&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.05799&quot;&gt;&lt;strong&gt;Horovod&lt;/strong&gt;&lt;/a&gt;: replaced the Baidu ring-AllReduce implementation with NCCL and designed a user-friendly interface for distributed training&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pytorch DPP&lt;/strong&gt;: fuse multiple sequential AllReduce communication operations into a single operation. This method avoids transmitting a large number of small tensors over the network by waiting for a short period of time and then combining multiple gradients into one AllReduce operation during the backward phase. 通信优化的一种办法，可以看看代码学习一下&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FSDP&lt;/strong&gt;: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.06965&quot;&gt;&lt;strong&gt;GPipe&lt;/strong&gt;&lt;/a&gt; 是之前听说过的一种方法，貌似是目前比较流行的方法，但是仍然会在开始和结束的时候有大量的 bubble 出现
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/kakaobrain/torchgpipe&quot;&gt;https://github.com/kakaobrain/torchgpipe&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;一些比较主流的和重要的概念&quot;&gt;一些比较主流的和重要的概念&lt;/h2&gt;

&lt;h3 id=&quot;parallelism-strategy&quot;&gt;Parallelism Strategy&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tensor parallelism&lt;/strong&gt;: partitions the parameter tensors of each layer along multiple dimensions, effectively distributing the model parameters across the available GPUs. 感觉 tensor parallelism 没有 data/model parallelism 那么常见，在工作中没怎么看到用这种方法的
    &lt;ul&gt;
      &lt;li&gt;it is challenging to overlap the communication with computation, necessitating the use of high-bandwidth connections. Consequently, tensor parallelism is more commonly employed in a single GPU node.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline parallelism&lt;/strong&gt;: pipeline parallelism only necessitates the exchange of intermediate tensors at designated cutting points, resulting in less frequent communication requirements, pipeline parallelism 算是比较常用的东西了
    &lt;ul&gt;
      &lt;li&gt;但是 pipeline parallelism 也有两个问题，一个是 pipeline bubble, 一个是 memory consumption imbalance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequence parallelism&lt;/strong&gt;: It divides the input data into multiple chunks along the sequence dimension and each chunk is fed to one GPU for computation. 没怎么听说过这种方法，可以找来一些 code 来学习一下
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1911.02150&quot;&gt;MQA&lt;/a&gt; 和 &lt;a href=&quot;https://arxiv.org/pdf/2305.13245&quot;&gt;GQA&lt;/a&gt; 就是属于这个范畴, 可以好好的学习一下&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.01889&quot;&gt;&lt;strong&gt;Ring Self-Attention&lt;/strong&gt;&lt;/a&gt; leverages sequence parallelism and calculates the self-attention with ring-style communication to scale up the context window of LLM training. It first transmits the key tensors among GPUs to calculate the attention scores in a circular fashion, and then calculates the self-attention output based on the attention scores and value tensors transmitted in a similar fashion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MoE parallelism&lt;/strong&gt;: MoE 的结构在目前主流的 LLM 里面都得到了大量的使用，可以看看下面的这几篇文章里面介绍的针对 MOE 的 parallel strategy 的方法 #MOE
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.16668&quot;&gt;&lt;strong&gt;GShard&lt;/strong&gt;&lt;/a&gt;: extends the idea of MoE to Transformers in distributed settings, where experts are distributed across different workers and collaborates with All-to-All communication&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.05596&quot;&gt;&lt;strong&gt;DeepSpeed-MOE&lt;/strong&gt;&lt;/a&gt;: proposes a new distributed MoE architecture that applies shared experts in each worker and places more experts in deeper layers to balance communication costs with training accuracy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Since General Matrix Multiplications (GeMMs) require the size of all experts’ inputs to be consistent, existing MoE training frameworks often perform token dropping and padding to match the same expert capacity, which wastes computation.
    &lt;ul&gt;
      &lt;li&gt;General Matrix Multiplications (GeMMs) 的工作原理可以参考: &lt;a href=&quot;https://spatial-lang.org/gemm&quot;&gt;https://spatial-lang.org/gemm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Token dropping and padding 的常用方法是什么？有没有具体的实现代码样例&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;针对 MOE parallel strategy 中 communication 的优化
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.03382&quot;&gt;&lt;strong&gt;Tutel&lt;/strong&gt;&lt;/a&gt;: divides the input tensors into groups along the expert capacity dimension and overlaps computation and communication among different groups to hide All-to-All overhead&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Tutel&lt;/strong&gt;: optimizes the All-to-All kernel implementation by aggregating small messages into a single large chunk inside the nodes before exchanging data among different nodes #Batching&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/system/files/atc23-li-jiamin.pdf&quot;&gt;&lt;strong&gt;Lina&lt;/strong&gt;&lt;/a&gt; analyzes the All-to-All overhead of MoE during distributed training and inference systematically and finds that All-to-All latency is prolonged when it overlaps with AllReduce operations. Lina proposes prioritizing All-to-All over AllReduce to improve its bandwidth and reduce its blocking period in distributed training
        &lt;ul&gt;
          &lt;li&gt;很有意思的发现，可以去学习一下原文里面是怎么发现这个问题的，然后应用在自己以后的工作中&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This heterogeneity is also reflected in model architectures, particularly with Reinforcement Learning from Human Feedback (RLHF). Utilizing heterogeneous hardware and diverse model architectures has become essential for the efficient training of LLMs
    &lt;ul&gt;
      &lt;li&gt;再重新学习一下 RLHF，来理解这里面提到的 &lt;strong&gt;异构性&lt;/strong&gt; 的特点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;memory-optimization&quot;&gt;Memory Optimization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.05682&quot;&gt;Rabe&lt;/a&gt; 这篇论文中证明了自注意力只需要 O(logn) 的内存就可以了，学习一下这篇论文里面的工作&lt;/li&gt;
  &lt;li&gt;了解一下 FP16 和 BF16 的工作原理，内存优化&lt;/li&gt;
  &lt;li&gt;LLM training 的过程中主要吃内存的部分
    &lt;ul&gt;
      &lt;li&gt;Model States: Model states encompass the memory consumed by the optimizer states, gradients, and model parameters&lt;/li&gt;
      &lt;li&gt;Activations refer to the tensors generated during the forward pass&lt;/li&gt;
      &lt;li&gt;Temporary Buffers: Temporary buffers are used to store intermediate results&lt;/li&gt;
      &lt;li&gt;Memory Fragmentation: Memory fragmentation can lead to scenarios where memory requests fail despite having a large amount of available memory, 这个在 Pytorch 里面由于内存分配机制会出现这种问题，可以再找一些额外的资料详细的了解一下&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep learning frameworks typically use a caching allocator with a memory pool to enable fast memory allocation and deallocation without requiring device synchronization.&lt;/li&gt;
  &lt;li&gt;一个用来估算所需要的内存的简易办法
    &lt;ul&gt;
      &lt;li&gt;When training a model with Φ parameters,4Φ bytes are needed to store parameters and gradients. The 32-bit copies of the parameters, momentum, and variance each require 4Φ bytes, totaling12Φ bytes. Therefore, the overall memory requirement for storing model states is 16Φ bytes，这个再好好看一下理解一下&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;一些用来进行 Memory 优化的整体大方向
    &lt;ul&gt;
      &lt;li&gt;Activation re-computation strategies, which trade increased computation for reduced memory usage, 这个是现在最主流的方法之一，可以找一些代码来看看是如何实现的，这个方法的一个关键就是节省的内存和额外计算之间的 trade off&lt;/li&gt;
      &lt;li&gt;Redundancy reduction methods that minimize data duplication across training processes&lt;/li&gt;
      &lt;li&gt;Defragmentation techniques that optimize memory allocation and deallocation to reduce fragmentation and improve memory utilization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.08156&quot;&gt;&lt;strong&gt;GMLake&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/stable/notes/cuda.html&quot;&gt;&lt;strong&gt;PyTorch expandable segments&lt;/strong&gt;&lt;/a&gt; propose to mitigate fragmentation by utilizing the virtual memory management (VMM) functions of the low-level CUDA driver application programming interface. 可以看看 PyTorch 里面这个工作&lt;/li&gt;
  &lt;li&gt;Swap and offload approaches that leverage CPU memory and NVMe SSDs to supplement GPU memory
    &lt;ul&gt;
      &lt;li&gt;CPU offloading: static/dynamic&lt;/li&gt;
      &lt;li&gt;SSD offloading, 这个在之前的 GPU training paper 里面好像看到过&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;communication-optimization&quot;&gt;Communication Optimization&lt;/h3&gt;

&lt;p&gt;一些和通信相关的优化&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA’s NCCL and AMD’s RCCL are highly optimized libraries that typically outperform MPI-based collective communication libraries on their respective AI accelerators. These libraries usually select pre-defined algorithms to perform collectives based on conditions such as network topology and input tensor size. 可以去学习一下 NCCL&lt;/li&gt;
  &lt;li&gt;通信的不同算法: &lt;strong&gt;Ring, Tree, Hybrid&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Conventional frameworks simultaneously perform gradient computation for both weights and outputs. &lt;a href=&quot;https://github.com/mlsys-seo/ooo-backprop&quot;&gt;&lt;strong&gt;Out-of-order backpropagation (ooo-backprop)&lt;/strong&gt;&lt;/a&gt; decouples the gradient computations for weights and outputs, scheduling the weight gradient computations flexibly out of their original order. This allows more critical computations to be prioritized and scheduled accordingly. Consequently, ooo-backprop optimizes overall performance by scheduling communications based on this out-of-order computation strategy. 这个工作很有意思，把 activation 和 gradient 的 communication 拆开然后进行类似不同的 priority 的 communication&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;In-network aggregation (INA)&lt;/strong&gt; uses the computational capabilities of network devices to perform aggregation operations like summing gradients of deep learning models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fault-tolerance&quot;&gt;Fault Tolerance&lt;/h3&gt;

&lt;p&gt;Failure tolerance 主流的还是使用 checkpoint&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Synchronous checkpoint&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/system/files/nsdi22-paper-eisenman.pdf&quot;&gt;&lt;strong&gt;Check-N-Run&lt;/strong&gt;&lt;/a&gt; decouples the snapshot and persist phases. It achieves atomic checkpointing by stalling training only during the snapshot phase and asynchronously persisting snapshots using dedicated background CPU processes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.cels.anl.gov/~woz/papers/DeepFreeze_2020.pdf&quot;&gt;&lt;strong&gt;DeepFreeze&lt;/strong&gt;&lt;/a&gt; applies both lightweight (snapshot) and heavy(persist) persistence strategies in the background, sharding checkpoints across data-parallel GPUs to distribute I/O workload.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gemini&lt;/strong&gt; proposes checkpointing to CPU memory for faster failure recovery, along with a checkpoint placement strategy to minimize checkpoint loss and a traffic scheduling algorithm to reduce interference with training.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/system/files/fast21-pan.pdf&quot;&gt;&lt;strong&gt;Tectonic&lt;/strong&gt;&lt;/a&gt;: Meta’s distributed filesystem, enables thousands of GPUs to save and load model checkpoints simultaneously, providing efficient and scalable storage solutions for extensive training operations&lt;/li&gt;
  &lt;li&gt;现在貌似主要用来对 checkpoint 用来存储的都是 object store, 这个可以去研究下看看各个公司都用啥（比如 AWS 是不是都上 S3）&lt;/li&gt;
  &lt;li&gt;Live migration leverages the inherent redundancy present in distributed LLM training setups, particularly the model replicas across different data parallel pipelines, to restore model states in case of failure. 这个感觉其实有点类似使用 Cassandra 里 consistency hashing 里面的 hinted hand off&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 01 Sep 2024 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//LLM-LLM-Training-101/</link>
        <guid isPermaLink="true">https://pyemma.github.io//LLM-LLM-Training-101/</guid>
      </item>
    
      <item>
        <title>读书笔记 - Patterns of Distributed System</title>
        <description>&lt;p&gt;最近读了一本和 distributed system 相关的书籍，介绍了在 distributed system 里面常用的一些 pattern. 这是一篇简要的读书笔记，把书中提到的几个 pattern 总结了下来; 我计划会经常更新这篇 blog, 把我新学习到的或者总结出来的一些 pattern 记录在这里; 希望能起到一个引导性的作用，给大家提供一个提纲挈领的思路&lt;/p&gt;

&lt;h2 id=&quot;patterns&quot;&gt;Patterns&lt;/h2&gt;

&lt;h3 id=&quot;write-ahead-log&quot;&gt;Write Ahead Log&lt;/h3&gt;

&lt;p&gt;把命令存储到一个 append only file 里面去，当挂了之后可以重新读 WAL 来 rebuild 内部的 state #Message-Queue #KV-Store #持久化&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flushing 来保证命令真的写到 physical media，好处是 persistent，代价就是 performance; 可以使用 batching 等方法来进行优化 #Batching
    &lt;ul&gt;
      &lt;li&gt;CRC record 来防止 corrupted entry #CRC&lt;/li&gt;
      &lt;li&gt;Log 里面可能有 duplication，每一个 request 需要一个 unique identifier 来进行区分 #Deduplication&lt;/li&gt;
      &lt;li&gt;可以用来实现 transaction，用来保证原子性 #Transaction&lt;/li&gt;
      &lt;li&gt;工业界里面的具体例子 #RocksDB #Kafka #Cassandra&lt;/li&gt;
      &lt;li&gt;Key/Value pairs that needs atomic store, write into a batch, and then batch is add into data store; the data store first create a WAL entry for the entire batch, once log is created successfully, the batch is added into datastore&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;segmented-log&quot;&gt;Segmented Log&lt;/h3&gt;

&lt;p&gt;把单一的 log file 切分成更多的 log 从而方便对老的数据进行 cleanup; 当数据超过一定的阈值之后就 rollover 到一个新的 log file 里面去, 业界的例子 #Kafka #Cassandra #Raft&lt;/p&gt;

&lt;h3 id=&quot;low-water-mark&quot;&gt;Low Water Mark&lt;/h3&gt;

&lt;p&gt;帮助保证 log 的大小不会无限制的增长，通过 low water mark 这样的一个 index，对 log 进行压缩 (通常是一个 background job 在进行这个操作)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Snapshot-based #Raft&lt;/li&gt;
  &lt;li&gt;Time-based #Kafka&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;leader-and-follower&quot;&gt;Leader and Follower&lt;/h3&gt;

&lt;p&gt;使用单一的 server 来 coordinate 多个 servers 的 replication #Replication&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Small cluster: leader election, #Zab #Raft&lt;/li&gt;
  &lt;li&gt;Large cluster: consistent core, 需要的几个核心功能 #Zookeeper #etcd
    &lt;ul&gt;
      &lt;li&gt;compareAndSwap to set a key atomically&lt;/li&gt;
      &lt;li&gt;heartBeat to expire the key if no heartBeat from leader, and trigger new election&lt;/li&gt;
      &lt;li&gt;notification mechanism to notify all servers if key is expired&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heartbeat&quot;&gt;Heartbeat&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;可以使用 separated thread 来异步发送 heartbeats  #Consul&lt;/li&gt;
  &lt;li&gt;在 large cluster 里面，1-to-1 的 heartbeat messaging 效率太低了，这个时候一般可以考虑使用 Gossip Protocol #Gossip-Protocol
    &lt;ul&gt;
      &lt;li&gt;两种主流的实现方式，Phi Accrual failure detector 和 SWIM #Cassandra #Consul&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;majoruty-quorum&quot;&gt;Majoruty Quorum&lt;/h3&gt;

&lt;p&gt;Flexible quorum, 我们可以通过动态的调整读写的 quorum size 来提高性能，只要能保证读写之间会有一个交集就行; 比如说一共有 5 个 node，然后我们有 90% 的读和 10% 的写，那么我们可以要求读只需要 2 个 quorum, 写需要 4 个 quorum #Quorum #Cassandra&lt;/p&gt;

&lt;h3 id=&quot;generation-clock&quot;&gt;Generation Clock&lt;/h3&gt;

&lt;p&gt;也可以叫做 Term, Epoch, 这个是 Lamport Clock 的一个具体样例 #Lamport-Clock&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each process maintains an integer counter, which is incremented after every action the process performs. Each process also sends this integer to other processes along with the messages processes exchange. The process receiving the message sets its integer counter by choosing the maximum between its own counter and the integer value of the message. This way, any process can figure out which action happened before the other by comparing the associated integers. The comparison is possible for actions across multiple processes as well, if the messages were exchanged between the processes. Actions which can be compared this way are said to be &lt;em&gt;causally related&lt;/em&gt;.
    &lt;ul&gt;
      &lt;li&gt;工业界的例子, Cassandra 里面的 server 在 restart 的时候会自增 1, 这样在 gossip 的 message 里面其他的 server 会知道这个 server restart 了，从而会把关于这个 server 的 stale 的 data drop 掉，然后要新的; Kafka 里面的 epoch number 会存在 Zookeeper 里面，每次一个新的 controller 被 elect 的时候，就会增加这个 epoch number; 同时 leader 也会 maintain 一个 Leader Epoch 来看是否有 follower 太落后了 #Cassandra #Kafka&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;high-water-mark&quot;&gt;High Water Mark&lt;/h3&gt;

&lt;p&gt;也被称作是 &lt;strong&gt;CommitIndex&lt;/strong&gt; #Replication #Raft #Kafka&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Client 最多只能读到这里，因为在 high water mark 之后的 entry 都还没有被 confirm 已经 replicate 了&lt;/li&gt;
  &lt;li&gt;这个在 stream 里面处理 delayed event 时候也叫这个，只不过那个 high water mark 是多等一段时间&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;paxos&quot;&gt;Paxos&lt;/h3&gt;

&lt;p&gt;这个太难了，等以后专门开一个总结一下吧 #Paxos #Consensus-Algorithm #2PC #Quorum #Lamport-Clock&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We can ensure liveness or safety, but not both. Paxos ensure safety first&lt;/li&gt;
  &lt;li&gt;工业界的具体应用: Google Spanner 使用的是 multi-paxos, which is implemented as a replicated log; Cassandra uses basic Paxos to implement lightweight transactions #Spanner #Cassandra&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;replication-log&quot;&gt;Replication Log&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;在 MongoDB 中，每一个 partition 会有一个自己的 replication log #MongoDB #Partition&lt;/li&gt;
  &lt;li&gt;在 Kafka 的 Raft 实现中，使用的是 pull 模式，也就是 follower 从 leader 那里 pull replication log #Kafka #Push-Pull&lt;/li&gt;
  &lt;li&gt;Read request optimization via bypassing the replication log, 可以使用两种不同的方法, 一个是 leader 再发送一个 heartbeat 然后看能不能得到 majority 的回复，来确认自己仍然是 leader; 另一个是使用 leader lease #Read-Optimization #Lease #etcd&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;idempotent-receiver&quot;&gt;Idempotent Receiver&lt;/h3&gt;

&lt;p&gt;client 可能会 retry request, server 端需要进行 deduplication, 这个在多种系统中都很常见 #Event-Aggregation #Payment&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;给每个 client 一个 unique id, 在 server 端进行注册，注册之后 client 才能开始给 server 发送 request; 这个数据也需要被 replicated 从而保证高可用性&lt;/li&gt;
  &lt;li&gt;Expiration of saved request, request number, next request only when received response, number of max in-flight request with request pipeline #Kafka&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;singular-update-queue&quot;&gt;Singular Update Queue&lt;/h3&gt;

&lt;p&gt;一种用来高效处理 concurrent request 的方法，向比较使用 lock 的话效率更高；具体的实现方法就是实现一个 work queue, concurrent 的 request 都放到 queue 里面，但是只有一个 worker thread 来处理 queue，从而实现 one-at-a-time 的保证 #Concurrency #Message-Queue #Coordination&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;工业界的例子有 Zookeeper, etcd, Cassandra&lt;/li&gt;
  &lt;li&gt;可能会用到这个思想的 system design: Booking, Google doc (OT)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;request-waiting-list&quot;&gt;Request Waiting List&lt;/h3&gt;

&lt;p&gt;一个 node 可能要和其他的 node 进行异步的 communication 之后才能返回 request, 保存一个 waiting list 来 map 一个 key 和一个 callback function #Concurrency #异步&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;工业界例子: Kafka 里面的 purgatory 来保存 pending request #Kafka&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;follower-reads&quot;&gt;Follower Reads&lt;/h3&gt;

&lt;p&gt;也就是大名鼎鼎的 read replica; 即使是在用 Raft 这种 consensus 算法来进行 replication 的系统中也会有 replication lag, 因为 leader 需要一个 additional 的 network call 来让所有的 follower 都 commit; read your own write，可以使用 lampart lock 来解决，写了之后传回去一个 version number，再读的时候要带着这个 version number 来看 read replica 上面的 value 是不是已经是更新的了&lt;/p&gt;

&lt;h3 id=&quot;version-number&quot;&gt;Version Number&lt;/h3&gt;

&lt;p&gt;To store versioned key values, a data structure that allows quick navigation to the nearest matching version is used, such as a skip list, 之前在 lucene 里面也看到了这个 skip list，需要研究一下 #数据结构&lt;/p&gt;

&lt;p&gt;在 RocksDB 里面，一个重要的原因需要把 key sorted 的是因为它们 underlaying 存储的都是 bytes array, its important to keep keys sorted when they are serialized into byte arrays&lt;/p&gt;

&lt;h3 id=&quot;version-vector&quot;&gt;Version Vector&lt;/h3&gt;

&lt;p&gt;在 Cassandra 里面，除了 value 以外，还把 timestamp 也当做一个 column 来存储了，从而实现了 LWW，但是代价就是 Cassandra 的 cluster 需要正确的设置 NTP, 否则的话 latest value 仍然可能被 old value 给 overwrite 掉&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果每一个 cluster client 有一个 unique id 的话，那么我们也可以使用 client id 来存 version vector (但是这样的话怎么进行 conflict resolve 呢)&lt;/li&gt;
  &lt;li&gt;一篇 Riak 里面讲针对使用 client id 还是使用 server id 来存储 version vector 的&lt;a href=&quot;https://riak.com/posts/technical/vector-clocks-revisited/index.html?p=9545.html&quot;&gt;文章&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fixed-partition&quot;&gt;Fixed Partition&lt;/h3&gt;

&lt;p&gt;先 create logic shard，然后再把 logic shard map 到 physical shard 上面去; 这些 metadata 都可以通过一个 coordination service 来负责 (分 partition 和 存储相应的 metadata); 另外一种做法是每个 physical node 上面的 partition 数量是固定的，也就是 propositional to number of nodes&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka 里面的每一个  topic 就是一个 fixed size partitions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;clock-bound-wait&quot;&gt;Clock Bound Wait&lt;/h3&gt;
&lt;p&gt;W
hile reading or writing, cluster node wait until the clock values on every node in the cluster are guaranteed to be above the timestamp assigned to the value&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google TrueTime, AWS Time Sync Service, 使用 atomic clock 和 GPS 来确保 clock drift across their cluster node is kept below a few milliseconds&lt;/li&gt;
  &lt;li&gt;这个概念有点复杂，需要再找一个好的资料学习理解一下这里的思想&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lease&quot;&gt;Lease&lt;/h3&gt;

&lt;p&gt;Use time-bound lease for cluster nodes to coordinate their activities, 这个在 GFS 里面就使用到了, 在 Facebook 的 Memcache 里面也有涉及到 Lease 的思想, Lease 一般可以通过一个 coordination core 来实现，由 leader 来进行 lease 的 replication 和 check&lt;/p&gt;

&lt;h3 id=&quot;state-watch&quot;&gt;State Watch&lt;/h3&gt;

&lt;p&gt;可以参考一下是怎么实现的, 在 server 端我们需要存储下来 event 和 client 的 connection, 在 client 端我们要存储 event 和对应的 handler&lt;/p&gt;

&lt;h3 id=&quot;emergent-leader&quot;&gt;Emergent Leader&lt;/h3&gt;

&lt;p&gt;直接用在整个 cluster 里面最老的那个 node 作为 coordinator node, 相比较 consistency core 所采用的 leader election 的方法， favor availability over consistency&lt;/p&gt;

&lt;h3 id=&quot;single-socket-channel&quot;&gt;Single Socket Channel&lt;/h3&gt;

&lt;p&gt;在 follower 和 leader 之间保持一个能够支持 retry 而且能够保证 message order 的通讯，可以通过 TCP 来实现; 在 Kafka 和 Zookeeper 里面使用了这种方式 #Kafka&lt;/p&gt;

&lt;h3 id=&quot;request-batch&quot;&gt;Request Batch&lt;/h3&gt;

&lt;p&gt;把多个 request 放在一起从而提高带宽的利用率; 在 client 端可以 maintain 一个 queue 来维护 request, 然后再放在一个 batch 里面一起发过去 (这个其实跟之前写的 batch commit logger 是一样的)&lt;/p&gt;

&lt;h3 id=&quot;request-pipeline&quot;&gt;Request Pipeline&lt;/h3&gt;

&lt;p&gt;server 在发出去 request 之后不需要等待 response, 又另外一个 thread 来负责接受和处理 response (有点像是 webhook 的思路); 为了防止 request overwhelming, 一般会有一个 upper bound on max in-flight request; 同时针对 retry 和 out-of-order 的 request 也需要针对性的处理 (比如 assign unique request id 等)&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://martinfowler.com/articles/patterns-of-distributed-systems/&quot;&gt;Patterns of distributed system&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 05 Jul 2024 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Book-Pattern-of-Distributed-System/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Book-Pattern-of-Distributed-System/</guid>
      </item>
    
      <item>
        <title>那些年，我们追过的 Feature</title>
        <description>&lt;p&gt;在今天的 blog 里面，我将结合我前一阵子面试 machine learning engineering 的经验，跟大家唠唠在 ML design 里面的 feature engineering 相关的问题。
在 ML design 里面，我们可能会被问到可以使用什么样的 feature, 以及具体一些 feature 可以被怎么处理以及怎么使用在模型之中。由于我之前主要做的是推荐和广告相关的内容，所以我在这里将主要介绍一下在设计推荐系统的时候，围绕 feature 可以聊的一些点，来给大家提供一些思路，帮大家更好的准备面试&lt;/p&gt;

&lt;h2 id=&quot;feature-的种类&quot;&gt;Feature 的种类&lt;/h2&gt;

&lt;p&gt;在推荐系统里面，我们经常要处理的场景是&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;给定一个用户和一个物品（以及一些可能的 context），预测用户会喜欢（或者其他的 action）这个物品的概率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于上面这个简单化的概括（我们在之后会对这个问题进行适当的展开），我们其实可以比较容易的归纳出 feature 的种类&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;User Side Feature&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Item Side Feature&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User-item Interaction Feature&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context Feature&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，还有一些相对比较特殊的 feature, 我们之后会单独介绍&lt;/p&gt;

&lt;h3 id=&quot;user-side-feature&quot;&gt;User Side Feature&lt;/h3&gt;

&lt;p&gt;这种类型的 feature 有时候也会被称作 request level feature, 针对推荐系统来说，一个 request 基本上就代表了一次用户请求，所以这么叫没什么毛病。在 user side feature 里面，有下面几种比较常见的形式&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Demographic feature&lt;/em&gt;, 也就是常说的一些基本用户属性，比如用户的年龄，性别，职业，local 等等; 这些 feature 是最最常用的 feature 了，不过由于现在针对 Machine Learning fairness 查的严，很多这种 demographic feature 被禁止用于推荐了&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;User behavior feature&lt;/em&gt;, 也就是用户的一些行为特征，比如用户过去一个月买过的物品 id，用户过去一周 follow 别人的 account 的个数，用户的历史 CTR 等等
    &lt;ul&gt;
      &lt;li&gt;这里其实列举了三种不同的“数据类型”，也是在推荐系统中 feature 的常见形式: id feature (比如用户 like 过的 post id, 用户之前 follow 过的用户的 id 等等), count feature (比如 follow 的个数，click 的个数等等), ratio feature (比如 CTR, CVR 等等)&lt;/li&gt;
      &lt;li&gt;一般针对 user behavior feature, 我们都会有一个窗口来进行 aggregation (比如过去一周的 sum 或者 average; 在工程实现的时候，经常是多个窗口一起实现了，比如 7D, 14D 等，因为可以通过一个 pipeline 就全部搞出来，比较方便高效); 随着模型技术的进步，现在有很多尝试开始用 user sequential behavior modeling 来取代这种 feature engineering 的方法了, 比如 &lt;a href=&quot;https://arxiv.org/pdf/1706.06978&quot;&gt;DIN&lt;/a&gt; 直接拿用户的 behavior 数据和 candidate item 做 attention 处理&lt;/li&gt;
      &lt;li&gt;User 的 behavior 多种多样，这部分需要针对具体的推荐系统所解决的 business problem 来进行处理，是最需要 domain knowledge 的部分; 比如在给用户推荐其他用户来 drive growth 的推荐系统中，你用用户过去点击过的 post id 可能效果就差一些，但是如果用点击过的 post 的 author id 可能就是一个不错的选择&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;User embedding&lt;/em&gt;, 或者叫 user profiling, 也是现在非常主流的一种 user feature, 当在面试中提到这部分的时候，基本上一个 follow up 是如何计算出来 user embedding, 以及如何在 inference 的时候进行 serving, 一般的 user embedding 的计算方法有这么几种:
    &lt;ul&gt;
      &lt;li&gt;直接用 item 本身，或者通过一些 heuristic rule 来表示: 比如说，我们可以把所有的 post 分类成 1024 个 category, 那么根据用户曾经看过的 post, 我们就可以用一个 1024 维度的向量来表示用户, 如果用户看过 GUNDAM 类型的 post, 那么 GUNDAM 类别在 1024 向量中的值就设置成 1, 否则就是 0; 这种方法其实非常类似 NLP 里面以前经常使用的 bag of words 的方法来做 sentence embedding; 这种方法的好处就是实现起来比较简单，但是缺点就在于捕捉到的信息比较粗糙，只能根据我们定义的 heuristic rule 来决定，不能根据用户跟物品的交互学习出一些 semantic 的信息（比如喜欢 GUNDAM 的用户可能对于游戏也感兴趣）&lt;/li&gt;
      &lt;li&gt;一个这对上面的方法提高的方法，是基于 item 的 embedding 去学习一个 user embedding, 这个方法在 Pinterest 的这篇&lt;a href=&quot;https://arxiv.org/abs/2205.04507&quot;&gt;论文&lt;/a&gt;中得到利用，他们基于 PinSage 出来的 item embedding, 结合用户的 sequence engagement 数据，利用一个模型来学习 user embedding&lt;/li&gt;
      &lt;li&gt;通过其他的一些方法来训练出 user embedding, 比如利用全体用户的 interest follow graph 作为数据，利用 collaborative filtering, dedicated model (比如 graph neural network) 来计算出 user embedding, 然后这个 embedding 可以被用作输入送进其他的模型; 比如 Facebook 就采取了用一个专门的模型来对大量的 user feature 进行学习，然后用这个 dedicated model 去生成 user embedding 然后给下游的模型使用，详情可以参考这篇&lt;a href=&quot;https://arxiv.org/pdf/2311.09544&quot;&gt;论文&lt;/a&gt;; 这种方法的好处在于能够学习更具有表征能力的 user embedding, 但是缺点在于 user embedding 需要进行单独的额外维护，以及不能很好的基于 downstream 的 task 来进行微调;&lt;/li&gt;
      &lt;li&gt;将用户 id 作为一个 spares feature 输入到模型里面，然后通过一个 embedding lookup table 来 convert 成一个 dense vector, 然后再和其他的 feature 输入 concat 在一起; 这个就是一个经典的如何在 Neural Network 里面使用 sparse feature 的方法，比如 LinkedIn 的这篇 &lt;a href=&quot;https://www.linkedin.com/blog/engineering/feed/enhancing-homepage-feed-relevance-by-harnessing-the-power-of-lar&quot;&gt;blog&lt;/a&gt;; 这种方法的一个好处是在于 user embedding 和模型是一起训练的，所以 embedding 可以理解为针对某个问题的专门优化，那么代价也显而易见，一反面是模型训练的参数量上升，另一方面对于 user embedding 的 reuse 也不是那么的方便了&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;item-side-feature&quot;&gt;Item Side Feature&lt;/h3&gt;

&lt;p&gt;Item side feature 其实和 user side feature 本质上是类似的，比如我们也可以有一些 item 的属性信息，比如物品, 产地，物品价格等等，或者 item 的一些历史交互数据，比如过去一周的 impression 数量，过于一周的 CTR 等等；同时我们也可以使用和 user embedding 类似的方法来生成 item embedding 然后用作模型输入; Item side feature 的一个特点是 feature value 变化不是很频繁，因此这部分的 feature 一般都会采用一些类似 pre-compute 的方法来提高性能&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一种常见的方法是使用&lt;em&gt;双塔模型&lt;/em&gt;来对 user 和 item embedding 进行训练，然后 user tower 可以在线的去 server request 来实时的计算 user embedding, 但是 item tower 可以离线的把所有的 candidate 的 embedding 计算好然后缓存起来，等需要的时候直接读 pre-compute 好的而不用再实时的计算了&lt;/li&gt;
  &lt;li&gt;相比较 user feature, item 可能也会有一些其他形式的数据可以利用，比如物品的图片，广告语等等，这些数据都可以经过专门的处理然后用作模型输入，比如利用 pre-train model 来把 image 转换成 embedding; 或者利用 object detection 模型来 predict 图片中包含物品的种类信息; 利用 BERT 来做 sentence embedding 等等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Item side feature 比较特殊的一种 feature，是可以利用一些 &lt;em&gt;Owner 的信息&lt;/em&gt;，而不单单是物品本身的信息: 比如说我们要推荐广告，那么我们可以拿 campaign level 或者 advertiser level 的信息(这些信息相比较 ads id 本身可能会更加的 stable, 这个是 ads 里面的一个问题，因为 ads 本身的 TTL 都相对比较短); 如果我们推荐 post, 那么我们可以拿 post owner 的一些信息，比如 post owner 过去一周看过的 post id 等，作为 additional signal 来做推荐&lt;/p&gt;

&lt;h3 id=&quot;user-item-interaction-feature&quot;&gt;User-item Interaction Feature&lt;/h3&gt;

&lt;p&gt;User-item interaction feature 就是具体到 user-item pair 的 signal。 在我们上面所描述的 feature 中，都是根据 user 或者 item 进行了 aggregation; user-item pair 可以理解为是 aggregation 之前的信息，能够给我们提供最 fine-granularity 的 signal， 比如，我们可以看 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_a&lt;/code&gt; 跟 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;item_a&lt;/code&gt; 过去 24 小时的 view count, time spent 等等; 或者 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_a&lt;/code&gt; 跟广告商 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;advertiser_a&lt;/code&gt; 过去 24 个小时的 impression 的数量, CTR 等等&lt;/p&gt;

&lt;p&gt;这一类的 feature 的 serving 是最困难的，原因就在于其 cardinality 非常的大（约等于 O(num of user) x O(num of item) 的数量级），因此我们很少在 early stage (这里其实涉及到了推荐系统里面的多层架构设计，可以理解为我们在层层做 filtering 从而来减少需要考虑的 candidate 的数量) 大量的使用这种类型的 feature&lt;/p&gt;

&lt;h3 id=&quot;context-feature&quot;&gt;Context Feature&lt;/h3&gt;

&lt;p&gt;这种类型的 feature 相对于前三种讨论到的而言不是那么的常见，context feature 有两种不同的理解形式&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一种是在 server 端我们能够拿到的一些 context feature，比如现在很多的推荐系统开始 adopt list ranking, 也就是考虑所推荐的物品彼此之间的影响，把彼此当做 context 来进行优化（彼此都是彼此的缺口 XD）&lt;/li&gt;
  &lt;li&gt;另外一种是在 server 端无法获取，只有在 device 上才能拿到的一些信息，比如用户当前的网络环境， 用户当前手机的电量等等，这些 context feature 也会对用户的行为产生影响，比如如果我的手机没什么电了，我可能会先收藏几个视频等之后再看&lt;/li&gt;
  &lt;li&gt;针对上面说的这两种情况，在快手的这篇&lt;a href=&quot;https://arxiv.org/pdf/2208.09577&quot;&gt;论文&lt;/a&gt;里面都有提及，详情可以参考阅读一下原文&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;其他-feature-的碎碎念&quot;&gt;其他 Feature 的碎碎念&lt;/h3&gt;

&lt;p&gt;除了上面提到的 feature, 还有一些比较特殊的 feature 承担着一些特殊的使命&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Privileged feature, 这种类型的 feature 是属于在 training 的时候 available 但是在 serving 的时候不 available 的，一个例子是在做 CVR 的预测的时候，用户在一个物品上面的逗留时间会是非常 powerful 的一个 signal, 但是我们在给用户展示物品的时候，是需要先预测 CVR 出来对物品排序，然后再给用户展示，所以在 serving 的时候这个逗留时间是不知道的，但是在 training 的时候我们可以根据 client 端的 logging 知道这个信息。在淘宝的这篇&lt;a href=&quot;https://arxiv.org/pdf/1907.05171&quot;&gt;论文&lt;/a&gt;中，他们采用了 teacher-student knowledge distillation 的方法来学习这类 feature&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Privileged-Features-Distillation.png&quot; alt=&quot;Privileged feature&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Position feature, 这个是在推荐系统中非常常考的一个 follow up. 在推荐系统中，存在这一种系统性的 bias, 也就是 position bias: 用户点击了某个物品，可能并不是因为我们的推荐做的有多么好，而是单纯的因为这个物品被排在了前面，导致有更多的 impression 引来更多的用户 action. 传统的解决办法是把物品的 position 当做一个 feature 放在模型中跟其他的 feature 一起训练学习，从而能够让模型学习出 position bias, 从而起到一定的 calibration 的作用; 在 Youtube 的 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3298689.3346997&quot;&gt;recommend next video to watch&lt;/a&gt; 的论文中，position feature 被单独的加到模型的一个 tower 里面来进行专门的学习，从而提高模型针对 position bias 的解决能力&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;常见的-feature-处理办法&quot;&gt;常见的 Feature 处理办法&lt;/h2&gt;

&lt;p&gt;上面的章节我们主要说了一些不同类型的 feature, 在介绍的时候我们也稍微提及了一下 feature 的不同的数据类型，这里我们再稍微复盘一下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;numeric feature, 我们也俗称 dense feature; 这里面也细分成两类，一类就是最普通的 float number, 比如 ratio/count 等等；另一类是 categorical feature, 比如性别，国别等等。这两类 feature 最主要的一个区别在于他们的 scale 是否具有意义&lt;/li&gt;
  &lt;li&gt;id feature, 我们也俗称 spares feature; 主要就是一些 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;entity id&lt;/code&gt;, 比如 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;post id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ads id&lt;/code&gt; 等等；具体的表示情况也有两种，一种就是单独的一个 id list, 另外一种则是给不同的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt; 一个 weight, 比如说我们可以根据用户跟这个 entity 交互的时间来做一个 weight decay, 从而实现一种简单的 recency 性质的 sparse feature, 让更近的 id 有更高的 weight, 一般我们管这个叫做 id score list; 所有的这种 id feature 都需要通过一个 lookup table 来转化成 dense vector&lt;/li&gt;
  &lt;li&gt;embedding feature, 一般就是其他模型输出的一些 dense vector; 一般整体直接使用，很少有只使用其中的某一些维度的情况&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这几种类型的 feature 当中, id feature 和 embedding feature 的处理办法一般都比较的统一，比如使用 lookup table 来转化 spares feature 到 dense format, 或者使用一些 clipping 来防止 embedding 里面的某些值过大; 针对 numeric feature 的处理比较常见，我们下面就主要聊一聊这些&lt;/p&gt;

&lt;h3 id=&quot;normalization&quot;&gt;Normalization&lt;/h3&gt;

&lt;p&gt;这个是针对 numeric feature 来说最常见的一种处理方式了，主要的目的就是让所有的 numeric feature value 的 scale 尽可能是统一的, 比如 ratio feature 的 scale 一般都是 0 ~ 1, 但是房价这个 feature 的 value 就可能 scale 在 0 ~ 10M, 这样会让 ratio feature 的 weight 跟房价 feature 的 weight 不在同一个数量级上，从而导致了 “vanish” 情况的发生。针对这种情况，一般我们可以采用 min-max scale 的方式来 normlize&lt;/p&gt;

&lt;p&gt;另外的一种情况，是 feature 本身的 distribution 是 highly skew 的，在 ML 里面，我们都是尽可能希望 feature 的分布尽可能的接近正态分布（为什么呢）。针对这种 data skewness 的情况，我们可以采用一些 transformation 比如 log-transform 或者 &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation&quot;&gt;cox-box transform&lt;/a&gt; 来进行处理，从而让 value 尽可能符合正态分布&lt;/p&gt;

&lt;h3 id=&quot;one-hotmulti-hot-encoding&quot;&gt;One-hot/multi-hot Encoding&lt;/h3&gt;

&lt;p&gt;这种处理办法是 categorical feature 的一种常见处理办法。在这种方法中，针对 categorical feature，我们会把它展开成一个 sparse vector，里面只有一个或者几个 element 是 1, 其他的位置上全都是 0。One-hot 和 multi-hot 的主要区别在于在这个 sparse vector 里面能有几个位置上有 1，one-hot 只有 1 个，而 multi-hot 可以有多个&lt;/p&gt;

&lt;h3 id=&quot;feature-transformation&quot;&gt;Feature transformation&lt;/h3&gt;

&lt;p&gt;除了上面的处理办法，针对 feature 还有一些额外的变换，从而增加模型能够学习的 signal 的数量&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 Youtube 的这篇论文中，他们把一些 feature 用上了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sqrt&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pow&lt;/code&gt; 等变换，作为新的 feature 和原本的 feature 一起送入模型进行学习&lt;/li&gt;
  &lt;li&gt;我们还可以把不同的 feature 彼此之间做 bi-gram，从而显形的增加 feature interaction (2nd order interaction, 这个也是之前很多研究的一个重点，因此退出了诸如 DCN 等模型架构，不过在这些模型里面， feature interaction 是通过模型表征，而不是在输入上面做文章)&lt;/li&gt;
  &lt;li&gt;另外一种比较常见的做 feature transformation 的办法是给原本的 feature 添加 breakdown, 比如说我们算一类 item 的 CTR 数据，那么可以根据用户的性别添加一个 breakdown (说个题外话，这种 breakdown 现在也被用于处理用户数据上，从而实现 privacy preserve);&lt;/li&gt;
  &lt;li&gt;更加放飞自我的一种办法，我们可以先 train 一个 gbdt, 然后把 gbdt 的 leave node 作为一个 feature 输入模型（可以考虑作为一个 binary sparse vector，或者带着 leave node 上的 weight）; 在 Facebook 的早期 ads 模型里面，大量的使用了这种方法，详情参考这篇&lt;a href=&quot;https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/&quot;&gt;论文&lt;/a&gt;
    &lt;blockquote&gt;
      &lt;p&gt;We found that boosted decision trees are a powerful and very convenient way to implement non-linear and tuple transformations of the kind we just described.&lt;/p&gt;
    &lt;/blockquote&gt;
    &lt;ul&gt;
      &lt;li&gt;针对这一点，我在和 reddit 的 ads ranking 面试的时候，跟 hiring manager 有一个比较激烈的讨论（可能也是因为这样她把我拒了把吧）; HM 一开始没有很理解使用 gbdt 来做 feature transformation 的思路，认为这不可行；在被我 convenience 这样做可行之后，又抛出来认为这样做没必要，因为可以调整模型架构来直接学习 tree 本身所 capture 到的一些复杂的 feature interaction; 针对这一点我是同意的，而且我也有幸参加到了 ads ranking 的 gbdt deprecation 的工作之中; 不够我们当时之所以 deprecate gbdt 更多的是从 maintenance cost 和 simplify tech stack 的角度，gbdt 本身仍然能够带来很不错的 model gain, 这也是为什么 deprecation 整了很长时间也没有完全取得胜利&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;missingsentinel-value&quot;&gt;Missing/Sentinel Value&lt;/h3&gt;

&lt;p&gt;如何处理 feature 的 missing value 或者 sentinel value 也是作为 machine learning engineer 我们经常要实际面对的问题。针对 missing value，大部分的时候我们会直接用 0 来取代，但是如果 0 也是一个合法值的话，会给模型造成困扰; 这个使用可以考虑使用 sentinel value 来表示 value 是否 missing, 把这个 sentinel value 做成一个新的 categorial feature 用在模型里面; 这样一来，原本的 feature 和这个新的 categorical feature 结合着使用，模型就能区分开来 feature value 是真的等于 0 还是单纯的 missing&lt;/p&gt;

&lt;h2 id=&quot;feature-optimization&quot;&gt;Feature Optimization&lt;/h2&gt;

&lt;p&gt;这一部分一般属于 bouns point, 在实际的面试中，很少有机会会真的讨论到这里，我之前在 Facebook 的时候带领很多这方面的项目，因此也借着这个机会给大家简单的介绍一下&lt;/p&gt;

&lt;h3 id=&quot;feature-importance&quot;&gt;Feature Importance&lt;/h3&gt;

&lt;p&gt;当我们有大量的 feature 之后，一个头痛的问题就是我们要选用哪些 feature 进入到模型里面; 由于 infra 的一些 limitation, 把所有的 feature 都放入到模型中开销过于巨大，无法支持，因此一般我们会选择一个 subset feature 来放到 production 模型中，而选取 feature 的一个重要指标，就是 feature importance&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里插入一些题外话，除了由于 infra limitation 导致我们不能使用所有的 feature 以外，另外一个考虑的因素是 feature coorelation，我们通过 empirical 的研究发现，把 correlated feature 放入到模型里面不会给模型带来任何效果上的提升，而且反而可能会降低模型的效果；不过 correlated feature 倒是有助于提高模型的 robustness，所以这也是一个实际中我们需要 make 的 trade off; 在 Facebook ads 我们是尽可能 minimize feature correlation，从而把尽可能多的有 additional gain 的 signal 放入到模型中&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Feature importance 可以从算法层面和工程层面两个角度讨论，工程层面的话主要就是涉及到各种 feature metadata 的管理，如何保证 feature candidate pool 是正确的（在复杂的系统中，feature serving 也是一个被高度优化的部分，导致有些 feature 只有在特定的环境下才可用），如何保证 feature 有足够的 coverage 等等；从算法层面，可以用来计算 feature importance 的方法有: shuffling algorithm, SHAP, integrated gradient 以及 binary stochastic neuron 等，这里就不再展开讨论了&lt;/p&gt;

&lt;p&gt;当通过上面提到的方法得到 feature importance score 之后，我们就可以使用 top k 的方法来选取 feature，除了这种简单的 selection strategy 之外，我们还可以引入其他的一些 metrics，比如 feature serving cost, feature storage cost 等等来进行 joint optimization&lt;/p&gt;

&lt;h3 id=&quot;hash-size-tuning&quot;&gt;Hash Size Tuning&lt;/h3&gt;

&lt;p&gt;我们在前讨论 id feature 的时候，大量的提到了 embedding lookup table 这样一个重要的 component. 它有两个很重要的 hyper parameter，embedding dim 和 cardinality, 可以理解为 embedding lookup table 的的行数和列数. 由于 id space 巨大，如果给每一个 id 都提供一个单独的 embedding 的话，模型的 paraemter 会过于巨大，因此我们通常采用 hash trick, 也就是通过一个 hash function 把原本的 id 映射到另外一个空间上面，然后这个空间的 cardiality 就是我们 embedding lookup table 的列数，一般我们也把这个 cardinality 称作 hash size. 如果 hash size 设置的太大，那么会浪费空间，如果太小，又会导致太多的 collision 从而影响性能（尤其是对 tail id 来说），因此如何找到一个比较好的 hash size 也是一个玄学&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里再插入一个题外话，目前主流的一些 hash function 基本上是 semantic-less 的，也就是说很有可能一些非常 popular 的 id, 比如大V们的 post 和一些 tail id 比如我的 post 是被 hash 到同一个 bucket 里面去了，这会导致这个 embedding 会被这些大V们的 post overwhlem，而不会有太多的我的 post 的 representation. 这其实是一个 known issue, 业界也有一些尝试用更复杂的 hash function, 比如基于 culster 的 hash 来处理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有两种 hash size 的方法可以使用，一种是在 traning dataset 上 sample 一部分数据之后做分析，看不同的 hash size 情况下 collision 的情况，然后动态的调整（比如使用二分搜索）使得 hash size 能够满足一定的 collision rate 的阈值需要；另外一种方法是先给一个很大的 hash size，然后我们在模型训练的时候同时保存一个 counter 来记录各个 column 的 hit rate, 然后再模型训练完毕之后根据这个 hit rate 来做一个 post-processing&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;针对 embedding dimension, 目前主流的做法就是用一个 single dimension, 但是有一些研究尝试用 variable dimension, Google 曾经有一篇 paper 但是我现在找不到了，之后找到了再 update 过来&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;acknowledgemment&quot;&gt;Acknowledgemment&lt;/h2&gt;

&lt;p&gt;感谢 Yuan 大佬，Bing 姐和 Yuzhong 大佬提供的修改和补充意见，让这篇 blog 更加的完善&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.06978&quot;&gt;Deep Interest Network for Click-Through Rate Prediction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2311.09544&quot;&gt;Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/blog/engineering/feed/enhancing-homepage-feed-relevance-by-harnessing-the-power-of-lar&quot;&gt;Enhancing homepage feed relevance by harnessing the power of large corpus sparse ID embeddings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/pinterest-engineering/user-action-sequence-modeling-for-pinterest-ads-engagement-modeling-21139cab8f4e&quot;&gt;User Action Sequence Modeling for Pinterest Ads Engagement Modeling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/pinterest-engineering/evolution-of-ads-conversion-optimization-models-at-pinterest-84b244043d51&quot;&gt;Evolution of Ads Conversion Optimization Models at Pinterest&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.09577&quot;&gt;Real-time Short Video Recommendation on Mobile Devices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1907.05171&quot;&gt;Privileged Features Distillation at Taobao Recommendations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/&quot;&gt;Practical Lessons from Predicting Clicks on Ads at Facebook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/pinterest-engineering/how-we-use-automl-multi-task-learning-and-multi-tower-models-for-pinterest-ads-db966c3dc99e&quot;&gt;How we use AutoML, Multi-task learning and Multi-tower models for Pinterest Ads&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.04507&quot;&gt;PinnerFormer: Sequence Modeling for User Representation at Pinterest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 24 Jun 2024 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Features-in-Recommendation-System/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Features-in-Recommendation-System/</guid>
      </item>
    
      <item>
        <title>How to Design Auction System</title>
        <description>&lt;p&gt;In this post, let’s discuss a little bit how to design an auction system similar to the one on eBay, where owner could list their items in the system and others could place a bid on it. User with highest bid would be the winner of this auction and could buy it.&lt;/p&gt;

&lt;p&gt;In a real world auction system, there are lots of components involved, such as the search (user could search active auction based on their interest), payment (winner need to make the payment) and inventory (owner could add new items). We would not dive deep into these components, but would only focus on the auction service itself. For search and payment, I plan to have other posts to discuss them in depth.&lt;/p&gt;

&lt;p&gt;In this post, we would discuss 2 different ways to design the auction system, &lt;em&gt;stateful&lt;/em&gt; and &lt;em&gt;stateless&lt;/em&gt;, and see what would be their pros and cons. In reality, &lt;em&gt;stateless&lt;/em&gt; is more common, while &lt;em&gt;stateful&lt;/em&gt; design still play a critical role in different use cases, e.g. stream processing.&lt;/p&gt;

&lt;h2 id=&quot;functional-requirement&quot;&gt;Functional Requirement&lt;/h2&gt;
&lt;p&gt;We would assume the following functional requirement to be offered by our system&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;User could start an auction&lt;/li&gt;
  &lt;li&gt;User could view the active auction, and place a bid in the auction; user could also get realtime update on the current highest bid&lt;/li&gt;
  &lt;li&gt;Auction is closed when there is no higher bid for 1 hour&lt;/li&gt;
  &lt;li&gt;Winner of the auction would receive notification and has 10 minutes to make the payment&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-functional-requirements&quot;&gt;Non Functional Requirements&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;High availability&lt;/li&gt;
  &lt;li&gt;High scalability&lt;/li&gt;
  &lt;li&gt;Low latency&lt;/li&gt;
  &lt;li&gt;Eventual consistency is acceptable for live bidding part (we could discuss for higher consistency level), but when determine the winner of the auction, it needs strong consistency&lt;/li&gt;
  &lt;li&gt;1B DAU, 100k auctions per-day, on average 10% of user place 1 bid per day, assume 10:1 read:write ratio&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;some-questions-to-clarify&quot;&gt;Some questions to clarify&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;What if there are multiple bids with the same price, who would be the winner?
    &lt;ul&gt;
      &lt;li&gt;The first bidder would be the winner&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do we allow a bidder to place multiple bids within the same auction?
    &lt;ul&gt;
      &lt;li&gt;No, each bidder could only place 1 bid, but they could increase their bid if their original one is not winner&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do we need to record all bids that user placed during the auction?
    &lt;ul&gt;
      &lt;li&gt;This is great question, let’s keep all bid that users have placed instead of just winners&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What shall we do if there is no bid for certain auction, do we need user to provide a TTL?
    &lt;ul&gt;
      &lt;li&gt;Let’s simplify the problem as of now and assume there is no TTL required&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;high-level-design&quot;&gt;High level design&lt;/h2&gt;
&lt;p&gt;Newton has said that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If I have been able to see further, it was only because I stood on the shoulders of giants&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this design, we would also stand on the shoulders of &lt;em&gt;giants&lt;/em&gt;, which is &lt;strong&gt;live comment&lt;/strong&gt; and &lt;strong&gt;cron job scheduler&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;auction-creation&quot;&gt;Auction creation&lt;/h3&gt;
&lt;p&gt;This part is relative simple. We have &lt;strong&gt;Auction Service&lt;/strong&gt; to handle the creation HTTP request from user. The auction service would write a new entry to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_table&lt;/code&gt; within &lt;strong&gt;Auction DB&lt;/strong&gt; and update to cache. Below is an example schema of our auction table. Besides the regular metadata such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;owner_id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;item_id&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;created_at&lt;/code&gt;, there are 2 important fields, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;expire_at&lt;/code&gt;, which is critical for us to manage the transition of auction and handle the payment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/auction_db.png&quot; alt=&quot;Auction Database&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we create a new auction, we would also update it into the cache and mark it as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTIVE&lt;/code&gt; auction. This design choice actually makes our auction service stateless: it does not need to maintain any data on the server regarding the auction. If it needs to know the status of an auction, it would query the cache and then do the necessary processing. The cache is primarily used to help us improve the read performance regarding the highest bid for a given auction. If DB write or cache update fails, we would return failure to client and client would retry the creation.&lt;/p&gt;

&lt;p&gt;There might be issue that the status in cache and in Auction DB are inconsistent, we would dive deeper into this topic in &lt;a href=&quot;#cache-and-auction-db-consistency&quot;&gt;Cache and Auction DB consistency&lt;/a&gt; section.&lt;/p&gt;

&lt;h3 id=&quot;auction-bid-place-and-update&quot;&gt;Auction Bid Place and Update&lt;/h3&gt;
&lt;p&gt;For this part, there are 2 key problems we need to answer:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the connection mechanism between client and our service&lt;/li&gt;
  &lt;li&gt;the mechanism to route highest bid to users who are viewing the current auction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the first problem, we would use a combination of &lt;em&gt;HTTP request&lt;/em&gt; and &lt;em&gt;server sent event (SSE)&lt;/em&gt;: to place a bid, we issue an HTTP request to &lt;strong&gt;Auction Service&lt;/strong&gt;; while to receive highest bid from others, we leverage SSE connection with &lt;strong&gt;Bid Update Service&lt;/strong&gt;. Other connection options are &lt;em&gt;HTTP long polling&lt;/em&gt; and &lt;em&gt;websocket&lt;/em&gt;. &lt;em&gt;HTTP long polling&lt;/em&gt; is relative less efficient because client needs to repeatedly query the backend for new bids. &lt;em&gt;Websocket&lt;/em&gt; is a little bit over killing in our scenario as we don’t expect each user viewing the auction actively place bids, thus a single direction connection is sufficient. However, &lt;em&gt;websocket&lt;/em&gt; might also be applicable in some cases. A more detailed comparison between &lt;em&gt;websocket&lt;/em&gt; and &lt;em&gt;SSE&lt;/em&gt; is available in &lt;a href=&quot;#websocket-and-sse&quot;&gt;Websocket vs SSE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the second problem, one naive approach is to write all bids into DB and let the &lt;strong&gt;Bid Update Service&lt;/strong&gt; to poll the DB to see if there are new bids. This approach works if there is not much traffic, but is less efficient in our scale and would put too much pressure on DB (# of auction x 60 / # of granularity QPS from a single &lt;strong&gt;Bid Update Service&lt;/strong&gt;). Here we would leverage a &lt;em&gt;hierarchy fan-out&lt;/em&gt; mechanism to route the bids.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/auction_bid_update.png&quot; alt=&quot;Auction Bid Update&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When user first navigate to an auction page, we would retrieve the information about the auction through &lt;em&gt;Auction Service&lt;/em&gt; via regular HTTP request. If the auction is still in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTIVE&lt;/code&gt; status, user would build a SSE connection with one &lt;strong&gt;Bid Update Service&lt;/strong&gt; (Load Balancer could randomly pick one). The &lt;strong&gt;Bid Update Service&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bus1&lt;/code&gt; would update its in-memory &lt;em&gt;subscription table&lt;/em&gt; to record that a user &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u1&lt;/code&gt; is viewing auction &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a1&lt;/code&gt;. Also, this server would also make a request to &lt;strong&gt;Dispatcher&lt;/strong&gt; specifying that itself is listening to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a1&lt;/code&gt; and &lt;strong&gt;Dispatcher&lt;/strong&gt; would also update its in-memory &lt;em&gt;subscription table&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# bus1 subscription table
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;a1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;u1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;u2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# dispatcher subscription table
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;a1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;bus1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;a2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;bus2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When user make a bid, client would send a HTTP request to &lt;strong&gt;Auction Service&lt;/strong&gt;, the node that handle the request would also make a request towards &lt;strong&gt;Dispatcher&lt;/strong&gt;. The &lt;strong&gt;Dispatcher&lt;/strong&gt; would check its internal &lt;em&gt;subscription table&lt;/em&gt; to figure out which &lt;strong&gt;Bid Update Service&lt;/strong&gt; (in this case &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bus1&lt;/code&gt;) needs this update. Once &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bus1&lt;/code&gt; receives the request, it would also check its internal &lt;em&gt;subscription table&lt;/em&gt; to figure out which connected user it needs to send this update.&lt;/p&gt;

&lt;p&gt;In the version we just described, &lt;strong&gt;Dispatcher&lt;/strong&gt; is a &lt;em&gt;stateful&lt;/em&gt; service because it needs to maintain the &lt;em&gt;subscription table&lt;/em&gt;. If it is down, we won’t able to forward bid update anymore and thus making it highly available is critical to our system. The following options could be considered:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Adopt &lt;em&gt;write ahead log&lt;/em&gt; and &lt;em&gt;snapshot&lt;/em&gt; to rebuild the state after failure&lt;/li&gt;
  &lt;li&gt;Replicate the state to external storage (e.g. KV store) so that other nodes could pick it up&lt;/li&gt;
  &lt;li&gt;Active standby node to be promoted to primary once original one fails&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another consideration here is that we might be able to remove dispatcher, and just use coordination service or a distributed kv store to maintain the &lt;em&gt;subscription table&lt;/em&gt;. &lt;strong&gt;Bid Update Service&lt;/strong&gt; would directly make update to coordination service, and &lt;strong&gt;Auction Service&lt;/strong&gt; directly query it to figure out the &lt;strong&gt;Bid Update Service&lt;/strong&gt; it needs to send update to.&lt;/p&gt;

&lt;p&gt;There are pros and cons of both approach&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dispatcher
    &lt;ul&gt;
      &lt;li&gt;pros: simplify &lt;strong&gt;Auction Service’s&lt;/strong&gt; responsibility (SRP), could scale individually, handoff on retry&lt;/li&gt;
      &lt;li&gt;cons: slightly more complex overall architecture&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Without Dispatcher
    &lt;ul&gt;
      &lt;li&gt;pros: simpler architecture, less maintenance cost&lt;/li&gt;
      &lt;li&gt;cons: &lt;strong&gt;Auction Service&lt;/strong&gt; needs to handle forwarding and retry&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we would like to achieve higher consistency, such as each update needs to be sent to all users that is within the same auction. We could enable ACK among the services. For example, if certain &lt;strong&gt;Bid Update Service&lt;/strong&gt; does not reply ACK to &lt;strong&gt;Dispatcher&lt;/strong&gt;, &lt;strong&gt;Dispatcher&lt;/strong&gt; would retry the request. It is possible that on the client side we receive duplicated events, but it is pretty simple to dedup as we only need to keep the highest bid.&lt;/p&gt;

&lt;p&gt;It is still possible that certain bid update is lost during the transmission and it might not a big duel. The reason is that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;During normal active auction, there would always new bids coming out, which overwrite the pervious one; so certain data loss on client side would not make a big issue.&lt;/li&gt;
  &lt;li&gt;The only critical one is the miss of highest bid, which would be the last bid on the current auction. We could set a timer on the client side, and if it has been 10mins since we receive last update on bid, we could issue a hard pull to &lt;strong&gt;Auction Service&lt;/strong&gt; to get the latest bid information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Having discussed about how bids are routed to other users, let’s take a look how we maintain the current highest bid. When user make a bid, one instance of &lt;strong&gt;Auction Service&lt;/strong&gt; is going to handle the request. It first check if the auction exists in &lt;strong&gt;cache&lt;/strong&gt; or not, and see if the status of the auction is still &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTIVE&lt;/code&gt; status. If there is a cache miss, it reads &lt;strong&gt;Auction DB&lt;/strong&gt; to check the status of the auction (this could happen but should be some corner case). If auction is still &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTIVE&lt;/code&gt;, then &lt;strong&gt;Auction Service&lt;/strong&gt; write the bid into the bid table in &lt;em&gt;append&lt;/em&gt; pattern, which is great for write throughput. This choice would result in multiple bids for a single user given an auction, and we would use the latest one as user’s final bid (&lt;em&gt;latest&lt;/em&gt; could be determined by insertion time, or we could have client side request id which would be more robust). Once DB write is done and if the new bid is higher than the current one in cache, we would also update the information in cache and &lt;strong&gt;Auction Service&lt;/strong&gt; would also send request to &lt;strong&gt;Dispatcher&lt;/strong&gt; to deliver this new update to all clients.&lt;/p&gt;

&lt;p&gt;It is possible that the DB write is failed or the cache update is failed. We would retry the request if is some transitional issue.&lt;/p&gt;

&lt;p&gt;In the cache, we would store the following metadata&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;auction_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;highest_bid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;highest_bidder_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;updated_at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expire_at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;highest_bid&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;highest_bidder_id&lt;/code&gt; is relative straightforward. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;updated_at&lt;/code&gt; is used to record the staleness of the cached entry, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;expire_at&lt;/code&gt; is used as timer to trigger the auction execution (see &lt;a href=&quot;#auction-bid-execution&quot;&gt;Auction Bid Execution&lt;/a&gt;). This state works because in our FR we assume that the same user could only modify his bidder to higher price instead of lower. If we allow user to bid lower, then we need to store all user’s bid or top 100 bid.&lt;/p&gt;

&lt;p&gt;Since we cache auction state by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_id&lt;/code&gt;, we could suffer from &lt;em&gt;hotspot&lt;/em&gt; issue. For example, &lt;strong&gt;Wing Gundam Zero&lt;/strong&gt; is so popular that everyone tries to bid it and we have lots of concurrent update to the cache. Below are some options that we could consider&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;To deal with high volume of concurrent write request, we could use lease to coordinate the update to avoid potential &lt;em&gt;stale update&lt;/em&gt;. The downside is that the update might need to retry multiple times to succeed.&lt;/li&gt;
  &lt;li&gt;If we choose quorum as our replication strategy, we could potentially set write commit to 1 to increase the write throughput and have customized conflict resolve (relative simple as &lt;em&gt;larger-is-winner&lt;/em&gt;). This works because in our FR we assume that the same user could only place higher bid but not lower.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;auction-bid-execution&quot;&gt;Auction Bid Execution&lt;/h3&gt;
&lt;p&gt;To execute the winner’s bid after 1 hour, we have a &lt;strong&gt;Fulfillment Service&lt;/strong&gt;. This service is similar to a cron job scheduler that it periodically scan the state we have in cache and see if there is any bid that needs to be executed by checking the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;expire_at&lt;/code&gt;. Once it identify one bid that needs to be executed, it would also send a request to &lt;strong&gt;Auction DB&lt;/strong&gt; to double check if this is indeed the winner bid we need to execute:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If not, it would make a write to cache to correct the information in cache. This is similar to &lt;em&gt;read repair&lt;/em&gt; in quorum replication.&lt;/li&gt;
  &lt;li&gt;If confirmed, then &lt;strong&gt;Fulfillment Service&lt;/strong&gt; would update the status of the auction to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PAYMENT_PENDING&lt;/code&gt; in both DB and cache. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;expired_at&lt;/code&gt; field in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_table&lt;/code&gt; would be set based on the policy (e.g. 10mins in our case). The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;winner_id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;winner_bid_id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;winner_price&lt;/code&gt; would also be populated all together. And then send request to notification system to send a payment notification to the winner. This event update would also be sent via the &lt;strong&gt;Dispatcher&lt;/strong&gt; to all live users in this auction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/auction_bid_execution.png&quot; alt=&quot;Auction Bid Execution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The actual payment would be handled by another dedicated system which we won’t discuss too much in details. But once the payment is done, the payment service would update the auction status to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUCCEED&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Fulfillment Service&lt;/strong&gt; would also periodically check the auction that is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PAYMENT_PENDING&lt;/code&gt; status and see if there is any auction that exceeds the deadline but still not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUCCEED&lt;/code&gt; yet, and move them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAILED&lt;/code&gt; status.&lt;/p&gt;

&lt;p&gt;Notice that in our design, the &lt;strong&gt;Fulfillment Service&lt;/strong&gt; depends on the cache to trigger the bid execution. This requires us to have cache to be highly available (through strategy such as different replication mechanism). Another option is to directly have the &lt;strong&gt;Fulfillment Service&lt;/strong&gt; to query the &lt;strong&gt;Auction DB&lt;/strong&gt; where our ground truth data exists. It needs to perform a relative complex query to join &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_table&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bid_table&lt;/code&gt; to find the wining bid of each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTIVE&lt;/code&gt; auction and check if they need to be executed or not. This is one tradeoff we need to consider:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;use cache, pros is reduced latency, cons is potential inconsistency issue which cause missed execution&lt;/li&gt;
  &lt;li&gt;directly read db, pros is accurate and no missed execution, cons is high latency and more pressure on DB&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;final-stateless-architecture&quot;&gt;Final Stateless Architecture&lt;/h3&gt;
&lt;p&gt;In the final design, we also introduce a &lt;strong&gt;Reconcile Service&lt;/strong&gt; which help us to detect certain abnormal situation. For example, the payment has succeed but the auction status is not correctly updated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/auction_all.png&quot; alt=&quot;Stateless Architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;stateful-choice&quot;&gt;Stateful Choice&lt;/h2&gt;
&lt;p&gt;The discussion above is mainly on the &lt;em&gt;stateless design&lt;/em&gt;. In this section, we discuss a little bit about the &lt;em&gt;stateful design&lt;/em&gt; and see how it would be different from the &lt;em&gt;stateless one&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/auction_stateful.png&quot; alt=&quot;Stateful Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We would make &lt;strong&gt;Auction Service&lt;/strong&gt; stateful, which means that it would maintain all bid related data for an auction. Once owner create an auction, it would be randomly assigned to a &lt;strong&gt;Auction Service&lt;/strong&gt; and all bid for this auction would be handled through this instance. To minimize the latency, we could make the state maintained in memory. But similar to &lt;strong&gt;Dispatcher&lt;/strong&gt;, we still need to make it highly available. &lt;em&gt;WAL&lt;/em&gt; + &lt;em&gt;snapshot&lt;/em&gt; or rebuilding from &lt;strong&gt;Auction DB&lt;/strong&gt; are available options.&lt;/p&gt;

&lt;p&gt;If user make a bid, we would leverage the load balancer to route this request to the right &lt;strong&gt;Auction Service&lt;/strong&gt; instance to handle it (&lt;em&gt;service discover&lt;/em&gt;). We don’t need another cron job scheduler to check if there is any bid needs to be executed, all these information is already available within the instance and it could handle that correctly.&lt;/p&gt;

&lt;p&gt;We could take a simple comparison between &lt;em&gt;stateful&lt;/em&gt; and &lt;em&gt;stateless&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;stateful&lt;/th&gt;
      &lt;th&gt;stateless&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;consistency&lt;/td&gt;
      &lt;td&gt;easier to achieve high consistency as all data related to an auction is handled by a single server, for example we don’t need a separate fulfillment service to check if there is a bid to be executed&lt;/td&gt;
      &lt;td&gt;more challenging because there could be concurrent data write on the same auction handled by different servers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;availability&lt;/td&gt;
      &lt;td&gt;more challenging to achieve as we need to replicate the stateful data&lt;/td&gt;
      &lt;td&gt;easier to handle as the server is stateless and all state data is handled by external storage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;scalability&lt;/td&gt;
      &lt;td&gt;more challenging to scale, especially hotspot&lt;/td&gt;
      &lt;td&gt;easier to scale as we could add more machines and evenly balance the traffic&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;additional-discussion&quot;&gt;Additional Discussion&lt;/h2&gt;
&lt;p&gt;In this section, we discuss about several additional points about the design.&lt;/p&gt;

&lt;h3 id=&quot;high-availability&quot;&gt;High Availability&lt;/h3&gt;
&lt;p&gt;During the high level design discussion, we have touched a little bit about how to achieve high availability in each component. In this section, we summarize the key points and add some additional ones.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Auction Service&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;in the &lt;em&gt;stateless&lt;/em&gt; design, there is not much concern here, if the node is down before response back to client, the client would just retry and another node would help server the request. There might be duplicated write/update but it is fine in our case
        &lt;ul&gt;
          &lt;li&gt;the auction creation could use upsert and check if there is the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;item_id&lt;/code&gt; combination within a time rage for dedup&lt;/li&gt;
          &lt;li&gt;the bid is designed to be in appends and only the last one (by request id or injection time) would be used as the user’s final bid&lt;/li&gt;
          &lt;li&gt;the update to cache is fine regarding duplicated ones&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;in the &lt;em&gt;stateful&lt;/em&gt; design, we need to replicate the service state, by having follower node or snapshot the state to external storage&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dispatcher&lt;/strong&gt;: this is also a &lt;em&gt;stateful&lt;/em&gt; service and the strategy is similar to the &lt;em&gt;stateful&lt;/em&gt; &lt;strong&gt;Auction Service&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cache, Auction DB, KV Store&lt;/strong&gt;: different replication strategy could be discussed here, such as single leader, multi leader and quorum based. If you are not familiar with these concepts, please refer to the video below learn more details&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/X8IhZx7fg24&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bid Update Service&lt;/strong&gt;: even though these service are also stateful because they need to maintain the connection with client, we don’t need to replicate them nor persistent the information similar to other &lt;em&gt;stateful&lt;/em&gt; service. The reason is that: 1. this stateful information is coupled with the liveness of this service, if the node is down, the connection has to be rebuild with other nodes; 2. the stateful information is not shareable with other nodes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fulfillment Service&lt;/strong&gt;: this is also stateless and we could have a active standby to take the work once the primary one is done&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;high-scalability&quot;&gt;High Scalability&lt;/h3&gt;
&lt;p&gt;We didn’t talk too much about how the system could scale.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Auction Service&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;in the &lt;em&gt;stateless&lt;/em&gt; design, it is pretty easy to scale as we could add more nodes to improve the request that we could handle&lt;/li&gt;
      &lt;li&gt;in the &lt;em&gt;stateful&lt;/em&gt; design, we could scale it via sharding by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;owner_id&lt;/code&gt;; sharding by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_id&lt;/code&gt; is an option if we have a separate id generator to assign it upon the creation request&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dispatcher&lt;/strong&gt;: the size of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subscription_table&lt;/code&gt; is manageable (# of bid update server x 100k x 8 bytes ~ GB level), thus a single sever should be sufficient; however, the size of data is only one dimension we need to consider when scale the system, the QPS would also be a factor that we need to consider. For &lt;strong&gt;Dispatcher&lt;/strong&gt;, it needs to deal with pretty high volume of request, thus we could add read replica to improve the throughput (sync replication for stronger consistency or async for eventual consistency), or we could also shard it by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_id&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cache, Auction DB, KV Store&lt;/strong&gt;: different sharding strategy could be discussed here, such as partition by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_id&lt;/code&gt; (which offers good co-locate property for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_table&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bid_table&lt;/code&gt; but has the downside of hotspot); or partition by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user_id&lt;/code&gt; (which might better distribute the write as is it relative rare for someone that becomes a hotspot and they could be rate limited)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bid Update Service&lt;/strong&gt;: it is also easy to scale by adding more nodes because they only keep in-memory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subscription_table&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fulfillment Service&lt;/strong&gt;: we could shard it by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_id&lt;/code&gt; to evenly distribute the processing to more nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cache-and-auction-db-consistency&quot;&gt;Cache and Auction DB consistency&lt;/h3&gt;
&lt;p&gt;In our &lt;em&gt;stateless&lt;/em&gt; design, we store all data into &lt;strong&gt;Auction DB&lt;/strong&gt;, and also store highest bid related information for each auction in a cache. We adopted something similar to &lt;em&gt;write through&lt;/em&gt;, in which we write DB first and then update the cache; another option to consider is &lt;em&gt;write back&lt;/em&gt;, in which we update cache first, and then at sometime later right back to DB. &lt;em&gt;Write back&lt;/em&gt; could be used if we decided to in real time update the winning bid into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auction_table&lt;/code&gt; to reduce the volume of write request.&lt;/p&gt;

&lt;p&gt;It is possible that we write to DB success but failed to update cache. For example, the request to update cache is failed or the node is down before try to update the cache. Retry could be used here, but it could still possible that the update to cache is failed after several retry. But since our &lt;strong&gt;Fulfillment Service&lt;/strong&gt; reads the cache to execute the bid, it might read some outdated data because of the above potential failure. That is also why we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;updated_at&lt;/code&gt; field to track if we should read from DB again to see if the data is up-to-date. Also upon serving request from client on pulling highest bid, we leverage &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;updated_at&lt;/code&gt; to do a &lt;em&gt;read repair&lt;/em&gt; to fix the potential out of date.&lt;/p&gt;

&lt;h3 id=&quot;websocket-and-sse&quot;&gt;Websocket and SSE&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Websocket&lt;/em&gt; and &lt;em&gt;SSE&lt;/em&gt; are 2 common way we build a connection with backend and keep it live to send/receive data; instead of repeatedly creating new request and sent it over. Below is a simple comparison of these 2 approach&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Websocket&lt;/th&gt;
      &lt;th&gt;SSE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;communication&lt;/td&gt;
      &lt;td&gt;bi-direction&lt;/td&gt;
      &lt;td&gt;single direction&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;support&lt;/td&gt;
      &lt;td&gt;most modern browser already supported&lt;/td&gt;
      &lt;td&gt;limited browser support&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;failure&lt;/td&gt;
      &lt;td&gt;could not reconnect and need to establish a new one&lt;/td&gt;
      &lt;td&gt;could reconnect&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data type&lt;/td&gt;
      &lt;td&gt;support both text and binary data&lt;/td&gt;
      &lt;td&gt;text data only&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;application&lt;/td&gt;
      &lt;td&gt;realtime messaging, online gaming&lt;/td&gt;
      &lt;td&gt;stock monitor, live comment&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In our current design, we are establish a new SSE whenever user navigate to a new auction. Another design choice here is to let user establish a new connection upon login to our application. And keep a &lt;em&gt;websocket&lt;/em&gt; connection. Whenever user navigate to another auction, it would send this event over the &lt;em&gt;websocket&lt;/em&gt; so that the &lt;strong&gt;Bid Update Service&lt;/strong&gt; could update the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subscription table&lt;/code&gt;. Depends on the pattern of how general users are interacting with our system, we could optimize the choice of the connection mechanism.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/presentations/linkedin-play-akka-distributed-systems/&quot;&gt;Streaming a Million Likes/Second: Real-Time Interactions on Live Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dropbox.tech/infrastructure/asynchronous-task-scheduling-at-dropbox&quot;&gt;How we designed Dropbox ATF: an async task framework&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dataintensive.net/&quot;&gt;Design Data-Intensive Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/news/2015/11/scaling-stateful-services/&quot;&gt;Scaling Stateful Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource&quot;&gt;Difference between Websockets and Server Sent Events&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf&quot;&gt;Scaling Memcache at Facebook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/stream-processing-with/9781491974285/&quot;&gt;Stream Processing with Apache Flink&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/85uigxGitIg?si=H6JFhiylr0OXphYt&quot;&gt;DDIA Chapter 6 Partition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/85uigxGitIg?si=cx6C9z5XDto6rj7g&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;Thanks Rita and Celia for the great discussion and lots of idea.&lt;/p&gt;

</description>
        <pubDate>Sat, 06 Apr 2024 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//How-to-design-auction-system/</link>
        <guid isPermaLink="true">https://pyemma.github.io//How-to-design-auction-system/</guid>
      </item>
    
      <item>
        <title>How to use LLM for recommendation task</title>
        <description>&lt;p&gt;Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey.&lt;/p&gt;

&lt;p&gt;Some key take away:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLM internally has encapsulated lots of knowledge about the world and it could leverage these knowledge to do some general recommendation (such as Movie)&lt;/li&gt;
  &lt;li&gt;In context learning is a powerful technique to inject various information into promote to provide more context for LLM, such as user profile and user past interaction history&lt;/li&gt;
  &lt;li&gt;Use training data that specifically constructed for recommendation task to fine tune LLM could further improve the performance of LLM&lt;/li&gt;
  &lt;li&gt;We could directly use LLM to output candidate, or use LLM output as additional signal to inject into existing recommendation models&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;PS: due to the rapid change of this area, the paper I read might have been outdated. Please feel free to leave comments on the latest work/idea in this domain. Also I’m reading the latest paper from arxiv and will potentially have a new series of post on summarizing the latest work in LLM and ML area, stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;PPS: I would primarily summarize my understanding without to much technical terms and mathematic formula; the main goal is to grasp the highlevel idea of the paper&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;context&quot;&gt;Context&lt;/h2&gt;
&lt;p&gt;In classical recommendation system, we usually adopt a 2-stage architecture. In first stage, we adopt heuristic rule, or leverage some simple model to quickly identify some promising candidates from the entire eligible population (&lt;em&gt;actually, there is indexing step before here as well, but for simplicity, let’s skip that&lt;/em&gt;). This first stage is called &lt;strong&gt;candidate retrieval&lt;/strong&gt;, which we usually optimize for &lt;strong&gt;recall&lt;/strong&gt;. In the second stage, we would rank the candidates we retrieved in the first stage, via more signals and more powerful model. This stage is usually called &lt;strong&gt;rerank&lt;/strong&gt;, which optimize for &lt;strong&gt;precision&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pairwise-ranking-via-llm&quot;&gt;Pairwise Ranking via LLM&lt;/h2&gt;
&lt;p&gt;In paper &lt;a href=&quot;https://arxiv.org/pdf/2306.17563.pdf&quot;&gt;“Large Language Model Are Effective Text Rankers With Pairwise Ranking Prompting”&lt;/a&gt;, the author proposed a new format of prompt that let LLM to rank a pair of candidates given a query, which outperforms the point-wise and list-wise format. The format of the prompt is as follow:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Given a query &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, which of the following two passage is more relevant to the query?

Passage A: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;

Passage B: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;

Output Passage A or Passage B
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For each pair of candidates, we use the above prompt to let LLM output the choice, and compute the final scores as&lt;/p&gt;

\[s_{i} = 1 * \sum_{j \neq i} I_{d_{i} &amp;gt; d_{j}} + 0.5 * \sum_{j \neq i} I_{d_{i} = d_{j}}\]

&lt;p&gt;and rank the document accordingly.&lt;/p&gt;

&lt;h2 id=&quot;enrich-the-information-for-llm-to-recommend&quot;&gt;Enrich the information for LLM to recommend&lt;/h2&gt;
&lt;p&gt;Personalized recommendation is critical to improve the conversion rate. Use profiling, user past’s item interaction history bring valuable signal for recommendation. In this section, we will take a look some idea on how to inject such information into prompt to let LLM “learn” the flavor of user and provide better personalized result.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2304.10149.pdf&quot;&gt;“Is ChatGPT a Good Recommender? A Preliminary Study”&lt;/a&gt;, the authors proposed different type of prompt of different type of tasks. These prompt could be decomposed as &lt;strong&gt;task descriptor&lt;/strong&gt;, &lt;strong&gt;user-specific injection&lt;/strong&gt;, &lt;strong&gt;formatting restrictions&lt;/strong&gt;. &lt;strong&gt;User-specific injection&lt;/strong&gt; is the part where we add user’s past item interaction info. The format for &lt;em&gt;sequential recommendation&lt;/em&gt; is as follow (content in bracket is comment)&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Requirement: you must choose 10 items for recommendation and sort them in order of priority, 
from hightest to lowest. [task descriptor]

Output format: a python list. Do not explain the reason for include any other words. [formatting restrictions]

Given user&apos;s interaction history in chronological order: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, 
the next interaction item is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;. [In context learning]
Now, if the interaction history is updated to &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; and the user is likely to interact again, 
recommend the next item. [user-specific injection]
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In this prompt, a common technique, which is called &lt;em&gt;in context learning&lt;/em&gt;, or &lt;em&gt;few shot prompting&lt;/em&gt; , is used. By showing LLM some examples to follow in the prompt, we could change the underlying distribution of LLM model and bias it to generate the output &lt;em&gt;conditionally&lt;/em&gt; on the examples we have given. This stanford &lt;a href=&quot;https://ai.stanford.edu/blog/understanding-incontext/&quot;&gt;blog&lt;/a&gt; is a great source to learn more on how &lt;em&gt;in context learning&lt;/em&gt; works. In short words, the additional example we provided helps LLM to better &lt;em&gt;locate&lt;/em&gt; concept internally, and thus more aligned. A Bayesian inference view on that is as follow, which is pretty easy to understand&lt;/p&gt;

\[p(output|prompt) = \int_{concept}p(output|concept, prompt)p(concept|prompt)d(concept)\]

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2305.07622.pdf&quot;&gt;“PALR: Personalization Aware LLMs for Recommendation”&lt;/a&gt;, author adopted similar approach to integrate users’ past interaction into prompt. One novel idea in this paper is to leverage LLM to generate user profile, which leverages the summarization capability of LLM. The prompt is as follow (use MovieLens-1M as example)&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Input: Your task is to use two keywords to summarize user&apos;s preference based on history interactions.
The output is an itemized list based on importance. The output template is:
&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KEYWORD_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;HISTORY_MOVE_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;HISTORY_MOVE_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KEYWORD_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;HISTORY_MOVE_2&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
The history movies and their keywords
&quot;MOVIE_1&quot;: KEYWORD_1, KEYWORD_2
&quot;MOVIE_2&quot;: KEYWORD_1, KEYWORD_3
&quot;MOVIE_3&quot;: KEYWORD_4
&quot;MOVIE_4&quot;: KEYWORD_1, KEYWORD_3, KEYWORD_4
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then the user profile is also input into the prompt to let LLM recommend items from the candidate set.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In context learning&lt;/em&gt; is a technique that I widely used during my project. It is much cheaper compared to fine-tune LLM, and the performance is also pretty good as long as you have high quality data. From my experience, &lt;em&gt;formatting control&lt;/em&gt; is pretty challenge and sometimes could not be 100% solved by explicit instructions or few shot. Sometimes, we need to have some dedicated business code to do some postprocessing on LLM output to parse the part we interested most out.&lt;/p&gt;

&lt;h2 id=&quot;go-beyond-in-context-learning-fine-tune-llm-for-recommendation-task&quot;&gt;Go beyond In-Context Learning: Fine-tune LLM for recommendation task&lt;/h2&gt;
&lt;p&gt;In context learning is a powerful technique, however, due to the fact that LLM is trained on NLP task instead of recommendation task, its performance is still sometime limited. Using some training data that is specifically constructed for recommendation to fine-tune LLM could help LLM to &lt;em&gt;learn&lt;/em&gt; more for recommendation task.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2305.00447.pdf&quot;&gt;TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation&lt;/a&gt;, the author proposed a 2-stage fine-tuning framework. In first stage, they leverage &lt;a href=&quot;https://crfm.stanford.edu/2023/03/13/alpaca.html&quot;&gt;Alpaca Tuning&lt;/a&gt; to improve LLM’s generalization ability, and then in 2nd stage, they use recommendation training data to do &lt;em&gt;rec tuning&lt;/em&gt;. The format of the training data is as follow&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Task instruction: Given the user&apos;s historical interactions, please determine whether the user
will enjoy the target new movie by answering &quot;Yes&quot; or &quot;No&quot;.
Task input:
    - User&apos;s liked items: GodFather.
    - User&apos;s disliked items: Star Wars.
    - Target new movie: Iron Man.
Task output: No
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A high level flow is as follow
&lt;img src=&quot;/assets/tallrec.png&quot; alt=&quot;TALLRec&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;work-with-existing-recommendation-models&quot;&gt;Work with existing Recommendation models&lt;/h2&gt;
&lt;p&gt;Besides directly let LLM to output the recommendation from the candidates, we could also use LLM together with existing recommendation models. Use the output of one model as input to another model has been a widely adopted practice in the ranking world, e.g. using the GBDT leave as feature in NN. You could think of that we leverage model to do some compression and preprocessing on the signals, which is similar to traditional feature engineering.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2307.15780.pdf&quot;&gt;LLM-Rec: Personalized Recommendation via Prompting Large Language Models&lt;/a&gt;, the author used different prompt to generate various text description from the original content, and then embedding them as additional signals and feed into MLP for ranking together with the original descriptions. Below is a high level architecture of their model&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/llm-rec.png&quot; alt=&quot;LLM-Rec&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Dec 2023 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//How-to-use-GPT-for-recommendation-task/</link>
        <guid isPermaLink="true">https://pyemma.github.io//How-to-use-GPT-for-recommendation-task/</guid>
      </item>
    
      <item>
        <title>How to Design Webhook</title>
        <description>&lt;p&gt;Today, let’s discuss about how to design a system that could let customer to register webhook and send webhook requests to destination.&lt;/p&gt;

&lt;p&gt;Let’s first align on some terms that we are going to use across this post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;webhook provider&lt;/em&gt;: the platform that let customer to register webhook and send the webhook request&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;webhook customer&lt;/em&gt;: they provide the endpoint they would like the provider to send the webhook request to&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-webhook&quot;&gt;What is Webhook&lt;/h2&gt;

&lt;p&gt;For readers that are not familiar with Webhook, it is a type of &lt;strong&gt;notification mechanism&lt;/strong&gt; that communicates in one direction. This is a technique widely used in SaaS platform (e.g. Shopify, Strip, Slack) for external applications to receive data when some events they interested in happened on these platform.&lt;/p&gt;

&lt;p&gt;For example, &lt;em&gt;codingmonkey.com&lt;/em&gt; is a website that I’m running (hosted on AWS maybe), and I have a shop on Shopify that sells awesome keyboards. I could register a Webhook on Shopify so that whenever there are some Shopify users purchase awesome keyboards, a purchase event would be sent to an endpoint that is hosting on my server to process (e.g. store it in database, issue an invoice to purchaser, or send a thank you email).&lt;/p&gt;

&lt;p&gt;Sounds similar? Yeah, it sounds pretty like a &lt;em&gt;notification system&lt;/em&gt;. The difference here is that customer need to register webhook to express which event they would like to listen to and which endpoint URL the data need to be send to. There are also some other difference, such as we need to know if the webhook request is successfully received by &lt;em&gt;codingmonkey.com&lt;/em&gt; or not, and additional security check to protect the data we are sending. Excited to learn more? Let’s dive deep and see how we could build such a system.&lt;/p&gt;

&lt;h2 id=&quot;functional-requirement&quot;&gt;Functional Requirement&lt;/h2&gt;

&lt;p&gt;I didn’t find very crystal requirements on this, the following is some FR I summarized from the industrial examples&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Customer could register webhook and they could register multiple webhook&lt;/li&gt;
  &lt;li&gt;Support retry of webhook and minimize lost webhook as much as possible&lt;/li&gt;
  &lt;li&gt;Provide observability to customers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-functional-requirements&quot;&gt;Non Functional Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1B events pre day, which is equivalent to 10k qps for webhook trigger and request sending&lt;/li&gt;
  &lt;li&gt;High availability&lt;/li&gt;
  &lt;li&gt;The design should scale&lt;/li&gt;
  &lt;li&gt;Security&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;some-questions-to-clarify&quot;&gt;Some questions to clarify&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Do we allow event loss?
    &lt;ul&gt;
      &lt;li&gt;No, we should avoid event loss as much as possible.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What delivery semantic do we provide? At least once, at most once or exactly once?
    &lt;ul&gt;
      &lt;li&gt;At least once&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If we resend webhook, could we resume the endpoint to be idempotent?
    &lt;ul&gt;
      &lt;li&gt;Yes, but we need to provide necessary info to achieve that&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;high-level-design&quot;&gt;High Level Design&lt;/h2&gt;

&lt;p&gt;I would skip the API design and the back envelop estimation for the sake of sanction of this post. We would start simple to first meet the functional requirements, and then improve the availability, scalability of our system.&lt;/p&gt;

&lt;h3 id=&quot;webhook-registration&quot;&gt;Webhook Registration&lt;/h3&gt;

&lt;p&gt;We need to be able to let user to register webhook in our system. Below is a simple design of this part&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/webhook_registeration.png&quot; alt=&quot;Webhook Registration Flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The design is pretty simple, which we have web server to handle request from client and store the information in the &lt;strong&gt;webhook metadata database&lt;/strong&gt;. This metadata database is going to be used by the webhook delivery flow to figure out where to send the webhook request to.&lt;/p&gt;

&lt;p&gt;For each webhook, we would generate a unique &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;webhook_id&lt;/code&gt; as the unique identifier of each webhook. Besides that, we also need to store the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event_type&lt;/code&gt; that this webhook listen to, as well as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;owner_id&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event_type&lt;/code&gt; is a list of per-defined events that are available on our platform, which could be revealed via API document provided to customer. Besides that, we also need to store the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;url&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secret_token&lt;/code&gt; in the database, to know where we should send the request to, as well as sending the request safely. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secret_token&lt;/code&gt; could be used for authentication and encryption for sending the webhook requests. In this schema, customers could register multiple webhook within the system.&lt;/p&gt;

&lt;p&gt;One challenge here is that how to verify that customers have ownership on the urls they have provided. One common solution here is to send a test event to the endpoint they are providing, and ask them to verify they have received it; or by including a “challenge” in the request that the endpoints need to echo back (e.g. &lt;a href=&quot;https://www.dropbox.com/developers/reference/webhooks#documentation&quot;&gt;Dropbox webhook&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;webhook-delivery&quot;&gt;Webhook Delivery&lt;/h3&gt;

&lt;p&gt;Next, let’s take a look at the webhook request deliver flow, which is the meaty part of the entire system. As mentioned earlier, the entire system is similar to notification system, and thus I use notification system as a template for this design. Below is a high level design of this flow&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/webhook_delivery.png&quot; alt=&quot;Webhook Delivery&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this design, we adopted a &lt;em&gt;single responsibility strategy&lt;/em&gt; and separate the delivery logic into several components&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Webhook Controller&lt;/strong&gt; is responsible for processing the events (that are generated on our platform) and figuring out which endpoint we should send the data, as well as constructing the payload of the request. Here we assume that the events generated from our platform contains the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event_type&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;owner_id&lt;/code&gt; information (because we don’t want the event that happened in our shop to be delivered to others’ endpoints). With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event_type&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;owner_id&lt;/code&gt;, controller could retrieve the record from the &lt;strong&gt;metadata database&lt;/strong&gt; and construct a webhook request task. Once the task is constructed, controller would write an entry into &lt;strong&gt;Webhook Delivery Log&lt;/strong&gt; database to persistent this information, and set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_status&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PENDING&lt;/code&gt;, which we could leverage later for different retry strategy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Message Queue&lt;/strong&gt; is adopted to store the webhook request task, which worker would consume. Using message queue bring the following benefits, which outperform the additional complexity they bring:
    &lt;ul&gt;
      &lt;li&gt;controller don’t need to wait for the current webhook request to be delivered to process the next one (it is async okay). This not only saves resource, but also increases robustness (e.g. if worker failed, controller could still make progress and put job onto the queue instead of being blocked).&lt;/li&gt;
      &lt;li&gt;if there is a burst of events come in, message queue could help buffer the increased volume of task so that worker won’t be throttling.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Webhook Worker&lt;/strong&gt; is responsible for consume webhook request task from the queue, and send the actual HTTP POST request to the endpoint. The payload of the HTTP POST request could be something like this&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;event_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;created&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;field_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;value_&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;field_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;value_&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Worker would need to wait for the response from the endpoints, to know if the request has been successfully received. If received, then worker could update the record’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_status&lt;/code&gt; in the &lt;em&gt;Webhook Delivery Log&lt;/em&gt; database to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUCCEED&lt;/code&gt;; otherwise, different strategy of retry could be adopted to resent the webhook request. Supporting retry also means that we are providing &lt;strong&gt;at least once&lt;/strong&gt; semantic, which could result in duplicated request sent to endpoints. We expect these endpoints need to be idempotent, which is doable with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt; sent along with the HTTP POST request.&lt;/p&gt;

&lt;h3 id=&quot;webhook-retry-strategy&quot;&gt;Webhook Retry Strategy&lt;/h3&gt;

&lt;p&gt;One critical consideration for webhook system is the retry mechanism in case HTTP POST returns 4xx or 5xx code, or timeout. There are different retry strategies:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Retry immediately upon failure within a time range repeatedly, or until max retry limit&lt;/li&gt;
  &lt;li&gt;Exponential backoff within a time range (e.g. 24hrs), or until max retry limit&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For example, &lt;a href=&quot;https://stripe.com/docs/webhooks#retries&quot;&gt;Strip&lt;/a&gt; would attempts to delivery webhook up to 3 days with an exponential backoff. &lt;em&gt;Option 1&lt;/em&gt; is easy to implement, but the issue is that: if endpoint is returning error code, then it might take some time to mitigate the issue; immediate retry is likely to hit the same error, try again later time would be a better option.&lt;/p&gt;

&lt;p&gt;In order to achieve exponential backoff retry mechanism, we would use a cron version &lt;strong&gt;Webhook Controller&lt;/strong&gt;, which dose not consume the events from upstream, but scan the &lt;strong&gt;Webhook Delivery Log&lt;/strong&gt; database to identify the webhook requests that are still in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PENDING&lt;/code&gt; status and have not exceed the max retry. For each of such request, the controller would bump their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;retry_count&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;retry_timestamp&lt;/code&gt;, and publish a new task into message queue.&lt;/p&gt;

&lt;p&gt;The addition of this cron version &lt;strong&gt;Webhook Controller&lt;/strong&gt; could also help mitigate worker failure issue. For example, if one webhook http request is consumed and removed from the message queue by a worker, but suddenly the worker failed; since the task is already removed from the queue, other worker won’t able to get it and process it again. However, the cron controller would notice in from the log that there is one &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PENDING&lt;/code&gt; request and schedule it to retry.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another option is that if message queue provide the capability to persistent messages, worker could commit the position of the message in the queue they have processed, and if worker failed, it could resume from its last committed position and process the message again&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If for some endpoints, the failure is consistent for a certain time and over the threshold, we could temporarily mark the endpoints as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;disabled&lt;/code&gt; in the metadata’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status&lt;/code&gt; field to prevent new events from further deliver to them. And we could send alert email to customers to have them investigate into the issue. Once the issue is mitigated, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;status&lt;/code&gt; could be changed back, and we could consumer the delivery log to resume the webhook request; or use other channel, such as dump the entire data that need to be delivered during this time and send it over to customer.&lt;/p&gt;

&lt;h3 id=&quot;observability&quot;&gt;Observability&lt;/h3&gt;

&lt;p&gt;Since we have already log the status of each webhook request in &lt;strong&gt;Webhook Delivery Log&lt;/strong&gt; database, it is easy to support the observability. This could be implemented via having web application server to send a query to the database to aggregate the data and render it as a dashboard for customers. They could know how many webhook request have been sent, what’s the failure rate, etc.&lt;/p&gt;

&lt;h3 id=&quot;security&quot;&gt;Security&lt;/h3&gt;

&lt;p&gt;Security is especially important in webhook system. In &lt;a href=&quot;#webhook-registration&quot;&gt;webhook registration&lt;/a&gt; section, we authenticate that the endpoints belongs to users, we also need to authenticate ourself that the HTTP request is from us.&lt;/p&gt;

&lt;p&gt;One common approach is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/HMAC&quot;&gt;HMAC&lt;/a&gt; to sign the request with a shared secret with the user and sent the signature along with the request(e.g. Strip uses &lt;a href=&quot;https://stripe.com/docs/webhooks#verify-events&quot;&gt;this approach&lt;/a&gt;) and user could verify the signature with the shared secret. This shared secret could be auto generated upon user register webhook in our system, and show them to user in their monitor dashboard. This approach could also help us prevent replay attack, by including a timestamp used to expire webhook request.&lt;/p&gt;

&lt;p&gt;Another approach, which is less common, is to get a token from the consumer and add it to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Authorization&lt;/code&gt; header for validation. For example, if the owner of the endpoint has authorization server, then before sending webhook request, we could first obtain a &lt;a href=&quot;https://jwt.io/introduction&quot;&gt;JWT token&lt;/a&gt; and store it within our metadata table &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secret_token&lt;/code&gt; and use it each time we need to send webhook request.&lt;/p&gt;

&lt;p&gt;Besides the authentication problem, we also need to prevent the data we are sending could be read by others. There are also several options with different trade off:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid send sensitive information in the webhook payload. Instead, we could only send some entity id which is totally meaningless and ask customer to pull data again via other API. Pros is that this is the most safe approach, and the cons is that customer experience is worst&lt;/li&gt;
  &lt;li&gt;Another option is to encrypt the data with a shared secret key, which is only known between webhook provider and webhook consumer. A follow up of this question is how could we share this secret key safely between customer and provider over the unsafe network? Here we could use RSA encryption. (This is a general practice, RSA is safe, since only yourself know the private key; but the amount of data could be transferred via RSA is limited. So it makes since to use RSA to send another secret key, which is used for encryption/decryption of large volume of data)&lt;/li&gt;
  &lt;li&gt;Sending data with HTTPS and certificate pinning is also an option to safely transfer sensitive data, but this would have some performance hurt and require customer to have HTTPS setup such as CA&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;high-availability&quot;&gt;High Availability&lt;/h3&gt;

&lt;p&gt;Let’s see if there is any single point of failure in our current design. What comes to us first is the database and message queue. There are multiple replication strategy here we could use, each comes with different trade off:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For &lt;strong&gt;Webhook Metadata Database&lt;/strong&gt;, we could adopt single leader strategy, and have 2 followers. The followers could use synchronized replication, which provides good consistency, but the write throughput on the leader would be low; while if we use async approach, leader could handle more write request while could lead to consistency issues among leader and followers. If we are building for a geo webhook system, we might also consider multi-leader strategy, with better write request severing based on location and annoy of write conflict.&lt;/li&gt;
  &lt;li&gt;For &lt;strong&gt;Webhook Delivery Log Database&lt;/strong&gt;, besides the aforementioned strategy, we could also consider the quorum based replication, which provides the best write throughput and eventual consistency is acceptable in this case. (Q: what would be the worst case here).&lt;/li&gt;
  &lt;li&gt;For &lt;strong&gt;Message Queue&lt;/strong&gt;, similar to the database, we could also have replica setup so that the message is written to multiple node instead of single one. Also, even if we only have a single node queue and it failed. Since we are storing all scheduled webhook request in the &lt;strong&gt;Webhook Delivery Log Database&lt;/strong&gt;, the &lt;strong&gt;webhook controller (corn)&lt;/strong&gt; would identify the abnormal ones and try to reschedule them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For other components such as &lt;strong&gt;web app server&lt;/strong&gt;, &lt;strong&gt;webhook controller&lt;/strong&gt; and &lt;strong&gt;webhook worker&lt;/strong&gt;, they could be stateless. If a node fails, there would be other nodes available to continue the work.&lt;/p&gt;

&lt;h3 id=&quot;scalability&quot;&gt;Scalability&lt;/h3&gt;

&lt;p&gt;For scalability, we could horizontally scale &lt;strong&gt;web app server&lt;/strong&gt;, &lt;strong&gt;webhook controller&lt;/strong&gt; and &lt;strong&gt;webhook worker&lt;/strong&gt; by adding more nodes into the cluster. For database, we could shard it to scale if the total volume of data is too large to fit onto a single machine. Message queue could also be horizontally sharded by increase the number of partitions.&lt;/p&gt;

&lt;p&gt;There could be hotspot. For example, my awesome keyboard is so popular that lots of customer is visiting my shop and vast amount of events are triggered. To handle the hotspot, we could use a dynamic config to redirect the traffic of hotspot to specific cluster of machines, instead of starving the quote with other customers; or we could further shard the hotspot by some approach such suffix with numbers.&lt;/p&gt;

&lt;h3 id=&quot;other-optimization&quot;&gt;Other optimization&lt;/h3&gt;

&lt;p&gt;There are couple of other optimizations we could add to our system to make it more robust&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;we could have load balances in front of webhook controller to route based on machine utilization; also we could integrate the rate limit here to prevent abuse of the system (such as bot triggered events)&lt;/li&gt;
  &lt;li&gt;we could add a layer of cache to reduce the amount of read to metadata&lt;/li&gt;
  &lt;li&gt;we could add a rate limiter to help control the http request we send to customers; for some customers that have high security requirement, they might only trust http request sent from specific IPs, we could have dedicated VPC to support that needs&lt;/li&gt;
  &lt;li&gt;for observability, we could add some pre-compute mechanism to reduce the volume of data that the query need to scan; for example T-1 snapshot + on demand query on T&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is our final design
&lt;img src=&quot;/assets/webhook-final.png&quot; alt=&quot;Final Design&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://workos.com/blog/building-webhooks-into-your-application-guidelines-and-best-practices&quot;&gt;Building Webhooks Into Your Application: Guidelines and Best Practices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stripe.com/docs/webhooks&quot;&gt;Strip Webhook Doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dropbox.com/developers/reference/webhooks#documentation&quot;&gt;Dropbox Webhook Doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://shopify.dev/docs/apps/webhooks/best-practices&quot;&gt;Shopify Webhook Best Practices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zapier.com/engineering/webhook-design/&quot;&gt;Add Webhooks to Your API the Right Way&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://icyfenix.cn/architect-perspective/general-architecture/system-security/confidentiality.html&quot;&gt;Phoenix Architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 03 Dec 2023 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//How-to-Design-Webhook/</link>
        <guid isPermaLink="true">https://pyemma.github.io//How-to-Design-Webhook/</guid>
      </item>
    
      <item>
        <title>DDIA Chapter 11 Stream Processing Part I</title>
        <description>&lt;p&gt;In this post, we would introduce stream processing. Since it is a large topic, we would break it down into 2 part, and in the first part, we would focus on the component that is related to the “flow” of stream, a.k.a, &lt;strong&gt;delivery of message&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-is-event&quot;&gt;What is Event&lt;/h2&gt;
&lt;p&gt;Stream is composed by sequence of &lt;em&gt;event&lt;/em&gt;, which we also use &lt;em&gt;message&lt;/em&gt; as an alternative term. Here is a quote from confluent on describing what is &lt;em&gt;event&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An event is any type of action, incident, or change that’s identified or recorded by software or applications. For example, a payment, a website click, or a temperature reading, along with a description of what happened.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Take the payment as an example, a &lt;em&gt;payment event&lt;/em&gt;, could be User A paid X dollars to User B, for the purchase of an item C, on date X. This event would be recognized by our system to trigger the necessary processing (e.g. record in database, make third-party API call).&lt;/p&gt;

&lt;h2 id=&quot;how-to-deliver-message&quot;&gt;How to deliver message&lt;/h2&gt;
&lt;p&gt;How could we deliver message from machine A to machine B? There are multiple options.&lt;/p&gt;

&lt;h3 id=&quot;direct-connection&quot;&gt;Direct connection&lt;/h3&gt;
&lt;p&gt;The most straight-forward approach is to build a direct connection between A and B via network. Once the connection is published, B could receive the message from A in &lt;em&gt;2 different patterns&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Proactively asking A if there is new message with some intervals in between these ask&lt;/li&gt;
  &lt;li&gt;Passively wait until A notify that there are some message for B to read&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These 2 different patterns, more formally speaking, &lt;strong&gt;pull&lt;/strong&gt; and &lt;strong&gt;push&lt;/strong&gt;, is common approach on &lt;em&gt;how&lt;/em&gt; message is delivered, or how &lt;em&gt;consumer&lt;/em&gt; (B in our example) would receive the message.&lt;/p&gt;

&lt;p&gt;Direct connection works, but what would happen if B somehow offline for a period of time $T$? B would miss all the message A plans to deliver during $T$. One potential solution is to add the capability of storing the message temporarily within A, but that would increase the responsibility of A and make it more complexity. We need some sort of dedicated component to help us, this lead to &lt;em&gt;message broker&lt;/em&gt;, or &lt;em&gt;message queue&lt;/em&gt;, which is really good at this job.&lt;/p&gt;

&lt;h3 id=&quot;message-queue&quot;&gt;Message Queue&lt;/h3&gt;
&lt;p&gt;Message queue could be treated as some type of &lt;em&gt;buffer&lt;/em&gt; in between of the message sender, a.k.a producer, and message receiver, a.k.a consumer. Producer would publish message to message queue, message queue would do some “necessary” processing on the message and hold it. Consumer could retrieve these message from message queue, by subscribing to some queue. Since message is buffered in message queue, it is okay that B is offline when A tries to send message, message queue would hold that message, and when B comes online, the message is not lost and could be consumed.&lt;/p&gt;

&lt;h3 id=&quot;when-to-use-message-queue&quot;&gt;When to use Message Queue&lt;/h3&gt;
&lt;p&gt;Message queue is pretty good to be used when the business involves certain &lt;strong&gt;async&lt;/strong&gt; property, which means that user don’t expect an immediate response from the application, but could retrieve the result sometime in the future. Some typical case including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Job scheduler: user schedule a job (e.g. project building, model training) and expect it to finish sometime in the future&lt;/li&gt;
  &lt;li&gt;Youtube video encoding: when user upload a video, the encoding job would be pushed onto a queue and be processed by some worker in the future&lt;/li&gt;
  &lt;li&gt;Notification: a job to send some customer SMS/Email would be placed on queue and be sent in the future&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the later section, we would see some more concrete example from industry on how message queue is being used in practice.&lt;/p&gt;

&lt;p&gt;Everything has two sides. The benefits of using message queue is that: &lt;strong&gt;1. improve overall robustness of the system be decoupling different components&lt;/strong&gt;; &lt;strong&gt;2. balance the workload for upstream/downstream system (e.g. in case of burst of traffic)&lt;/strong&gt;. The downside of message queue is that, it would increase the complexity of the overall system (e.g. how to handle duplicated events gracefully).&lt;/p&gt;

&lt;h2 id=&quot;industry-practice&quot;&gt;Industry practice&lt;/h2&gt;
&lt;h3 id=&quot;rabbitmq--kafka&quot;&gt;RabbitMQ &amp;amp; Kafka&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.rabbitmq.com/&quot;&gt;RabbitMQ&lt;/a&gt; and &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; is 2 commonly adopted message queue in industry. For a deeper dive into these 2 message queue, we would put it into another post. Here we would first summarize some highlight of them:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;RabbitMQ&lt;/th&gt;
      &lt;th&gt;Kafka&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Message Persistent&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;control by request parameter&lt;/td&gt;
      &lt;td&gt;persistent&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Message Delivery&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;pull&lt;/td&gt;
      &lt;td&gt;push&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Message Ack&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;auto-ack or explicit ack&lt;/td&gt;
      &lt;td&gt;no ack, consumer commit offset&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;vertical&lt;/td&gt;
      &lt;td&gt;horizontal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;single node in general&lt;/td&gt;
      &lt;td&gt;leader-follower replication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Order Guarantee&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;FIFO in general, special case: priority, sharded queue, multi consumer&lt;/td&gt;
      &lt;td&gt;FIFO on partition level&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Consumer Load Balance&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;priority or round robin&lt;/td&gt;
      &lt;td&gt;different strategy specified by consumer group&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;doordash&quot;&gt;DoorDash&lt;/h3&gt;
&lt;p&gt;In this &lt;a href=&quot;https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/&quot;&gt;engineering blog&lt;/a&gt;, DoorDash introduced how they are using message queue in their business and why they migrate from RabbitMQ to Kafka.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Several business task in DoorDash is done in async, such as order checkout, merchant order transmission and dasher location processing&lt;/li&gt;
  &lt;li&gt;DoorDash use Celery + RabbitMQ as their initial async task processing infra. However, they identified several pain points:
    &lt;ul&gt;
      &lt;li&gt;Availability is low. RabbitMQ would easily down during peak traffic. Traffic control needs to be enabled to prevent the issue that task consumption could not keep up with task publishing, which cause serious network lagging.&lt;/li&gt;
      &lt;li&gt;Scalability is low. They are running the largest RabbitMQ node already (vertical scale). And they are using the primary-secondary HA mode, which also prevent them from scale (the down time could easily goes to 20mins to recover)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;They migrate RabbitMQ to Kafka to achieve better availability (partition replicated) and scalability (partitioned topic)
    &lt;ul&gt;
      &lt;li&gt;They also mentioned on improvement on dealing with “straggler”: using one dedicated thread to read message from topic partition, and use multi-threading to process the message. Thus, if one message takes long time to process, then only one thread would be blocked, while other thread could continues to process the messages&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;robinhood&quot;&gt;Robinhood&lt;/h3&gt;
&lt;p&gt;In this &lt;a href=&quot;https://newsroom.aboutrobinhood.com/part-i-scaling-robinhood-clearing-accounting/&quot;&gt;blog&lt;/a&gt; from Robinhood, the author introduced how they are using Kafka to build their clearing service (which is one critical service to make sure the inside and outside account information is insync).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Clearing service is not on the critical path of users (users don’t need to be aware of this), and thus they decided to build it as an async service.&lt;/li&gt;
  &lt;li&gt;In their initial design, they use a monolith consumer, which contains a giant transaction to make update to several tables. This raise the contention issue and the efficiency is low.&lt;/li&gt;
  &lt;li&gt;In their new design, they breakdown the original transaction into several smaller transaction to update only 1 ~ 2 tables. They also adopt the event source pattern that, once one job is done (e.g. user table update finished), it would fire one event to a Kafka topic, and one downstream consumer would consume the event and to the necessary update (e.g. update account table), and then fire another event.
    &lt;ul&gt;
      &lt;li&gt;The benefit of this reduction in contention and overall throughput improvement&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;But what if one consumer in the middle failed, how to resume and avoid duplicated write?
    &lt;ul&gt;
      &lt;li&gt;Use Kafka commit log to resume where left&lt;/li&gt;
      &lt;li&gt;When do the DB write, first update the lookup table, then the duplicated write would be no-op&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 21 Nov 2023 00:00:00 -0800</pubDate>
        <link>https://pyemma.github.io//DDIA-Stream-Processing-I/</link>
        <guid isPermaLink="true">https://pyemma.github.io//DDIA-Stream-Processing-I/</guid>
      </item>
    
      <item>
        <title>MIT Distributed System Course - KVRaft</title>
        <description>&lt;p&gt;It has been a long time since last update on the project. Finally I have found some time that I could resume this project and finish it up.&lt;/p&gt;

&lt;p&gt;In this post, I would mainly introduce the work on the LAB 3A, which is to leverage RAFT we have implemented in LAB 2 to build a reliable distributed key-value store. Before jumping into this part, I would also highlight some change to my RAFT implementation. I haven’t used GO language in my daily work a lot, and still adopting lots of philosophy in Python, which makes my implementation not elegant. I have learnt from ideas from online resources, which not only makes the code more readable, but also more reliable to pass the tests in the project.&lt;/p&gt;

&lt;h2 id=&quot;update-on-raft&quot;&gt;Update on RAFT&lt;/h2&gt;

&lt;p&gt;In the pervious &lt;a href=&quot;https://pyemma.github.io/Distributed-System-RAFT/&quot;&gt;post&lt;/a&gt;, I have used lots of different &lt;em&gt;background go routines&lt;/em&gt; to repeatedly checking if certain condition is meet and we need to trigger some actions. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A go routine to check if the leader election timeout needs to be triggered or not. This logic is jointly coupled with the AppendEntries/RequestVote RFC API where we need to handle the reset of the timer&lt;/li&gt;
  &lt;li&gt;A go routine to periodically send replica log request or heartbeat signal to other nodes, if the current node is the leader&lt;/li&gt;
  &lt;li&gt;When sending RequestVote rpc call, we start a new go routine, and use a condition variable to check if we have collect enough vote&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This implementation is not that elegant, as it losses a “causal relationship” among different events. After searching a little bit on the web, I learnt a new approach to implement, which is to use &lt;em&gt;channel&lt;/em&gt; as the media to pass signals, and use &lt;em&gt;select&lt;/em&gt; to organize the events that is happening concurrently.&lt;/p&gt;

&lt;p&gt;Here is a code snippet of the new design&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Raft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;startBackgroundEvent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;switch&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Leader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backToFollower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HeartbeatTimeout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcastAppendEntries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Follower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;votedCh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heartBeatCh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getElectionTimeout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Millisecond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convertToCandidate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Candidate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backToFollower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;winCh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convertToLeader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getElectionTimeout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Millisecond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convertToCandidate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For example, when follower make a vote upon receiving RequestVote RPC call from candidate, if we vote, then we would send a signal over the votedCh, this would suppress the election timeout, similar case when we receive heartbeat on AppendEntires RPC. For leader, unless it receive signal over backToFollower channel, which would be send if certain condition is met during handel RPC call response, it will periodically send the AppendEntries call to all nodes.&lt;/p&gt;

&lt;p&gt;In this change, I also move the leader commit and apply command on channel from background routine to be part of functions to broadcast AppendEntries RPC call.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Raft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sendAppendEntriesV2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AppendEntriesArgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AppendEntriesReply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;peers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raft.AppendEntries&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;defer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Term&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentTerm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Leader&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Term&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentTerm&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;// at this time we need to step down&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convertToFollower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Success&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;matchIndexNew&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PrevLogIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matchIndexNew&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matchIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matchIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matchIndexNew&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matchIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updateNextIdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matchIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updateCommit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;applyLogs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With the new design of the code, the test in lab2 could be passed more reliably.&lt;/p&gt;

&lt;h2 id=&quot;build-distributed-key-value-store-over-raft&quot;&gt;Build Distributed Key-value Store over RAFT&lt;/h2&gt;

&lt;p&gt;For the next part, let’s go over some details on how to build a distributed key-value store over RAFT. All of the code could be found in this &lt;a href=&quot;https://github.com/pyemma/mit-distributed-system/tree/master/src/kvraft&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Overall, the architecture of the key-value store is that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each kv server has a raft node peer, the kv server would only talk to its raft peer and no other communication.&lt;/li&gt;
  &lt;li&gt;Client would make RPC call to kv server. It would only talk to the server whose raft peer is leader, and it may take sometime for client to figure out who is leader. In this lab we just use round robin’s approach to check who is leader, in production we might consider establish such info into zookeeper for client to quickly know who is leader and if there is leader change&lt;/li&gt;
  &lt;li&gt;Upon each request received, kv server leader would submit the command to raft node for log replication. And it is going to listen on the applyCh channel to see if the command from some client’s request has been handled or not. Listening on applyCh for command to execute and response to the RPC call from client is happening on different go routines (one we created, one created based on how go handle RPC call). To coordinate them, we use channel to send signal.
    &lt;ul&gt;
      &lt;li&gt;We use the command index returned from raft Start() function as the identifier for our request and register a channel on it (using a map structure). In the listening routine, we read the command, execute it, and send the signal to the channel retrieved from the map.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt;  &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Millisecond&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ErrWrongLeader&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ErrWrongLeader&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;reply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OK&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-go&quot; data-lang=&quot;go&quot;&gt; &lt;span class=&quot;c&quot;&gt;// start the background thread to listent to the applyCh&lt;/span&gt;
 &lt;span class=&quot;c&quot;&gt;// to get the committed op and mutate the kv store&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;applyCh&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CommandIndex&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;       &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;     &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;CmdIdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CommandIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;// start to handle the committed op&lt;/span&gt;

      &lt;span class=&quot;c&quot;&gt;// handle the duplicated request, by checking the request id&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;lastId&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lastRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastId&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lastRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Get&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OK&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ErrNoKey&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Put&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OK&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OK&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OK&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Get&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ErrNoKey&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

      &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;DPrintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Finish processing one result %s, %s, %s, client %d, request %d, server %d&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ClientId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RequestId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;me&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

      &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;There is a timeout for each cleint’s RPC call. Client would resend the request to other server if there is timeout. However, we should only execute the “Put” and “Append” request only once. Sometime server might have already commit the command, but timeout and fail to response to client. This request us to have a mechanism of duplication identification. The solution I adopted is to attach a request id to each client’s request, and on the server we hold the latest request id we have executed. We directly skip the duplicated “Put”/”Append” request by checking the client id and request id.&lt;/li&gt;
  &lt;li&gt;For each follower, their raft peer receive the replicate their log according to leader, and return command committed to the applyCh. KV server just execute these commands and there is no need to handle the client request.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Initially my implementation could not pass all the test cases in the lab reliably. After some search on the web, I found the following 2 is the most critical part of the implementation&lt;/p&gt;

&lt;h3 id=&quot;use-command-idx-as-the-key-for-channel-to-signal-rpc-handler&quot;&gt;Use command idx as the key for channel to signal RPC handler&lt;/h3&gt;

&lt;p&gt;In the beginning, I was using client id + request id as the identifier of the channel. However, this approach is hard to manage correctly. Command idx is universally unique among RAFT, and use it as the identifier would greatly simply the management logic to signal RPC call.&lt;/p&gt;

&lt;p&gt;Also, one nits that simply the logic to manage channel a lot is that, we create the channel in the routine that is listening on applyCh channel and apply command to kv server’s state. Although the channel might be outdated and no RPC call is waiting on that, it could help avoid a accidentally sending signal on closed channel.&lt;/p&gt;

&lt;h3 id=&quot;adding-timeout-in-the-rpc-handler&quot;&gt;Adding timeout in the RPC handler&lt;/h3&gt;

&lt;p&gt;Although in the lab statement there is no explicit ask to add timeout on the RPC call, I found it is one of the most critical mechanism to implement to make the kv server working.&lt;/p&gt;

&lt;p&gt;One corner cases I have found could not be mitigated without timeout is as follow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sometime the network is partitioned, and once the pervious partitioned leader comes back, there might be a new round of leader election. In this case, we could hit a situation that, the current leader still becomes the leader, but we are in a new term right now&lt;/li&gt;
  &lt;li&gt;And then, if during this change, there is a RPC call happens and is waiting for response, without timeout mechanism, this request might waiting indefinitely. And the entire system might freeze and making no progress.
    &lt;ul&gt;
      &lt;li&gt;The reason of this issue is that, leader could not commit the entires in previous term. And a corner cases could happens that, although leader has replicate the logs all on followers, but just before leader update the commit index to commit the log, leader selection happens and we are in a new term. And although all follower has exactly same log with leader, leader could not update commit index because leader could only commit ones in its own term.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By adding timeout mechanism, we could break the above issue that. Client would send the same request again, and leader would add it to our log as well. Although on the log, we would have duplicated command, we have already added the dedup mechanism in our system to handle it. And since client send a new request, it would be added as a new log in new term. Once leader confirmed that this command has been replicated on all followers, it could commit all logs before it (including the duplicate one in pervious term), and thus the entire system could make progress smoothly.&lt;/p&gt;

&lt;p&gt;Debugging this corner case is really challenge. I would never forget the excitement when I finally found and understood the root cause.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jul 2022 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Distributed-System-KVRaft/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Distributed-System-KVRaft/</guid>
      </item>
    
      <item>
        <title>投资小白读《指数基金投资指南》</title>
        <description>&lt;p&gt;在上一篇&lt;a href=&quot;https://pyemma.github.io/Newbee-Reading-Investment-Books-I/&quot;&gt;blog&lt;/a&gt;中，我们多次提到了指数基金的定投是一个对于投资新手来说非常不错的方法。为此，我去专门找了一本讲解指数基金投资的书：《指数基金投资指南》。这本书比较吸引我的两个章节分别是如何选择指数基金，以及如何选择定投的策略。在这篇blog中，我会总结一些书中提到的一些方法和大家分享。&lt;/p&gt;

&lt;p&gt;这篇blog仅代表我个人观点，不构成任何具体的投资建议，特此声明。&lt;/p&gt;

&lt;h2 id=&quot;指数基金的选择方法&quot;&gt;指数基金的选择方法&lt;/h2&gt;

&lt;h3 id=&quot;道听途说法笑&quot;&gt;道听途说法（笑）&lt;/h3&gt;
&lt;p&gt;这个方法是我之前自己用的方法，也就是从网上的一些推荐股票的博主，或者投资的论坛的帖子来挑选要投资的ETF。这个方法没有任何的技术含量，而且有被忽悠的风险，谨慎选择。不过如果是规模比较大的ETF的话，其实还是相对比较靠谱。我现在自己一直在坚持定投的VOO就是通过这种方法了解到的。&lt;/p&gt;

&lt;h3 id=&quot;盈利收益率法&quot;&gt;盈利收益率法&lt;/h3&gt;
&lt;p&gt;所谓的盈利收益率，其实指的就是市盈率（PE）的倒数。它所代表的意义就是说，如果我们买下整个公司的话，这家公司一年的盈利能够带给我们的收益率。而所谓的盈利收益率法，其实就是在盈利收益率高的时候出手，买入ETF；而在盈利收益率低的时候卖出。书中给出的具体的参数是这样的：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;当盈利收益率高于10%的时候，分批投资&lt;/li&gt;
  &lt;li&gt;当盈利收益率低于10%但是高于6.4%的时候，持有&lt;/li&gt;
  &lt;li&gt;当盈利收益率低于6.4%的时候，卖出
盈利收益率法有其一定的局限性，只适用于流通性比较好，盈利比较稳定的品种。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;同时，书中给出的10%以及6.4%的阈值区间，主要是针对A股的。这个数值不一定适用于美股。如果有哪位大神知道适用于美股的阈值的话，欢迎评论！&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;博格公式法&quot;&gt;博格公式法&lt;/h3&gt;
&lt;p&gt;博格公式法，是指数基金之父约翰博格所提出来的。是针对能够影响股市长期回报率的几个关键因素所总结出的一种方法。他提出了指数基金的收益公式为&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;指数基金未来的年复合收益率，等于指数基金的投资初期股息率，加上指数基金每年的市盈率变化率，再加上指数基金每年的盈利变化率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而，这个公式的实际计算还是很复杂的（这个下面的例子可以看出来），但是根据这个公式中所包含的变量，我们其实可以得到一些简单的步骤来操作&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在股息率高的时候买入&lt;/li&gt;
  &lt;li&gt;在市盈率处于历史较低位置的时候买入&lt;/li&gt;
  &lt;li&gt;买入之后，等待市盈率从低到高&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;举个&quot;&gt;举个🌰&lt;/h3&gt;
&lt;p&gt;下面我们就用VOO这个ETF作为例子，来运用一下上面的两个方法。
PS: 数据的截取时间点是5/31/2021&lt;/p&gt;

&lt;p&gt;我们可以从Vanguard的&lt;a href=&quot;https://institutional.vanguard.com/web/c1/investments/product-details/fund/0968&quot;&gt;官网&lt;/a&gt;上找到VOO的一些数据（官网相比较其他的野鸡网站来说数据源更加靠谱，我个人觉得）。从这上面看，VOO这个ETF现在的PE值大概是26.3，那么其盈利收益率大概为3.80%。根据盈利收益率法，我们应该选择卖出VOO。&lt;/p&gt;

&lt;p&gt;然后我们可以再根据博格公式法来算算看VOO的未来年复合收益率。为了计算这个，我们需要知道股息率，市盈率变化率以及盈利变化率。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;股息率在五年前的时候是2.23% &lt;a href=&quot;http://www.lazyportfolioetf.com/etf/vanguard-sp-500-voo-dividend-yield/&quot;&gt;数据来源&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;市盈率变化率在过去五年中从5.65%变化到了现在的3.80%, 那么年平均变化率为-7.6%左右。（这个数字在官网上找不到，而且官网上也没有提供historical的PE的数据，所以我用了这个&lt;a href=&quot;https://www.gurufocus.com/etf/VOO&quot;&gt;资源&lt;/a&gt;上的数据&lt;/li&gt;
  &lt;li&gt;盈利变化率，在官网上能个得到，在过去五年中这个数值为18.9%&lt;/li&gt;
  &lt;li&gt;将上面的数据结合起来，我们能得到18.9 - 7.6 + 2.23 = 13.53%，这个与官网上实际给出的五年平均收益13.87%相差不多&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以比较Vanguard旗下的另外一个ETF VCR。这个是追踪必须非必须消费品指数的一个ETF。具体的计算结果如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;股息率在五年前的时候是1.63% &lt;a href=&quot;https://seekingalpha.com/symbol/VCR/dividends/yield&quot;&gt;数据来源&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;市盈率变化率在过去的五年中从3.34%变化到了现在的3.12，平均变化率为-9.9左右。&lt;/li&gt;
  &lt;li&gt;盈利变化率，在过去五年中数值为33.6%&lt;/li&gt;
  &lt;li&gt;带入公式我们能够得到 33.6 - 9.9 + 1.63 = 25.33%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由此可见，VCR相比较VOO而言，在博格公式法中更好，但是在市盈率中则要相对差一些。除此之外，VCR的管理费用为0.1%，这个比VOO的0.03%可是贵了3倍，也是不可小觑的一点。至于具体应该投资VOO还是VCR，这个就要靠投资人自己的喜好来决定了。&lt;/p&gt;

&lt;h2 id=&quot;指数基金的定投方法&quot;&gt;指数基金的定投方法&lt;/h2&gt;

&lt;h3 id=&quot;定时定量法笑&quot;&gt;定时定量法（笑）&lt;/h3&gt;
&lt;p&gt;这个是我自己之前一直在用的一种傻瓜式方法。就是每个月在固定的时间（比如说每个月的第一天），买入固定股数的ETF（比如说5股VOO）。这种方法自然不是最优的，但是简单无脑，省去很多需要操心的东西。少拿点收益，多空余一些时间去干点别的事情（比如说玩儿手游）。&lt;/p&gt;

&lt;h3 id=&quot;定时定资金&quot;&gt;定时定资金&lt;/h3&gt;
&lt;p&gt;这个和定时定量差不太多，只是每次固定投入的金额，比如固定从工资单中扣除2000美金投入到指数基金中。我最近被安利使用了一款智能定投的APP wealthfront，在这个APP中我就使用了这种方法。等年底的时候我回来通报一下实际的收益如何。&lt;/p&gt;

&lt;h3 id=&quot;不定时不定资金价格低于价值时定投&quot;&gt;不定时不定资金（价格低于价值时定投）&lt;/h3&gt;
&lt;p&gt;这个就是一个相对高端的方法了，书中给出的建议是这样的&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在盈利收益率大于10%的时候坚持定投 （或者市盈率/市净率处于历史底部区域的时候&lt;/li&gt;
  &lt;li&gt;盈利收益率小于10%但是大于6.4%的时候，暂停定投，继续持有；可以定投其他盈利收益率大于10%的品种 （或者当市盈率/市净率回归正常估值&lt;/li&gt;
  &lt;li&gt;盈利收益率小于6.4%的时候卖出 （或者当市盈率/市净率处于历史高位&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个方法蛮需要经验，来判断现在ETF的价值所处在的区间是什么样子的。我打算在投资这个领域先摸爬滚打一段时间之后再考虑使用这种高端的方法。&lt;/p&gt;
</description>
        <pubDate>Fri, 14 May 2021 00:00:00 -0700</pubDate>
        <link>https://pyemma.github.io//Newbee-Reading-Investment-Books-II/</link>
        <guid isPermaLink="true">https://pyemma.github.io//Newbee-Reading-Investment-Books-II/</guid>
      </item>
    
  </channel>
</rss>
