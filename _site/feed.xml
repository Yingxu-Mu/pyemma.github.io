<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yang Pei</title>
    <description>The effort a coding monkey paid when he managed to become a coding machine.</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>What I Read This Week 1</title>
        <description>&lt;h3 id=&quot;the-3-tricks-that-made-alphago-zero-work&quot;&gt;&lt;a href=&quot;https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef&quot;&gt;The 3 Tricks That Made AlphaGo Zero Work&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using &lt;strong&gt;better training sample&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;Use a single neural network to predict which &lt;strong&gt;move&lt;/strong&gt; to recommend &lt;em&gt;and&lt;/em&gt; which &lt;strong&gt;move&lt;/strong&gt; are likely to win the game (This is the idea of using &lt;a href=&quot;http://ruder.io/multi-task/index.html#introduction&quot;&gt;&lt;strong&gt;Multitask Learning&lt;/strong&gt;&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Use a upgrade version of neural network (from convolutional neural network to &lt;strong&gt;residual neural network&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intuitive-rl-intro-to-advantage-actor-critic-a2c&quot;&gt;&lt;a href=&quot;https://medium.com/@rudygilman/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752&quot;&gt;Intuitive RL: Intro to Advantage-Actor-Critic (A2C)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very vivid introduction of &lt;strong&gt;Advantage-Actor-Critic&lt;/strong&gt; reinforcement learning.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;First we should keep in mind that &lt;strong&gt;Actor-Critic&lt;/strong&gt; is a blend of both value estimation and policy estimation reinforcement learning method, a.k.a we will try to learn value function, as well as policy from game play (This is different from pure value function based method and policy based method).&lt;/li&gt;
  &lt;li&gt;In &lt;strong&gt;Actor-Critic&lt;/strong&gt;, the &lt;strong&gt;Actor&lt;/strong&gt; will tries to optimize the parameter for policy and &lt;strong&gt;Critic&lt;/strong&gt; will tries to optimize the parameter for the value function of a state. This can be done by having a single model outputting both the value of the state, as will as the probability of action.&lt;/li&gt;
  &lt;li&gt;By jump into one state, taking action and get reward. We will get the training examples for our &lt;strong&gt;Critic&lt;/strong&gt;. The estimate for each state will become more and more accurate. In this way, we don’t need to wait until the end of the game to get the value of each state, which is high in variance.&lt;/li&gt;
  &lt;li&gt;In stead of simply policy gradient update the policy (which tries to avoid the action that lead to a state with low value), we use &lt;strong&gt;Advantage&lt;/strong&gt;, which is the relative improvement of the action take (e.g. current state is -100, and take action A we arrive in a state with -20, the improvement of the action is 80!). The idea behind this is that the action might be the result that result in a low value.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ai-and-deep-learning-in-2017--a-year-in-review&quot;&gt;&lt;a href=&quot;http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/&quot;&gt;AI and Deep Learning in 2017 – A Year in Review&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A really awesome post that summarize what is going on in deep learning in 2017. Some points that I enjoy most:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evolution Algorithm (e.g. Genetic Algorithm) is coming back again.&lt;/li&gt;
  &lt;li&gt;Lots of deep learning framework is available right now: PyTorch is pretty popular in academic, but personally I thing TensorFlow is still the bests to try out (It’s also my plan to be more familiar with TensorFlow and work on some side project).&lt;/li&gt;
  &lt;li&gt;A good online reinforcement learning algorithm to read: &lt;a href=&quot;https://github.com/openai/baselines&quot;&gt;&lt;em&gt;OpenAI Baseline&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A good online courses: https://stats385.github.io/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;effective-modern-c&quot;&gt;Effective Modern C++&lt;/h3&gt;
&lt;p&gt;Mainly read the chapter about lambda function, some take away is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Avoid using default capture&lt;/li&gt;
  &lt;li&gt;In C++14, we can use init capture to move data we would like to use into the closure class, or use some expression to initialize the data member&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)](){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_unique&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()](){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Use decltype on auto&amp;amp;&amp;amp; parameters to forward them&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[](&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;decltype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Another thing to remember is to prefer using &lt;strong&gt;alias&lt;/strong&gt; declarations than &lt;strong&gt;typedefs&lt;/strong&gt;, because &lt;strong&gt;alias&lt;/strong&gt; supports templates better:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyAllocList&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyAlloc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sun, 14 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//What-I-Read-This-Week-I/</link>
        <guid isPermaLink="true">http://localhost:4000//What-I-Read-This-Week-I/</guid>
      </item>
    
      <item>
        <title>2018 新年计划</title>
        <description>&lt;p&gt;&lt;em&gt;2018年的主题，就是要变得自信以及学会Deep Work!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;工作&quot;&gt;工作&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;升职加薪
    &lt;ul&gt;
      &lt;li&gt;跳出自己的安逸区，接受更具有挑战的项目&lt;/li&gt;
      &lt;li&gt;加深对C++和Python的理解&lt;/li&gt;
      &lt;li&gt;加深机器学习领域的研究，多读post, blog, paper，每两周写一篇blog总结&lt;/li&gt;
      &lt;li&gt;提高自己的见识，要多和高人交流，学习别人的经验&lt;/li&gt;
      &lt;li&gt;(bonus)发表一遍学术论文，参加一次学术会议&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;学习&quot;&gt;学习&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;完成一个side project
    &lt;ul&gt;
      &lt;li&gt;starcraft AI?&lt;/li&gt;
      &lt;li&gt;photo style transfer?&lt;/li&gt;
      &lt;li&gt;chatbot?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;参加kaggle的比赛&lt;/li&gt;
  &lt;li&gt;一门新的语言: GO&lt;/li&gt;
  &lt;li&gt;一门新的计算机技术&lt;/li&gt;
  &lt;li&gt;每个月至少读一本书，每本书写一篇读后感&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;生活&quot;&gt;生活&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;胸围再大一圈，练出腹肌
    &lt;ul&gt;
      &lt;li&gt;每天坚持去gym，坚持每次练一组腹肌强化&lt;/li&gt;
      &lt;li&gt;每周晨跑三次&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;日语
    &lt;ul&gt;
      &lt;li&gt;每周安排计划固定学习的时间&lt;/li&gt;
      &lt;li&gt;参加N2考试&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;口琴&lt;/li&gt;
  &lt;li&gt;出去旅游两次&lt;/li&gt;
  &lt;li&gt;喝遍湾区的咖啡店，逛遍湾区的书店&lt;/li&gt;
  &lt;li&gt;(bonus) 每周去尝试一家新的饭店&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2018-%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/</link>
        <guid isPermaLink="true">http://localhost:4000//2018-%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/</guid>
      </item>
    
      <item>
        <title>Deep Work Reading Note</title>
        <description>&lt;p&gt;“Deep Work” is the first book I read this year. I was pretty impressed by the idea and methods the author purposed to help you gain the ability to do “deep work”, which means how to be concentrate on the work that can really generate value. In this note, I would like to summarize the main points of author, together with my personal experience.&lt;/p&gt;

&lt;p&gt;Deep work is the work which can really generate value, compared with shallow work, which is more like some routines that don’t need to think about too much to finish. Only by doing deep work, can we really improve ourselves, broaden our vision and shape our skills. As my personal experience, learning a new machine learning algorithm, or build a new APP counts to deep work, but repeatedly doing some interview algorithm exercise is shallow work.&lt;/p&gt;

&lt;p&gt;There are mainly 4 different strategies for deep work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Monastic: This is the most crucial strategies, which means that you are going to disconnected with all around and only focusing on the work you are doing all the time. This strategy does not apply to me, as I have work to do everyday and could not totally block others.&lt;/li&gt;
  &lt;li&gt;Bimodal: This is less crucial then the above one. For this one, you are not going to disconnected with the world all the time, but periodically. For example, for each year, you might choose two or three month to disconnect; or for each month, you choose one week to disconnect. This also does not apply for me, if I’m absent for too long, my manager would go crazy about me.&lt;/li&gt;
  &lt;li&gt;Rhythmic: This maybe the most common way to do deep work, which is to schedule some time each day dedicatedly. For example, every morning from 7:00 am to 9:00 am do deep work. This is the most suitable strategy for me, as well as for most of us. Although the “deepness” is not as powerful as the above two, but it still works pretty well. My plan is to change my schedule, and leave some time before go to work and go to bed, do some deep work.&lt;/li&gt;
  &lt;li&gt;Journalistic: This is the most flexible version, which is just to do deep work occasionally across the day, whenever you have some time. But this really need have strong deep work ability, so that you can switch to deep work easily. Because the switch will consume lots of power-will, which is limited resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides these strategies, there are also some tips to help do deep work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Come up with some rules when you do deep work, e.g. where to work (red rock cafe?), how to work (ban internet, no wechat) and how to support you deep work (relax music and good coffee)&lt;/li&gt;
  &lt;li&gt;Go to some really really really expensive places :)&lt;/li&gt;
  &lt;li&gt;Collaboration with others to help inspire your idea, but still work alone to do deep work&lt;/li&gt;
  &lt;li&gt;Execute like a business:
    &lt;ul&gt;
      &lt;li&gt;Have a small number of clear ambitious. I usually fail this one, as I was always trying to do lots of thing together to maximums my parallelism.&lt;/li&gt;
      &lt;li&gt;Record to number of deep hour you spend on deep work&lt;/li&gt;
      &lt;li&gt;Keep a compelling scoreboard to track your progress&lt;/li&gt;
      &lt;li&gt;Spend sometime to introspect how well you are doing deep work, what you success and what you fail to do&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Take good rest, e.g. stop working after 8:00 pm&lt;/li&gt;
  &lt;li&gt;Divide the working hour into some large block (deep work block and shallow work block). This would be extreme helpful for me, as I was easily distracted when I was writing code. What I plan to do is to have three long (more than 1hr) deep working blocks to focusing on coding, reading/writing important document/posts. Also schedule every minutes of the work time.&lt;/li&gt;
  &lt;li&gt;Do some meditate productively. What I plan is to spend some time walking around the campus
    &lt;ul&gt;
      &lt;li&gt;Be wary of distraction and looping. When you start to distract from your original problem, remind yourself and come back. Also you need to avoid the looping question you have already thought about&lt;/li&gt;
      &lt;li&gt;Structure deep thinking&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do some memory exercise to help you improve your concentration&lt;/li&gt;
  &lt;li&gt;Quit social media (no Facebook, wechat :) )&lt;/li&gt;
  &lt;li&gt;Know how to tell what is deep work and what is shallow work, and prioritize deep work.&lt;/li&gt;
  &lt;li&gt;Talk with your manager about the percentage of shallow work spend on each day.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//Deep-Work-Reading-Note/</link>
        <guid isPermaLink="true">http://localhost:4000//Deep-Work-Reading-Note/</guid>
      </item>
    
      <item>
        <title>2017 年终终结</title>
        <description>&lt;p&gt;2017年一转眼就过去了，自己也从一个刚刚毕业的学生成了一个快上了两年班的上班族了。趁着年末好好回顾一下这一整年发生的事情，一方面是给自己留个纪念，另一方面也是好好总结一下这一年的得失，找到改进的方向。&lt;/p&gt;

&lt;h3 id=&quot;工作&quot;&gt;工作&lt;/h3&gt;
&lt;p&gt;前半年工作比较顺利，但是后半年工作比较艰辛。首先最开心的事情就是年初得升职了，然后上半年的表现的评级也还不错，算是对自己能力的一个肯定。但是下半年由于种种原因，主要负责的项目一直进展非常不顺利，而且最后没能发布出去，很郁闷。辛辛苦苦干了大半年，每天基本从早上9点工作到晚上10点才回家，每个周末也去公司加班，但是还是事与愿违。而且由于一直专注于这个项目，导致疏忽了其他的方面工作，例如roadmap啊，mentor新人啊之类的，和上半年比起来差不少。&lt;/p&gt;

&lt;p&gt;反思了一下，感觉还是太焦躁心急，不够沉稳。心里总是想着早点升职多赚钱什么的，然而往往事与愿违。尤其是自己的计划被打乱之后，整个人就会变得非常的消沉，郁闷。一个很好的例子就是今年回国H1B签证被check，导致在国内待了两个月。当时的心情是非常的郁闷，还经常自己吓唬自己万一拿不到签证回不去美国咋办，好好的工作没了，自己怎么就这么倒霉。现在回头看感觉自己当时真是好笑，本来两个月的时间完全可以出去逛逛玩玩当个长假来休息，但是由于自己太重视这份工作，使自己一直纠结于被check的这一无法改变的事实，在家愁了两个月。不过经历过这一出之后，自己的心态也有所改善，脾气基本上是给磨没了。现在遇到点事情能比当时更沉得住气一些了。然而离自己理想的境界还有很长的距离， 希望在今后的日子里能在“不以物喜，不以己悲”的修炼上更进一步。&lt;/p&gt;

&lt;p&gt;另一个就是要想办法挑战自己。最近在工作上一直处于自己的舒适区，只是在做一些自己已经很熟练的工作而不去做一些自己没有尝试过的更具有挑战性的事情。很大的程度上这是因为担心脱离舒适区的后果，比如想换组，但是万一换到别的组表现不如现在的好影响了自己的评级了咋办？风险自然是有的，磨难肯定也是不少，但是只有经历过这些，才能成长，才能有进一步的提高，才能有更广的可能性。套用钢之炼金术师里面的台词，“在经历过这些磨难之后，就能获得钢铁般的心”。最近刚刚看过了吴军老师对于职业发展的一个观点：“稀缺性”。现在自己的工作，换一个人来干一样能干，都是一些简单的写写代码，调调参数什么的。这说明自己的稀缺性还不够，想要增加自己的稀缺性，那么就要敢于去尝试一些别人不能做的事情，这种事情往往风险比较大，存在于舒适区之外。另一方面，就是要增加自己的见识，对自己的领域要有一些比较深入的看法和见解，因为这个东西，别人是不可能轻易学去的，当然培养这方面也很困难，需要大量的经验积累，而这种经验，如果一直处于舒适区，是很难获得的。&lt;/p&gt;

&lt;h3 id=&quot;学习&quot;&gt;学习&lt;/h3&gt;
&lt;p&gt;年初定下来的好多计划最终都没有坚持下来：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;想要学口琴，但是一点都没有练过&lt;/li&gt;
  &lt;li&gt;想要学习日语，但是没能坚持下来&lt;/li&gt;
  &lt;li&gt;想要学习GO语言，但是根本就没有开始&lt;/li&gt;
  &lt;li&gt;想要看一些开源的代码，结果没看过&lt;/li&gt;
  &lt;li&gt;想要做一些side project，但是一个也没做出来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是还是有一些坚持下来了：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;每个月读一本书&lt;/li&gt;
  &lt;li&gt;学习了TensorFlow并且写了些简单的算法&lt;/li&gt;
  &lt;li&gt;自学了Reinforcement Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;感觉自己在增长新技能这方面有点不够专注，什么都想沾一手，但是最终的结果是什么都没有学到，典型的“想得太多，做得太少”。而且我还另一个特点，开始干一件事情的阈值很高，但是只要开始了，我就会坚持下去，尽量做到完美，这也表现出我不是一个半途而废的人，那么关键就是怎么开始了。所以在2018年，打算缩减自己的计划，把精力集中在两三个事情上，强迫自己开始并坚持下去。&lt;/p&gt;

&lt;p&gt;另一个就是自己不太擅长记录，比如读过的书，看过就扔了，没有提炼出一些个性化的东西；学过的一些技术，也是没有好好的总结，过一阵子也就忘记了。之前尝试过写读书笔记以及技术博客，但是都没有坚持下来。在新的一年里要坚持更新自己的博客。&lt;/p&gt;

&lt;h3 id=&quot;感情&quot;&gt;感情&lt;/h3&gt;
&lt;p&gt;感情这一年就是很扎心了。先是跟前女友分手了，之后被一个妹子表白第二天就又被发卡，现在联系一个新认识的妹子结果一直不怎么受待见。这很可能跟我负能量比较多，不够自信有关。我这个人看待事物的时候有点消极，遇到什么事情总是喜欢把消极面放大而不会从积极的焦虑去考虑。然后自信也比较低，总感觉自己比不上别人。感觉这是自己性格方面的两个很重要的缺陷。为此我还买了本书叫《正能量》，想看看从心理学的角度怎么能改正一下。总而言之一句话就是表现的乐观就会感觉乐观，表现的自信就会感觉自信，即行为影响情绪。所以平时说话多用积极的字眼，经常保持微笑，不整天愁眉苦脸的，来帮助自己树立正能量和自信。除此之外，也更加深入的明白了一个道理，找女朋友这件事绝对不能将就，要找就找自己喜欢的，想一辈子过日子的。不能抱着说，“先谈着，慢慢培养感情”，这种思想。培养感情是要建立在一定的冲动之上，如果连一点喜欢的感觉都没有，强迫自己培养感情喜欢上对方，对方痛苦，你自己也痛苦，还浪费时间跟精力，实在不是明智之举。&lt;/p&gt;

&lt;p&gt;当然啦，平时跟基友也总吐槽自己单身，埋怨自己怎么找不到女朋友啥的，但是其实单身的生活还是比较享受的。每天想在公司加班就加班，想回家打游戏就打游戏，周末想去吃个日料就去吃个日料，想到咖啡店看书就去咖啡店看书。时间充分的自由支配也挺好的。&lt;/p&gt;

&lt;h3 id=&quot;生活&quot;&gt;生活&lt;/h3&gt;
&lt;p&gt;生活方面感觉是这一年取得的成就最大的，那就是健身整整坚持了一整年，而且小有成效，再也不用为买什么衣服而发愁了哈哈。从上半年的每天坚持跑步，到下半年每天和同事去健身房做力量训练，虽然过程很痛苦，但是心里充满了成就感。年初的时候参加了公司组织的10km的长跑比赛，跑进了1小时；卧推现在能举起200lb以上；体重曾经一度控制在70kg，这些都是曾经我不敢想象的，然而通过自己的努力都做到了，非常的欣慰。&lt;/p&gt;

&lt;p&gt;另一个感觉很欣慰的事情就是成了一个狂热的咖啡和日料的爱好者。每个周末都会抽个半天去各种各样的咖啡店品尝咖啡看看书，体验一下不同的咖啡店的环境和氛围；晚上也会去各种日料店吃日料，顺便发个美食攻略贴。这些算是给自己单调的生活填上了一些乐趣。&lt;/p&gt;

&lt;p&gt;俗话说得好，“读万卷书，行万里路”。新的一年里想多出去走走，领略一下不同城市的风景。&lt;/p&gt;

&lt;h3 id=&quot;结语&quot;&gt;结语&lt;/h3&gt;
&lt;p&gt;不管2017年怎么样，都已经成为过去，不必太纠结于这一整年的失落和痛苦，让那些成为以后酒桌上的笑谈。昂首挺胸，迎接崭新的一年！&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Dec 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</link>
        <guid isPermaLink="true">http://localhost:4000//2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 8</title>
        <description>&lt;p&gt;This is the last lesson for the entire reinforcement learning, and in this lesson we will learn something related to exploit and explore. In machine learning service, like recommendation service, there is always a trade off between exploit and explore. Exploit means we are always choosing the best given the current information we have, while explore means try something new we haven’t tried yet. An example is if you go to restaurant, you can always go to the one you enjoy most(exploit), while you can also try a new one(explore).&lt;/p&gt;

&lt;p&gt;This problem is usually formularized as multi bandit problem, which can be represented as \(&amp;lt;A, R&amp;gt; \). Here \( A \) is a set of action we can take, and \( R^a(r) = P[R=r, A=a] \) is an &lt;strong&gt;unknown&lt;/strong&gt; probability distribution over rewards. At each time, our agent is going to pick an action, and the environment will generate a reward. The goal is to maximize the cumulative reward.&lt;/p&gt;

&lt;h4 id=&quot;regret&quot;&gt;Regret&lt;/h4&gt;
&lt;p&gt;We can measure the goodness of our action use &lt;strong&gt;regret&lt;/strong&gt;. Suppose the action value is the mean reward for an action \( a \)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(a) = E[r|a]&lt;/script&gt;

&lt;p&gt;and the optimal value \( V^\star \) is the max mean reward we can get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^\star = Q(a^\star) = max_{a\in A}Q(a)&lt;/script&gt;

&lt;p&gt;Then maximize the cumulative reward is equivalent to minimize the total regret, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_t = E[\sum_{i=1}^t (V^\star - Q(a_i))]&lt;/script&gt;

&lt;h4 id=&quot;upper-confidence-bound&quot;&gt;Upper Confidence Bound&lt;/h4&gt;
&lt;p&gt;We can try to solve this problem in the face of uncertainty. The best action we should try is the one that would on one hand has a high mean reward, and on the other hand have a high uncertainty. We might get a higher reward, which is good. While we can also get a worse reward, but that does not matter, since we can reduce our uncertainty about that action, and prefer other action which might have higher reward. A more formal description is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Estimate an upper confidence \( \hat{U_t}(a) \) for each action value, which depends on the number of times \( a \) has been selected, the larger the times, the smaller the upper confidence&lt;/li&gt;
  &lt;li&gt;Such that \( Q(a) \le \hat{Q_t}(a) + \hat{U_t}(a) \) with high probability&lt;/li&gt;
  &lt;li&gt;Select action maximize Upper Confidence Bound (UCB)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} \hat{Q_t}(a) + \hat{U_t}(a)&lt;/script&gt;

&lt;p&gt;We need to come up with some method to calculate the upper bound. Here, we bring &lt;em&gt;Hoeffding’s Inequality&lt;/em&gt; for help&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Let \( X_1,…, X_t \) be i.i.d. random variables in \( [0, 1] \), and let \( \bar{X_t} = \frac{1}{i} \sum_{i=1}^t X_i \) be the sample mean. Then&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[E[X] &gt; \bar{X}_t + u] \le e^{-2tu^2}&lt;/script&gt;

&lt;p&gt;With this we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[Q(a) &gt; \hat{Q_t}(a) + \hat{U_t}] \le e^{-2N_t(a)U_t(a)^2}&lt;/script&gt;

&lt;p&gt;where \( N_t(a) \) is the expected number of \( a \) is selected. We then can pick a probability \( p \) that true value exceeds UCB, and reduce $p$ as we observer more rewards, e.g. \( p = t^{-4} \). Then we could obtain the upper bound as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_t(a) = \sqrt{\frac{2logt}{N_t(a)}}&lt;/script&gt;

&lt;p&gt;And finally we have the UCB1 algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} (Q(a) + \sqrt{\frac{2logt}{N_t(a)}})&lt;/script&gt;
</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lesson-8/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lesson-8/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 7</title>
        <description>&lt;p&gt;In the pervious notes, we are all using &lt;strong&gt;model-free&lt;/strong&gt; reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1&quot;&gt;Lesson 1&lt;/a&gt; we know that a MDP can be represent by $&amp;lt;S, A, P, R&amp;gt;$, and our model is going to understand and simulate this. We will only introduce the simple version here, in which we assume that the $S$ and $A$ is known, and thus we only need to model $P$ and $R$. We can formulate it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{t+1} ~ P_\eta(S_{t+1}|S_t, A_t) \\
R_{t+1} = R_\eta(R_{t+1}|S_t, A_t)&lt;/script&gt;

&lt;p&gt;where the prediction of next state is a density estimation problem and the reward is a regression problem.&lt;/p&gt;

&lt;h4 id=&quot;integrated-architecture&quot;&gt;Integrated Architecture&lt;/h4&gt;
&lt;p&gt;In this architecture, we are going to consider two types of experience. &lt;strong&gt;Real experience&lt;/strong&gt; which is sampled from the environment, and &lt;strong&gt;Simulated experience&lt;/strong&gt; which is sampled from our model. In the past, we only use the real experience to learn value function/policy. Now, we are going to learn our model from real experience, then plan and learn value function/policy from both real and simulated experience. This is thus called integrated architecture (integration of real and fake), the &lt;strong&gt;Dyna Architecture&lt;/strong&gt;. Here is an picture to illustrate what the logic flow of Dyna is like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dyna.png&quot; alt=&quot;Dyna Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to the Dyna architecture, we can design many algorithm, here is an example of &lt;strong&gt;Dyna-Q Algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q(s, a)$ and $Model(s, a)$ for all $s$ and $a$&lt;/li&gt;
  &lt;li&gt;Do forever:
    &lt;ul&gt;
      &lt;li&gt;$S =$ current (nonterminal) state&lt;/li&gt;
      &lt;li&gt;$A = \epsilon - \text{greedy}(S, Q)$&lt;/li&gt;
      &lt;li&gt;Execute action $A$; observe result reward $R$, and state $S’$&lt;/li&gt;
      &lt;li&gt;$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$ (This is using real experience)&lt;/li&gt;
      &lt;li&gt;Update $Model(S, A)$ using $R, S’$&lt;/li&gt;
      &lt;li&gt;Repeat $n$ times: (This is using simulated experience to learn value function)
        &lt;ul&gt;
          &lt;li&gt;$S =$ random previously observed state&lt;/li&gt;
          &lt;li&gt;$A =$ random action previously taken in $S$&lt;/li&gt;
          &lt;li&gt;Sample $R, S’$ from $Model(S, A)$&lt;/li&gt;
          &lt;li&gt;$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;monte-carlo-tree-search&quot;&gt;Monte-Carlo Tree Search&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Monte-Carlo Tree Search&lt;/strong&gt; is a very efficient algorithm to plan once we have a model.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model $M_v$&lt;/li&gt;
  &lt;li&gt;Simulate $K$ episodes from current state $s_t$ using current simulation policy $\pi$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{s_t, A_t^k, R_{t+1}^k, S_{t+1}^k, ..., S_T^k} ~ M_v, \pi&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Build a search tree containing visited states and actions&lt;/li&gt;
  &lt;li&gt;Evaluate state $Q(s, a)$ by mean return of episodes from $s, a$&lt;/li&gt;
  &lt;li&gt;After search is finished, select current (real) action with maximum value in search tree&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In MCMT, the simulation policy $\pi$ improves. Each simulation consists of two phases (in-tree, out-of-tree):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree policy (improves): pick action to maximize $Q(S, A)$&lt;/li&gt;
  &lt;li&gt;Default policy (fixed): pick action randomly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Repeat (each simulation):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evaluate states $Q(S, A)$ by Mento-Carlo evaluation&lt;/li&gt;
  &lt;li&gt;Improve tree policy, e.g. by $\epsilon-\text{greedy}(Q)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are several advantages of MCMT:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Highly selective best-first search&lt;/li&gt;
  &lt;li&gt;Evaluates states dynamically&lt;/li&gt;
  &lt;li&gt;Uses sampling to break curse of dimensionality&lt;/li&gt;
  &lt;li&gt;Works for “black-box” models (only requires samples)&lt;/li&gt;
  &lt;li&gt;Computationally efficient, anytime&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 11 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lesson-7/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lesson-7/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 6</title>
        <description>&lt;p&gt;In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_{\theta}(s ,a) = P[a|s, \theta]&lt;/script&gt;

&lt;p&gt;In this case, we are not going to have any value function. A slight variance of this method is called &lt;strong&gt;Actor-Critic&lt;/strong&gt;, in which both value function and policy are modeled and learnt.&lt;/p&gt;

&lt;p&gt;The advantage of &lt;strong&gt;Policy based RL&lt;/strong&gt; is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Better convergence properties&lt;/li&gt;
  &lt;li&gt;Effective in high-dimensional or continuous action spaces&lt;/li&gt;
  &lt;li&gt;Can learn stochastic policies&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h4&gt;
&lt;p&gt;Since we are going to learn $\pi_\theta (s, a)$ and find the best $\theta$, we need to first find a way to measure the quality of our policy. These are called &lt;strong&gt;policy objective function&lt;/strong&gt; and some we can use are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In episode environment we can use the start value&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_1(\theta) = V^{\pi_\theta}(s_1) = E_{\pi_\theta}[v_1]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;In continuous environment we can use average value or average reward pre time-step&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_{avV}(\theta) = \sum_{s}d^{\pi_\theta}(s)V^{\pi_\theta}(s) \\
J_{avR}(\theta) = \sum_{s}d^{\pi_\theta}(s)\sum_{a}\pi_\theta(s, a)R_s^a&lt;/script&gt;

&lt;p&gt;where $d^{\pi_\theta}(s)$ is stationary distribution of Markov chain for $\pi_\theta$.&lt;/p&gt;

&lt;p&gt;After we have the measurement of the policy quality, we are going to find the best parameter which gives us the best quality and this becomes an optimization problem. Actually, similar to the last post, we can also use stochastic gradient to help use here. Since we are trying to find the maximum value, we are going to use what is called gradient ascent to find the steepest direction to update our parameter (very similar to gradient decrease).&lt;/p&gt;

&lt;h4 id=&quot;score-function&quot;&gt;Score Function&lt;/h4&gt;
&lt;p&gt;In order to compute the policy gradient analytically, we introduced the &lt;strong&gt;score function&lt;/strong&gt;. Assume policy $\pi_{\theta}$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta (s, a)$. Then using some tricky we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_\theta \pi_\theta (s, a) &amp; = \pi_\theta (s, a) \frac{\nabla_\theta \pi_\theta (s, a) }{\pi_\theta (s, a)} \\
&amp; = \pi_\theta (s, a) \nabla_\theta log \pi_\theta (s, a)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here, $\nabla_\theta log \pi_\theta (s, a)$ is the &lt;strong&gt;score function&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;For any differentiable policy $\pi_\theta (s, a)$, for any of the policy objective functions $J_1, J_{avV}, J_{avR}$, the policy gradient is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log \pi_\theta (s, a) Q^{\pi_\theta} (s, a)]&lt;/script&gt;

&lt;h4 id=&quot;monte-carlo-policy-gradient&quot;&gt;Monte-Carlo Policy Gradient&lt;/h4&gt;
&lt;p&gt;Use return as an unbiased sample of $Q^{\pi_\theta} (s, a)$, the algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $\theta$ arbitrarily
    &lt;ul&gt;
      &lt;li&gt;for each episode ${s_1, a_1, r_2, …, s_{T-1}, a_{T-1}, r_T} ~ \pi_\theta$ do
        &lt;ul&gt;
          &lt;li&gt;for $t = 1$ to $T - 1$ do
            &lt;ul&gt;
              &lt;li&gt;$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) v_t$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;end for&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;end for&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $\theta$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;actor-critic-policy-gradient&quot;&gt;Actor Critic Policy Gradient&lt;/h4&gt;
&lt;p&gt;The problem with Monte-Carlo Policy Gradient is that is has a very high variance. In order to reduce the variance, we can use a &lt;strong&gt;critic&lt;/strong&gt; to estimate the action value function. Thus in &lt;strong&gt;Actor Critic Policy Gradient&lt;/strong&gt;, we have two components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Critic&lt;/em&gt; updates action value function parameters $w$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Actor&lt;/em&gt; updates policy parameters $\theta$, in direction suggested by critic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example when we use linear value function approximation for the critic:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $s$, $\theta$&lt;/li&gt;
  &lt;li&gt;Sample $a ~ \pi_\theta$&lt;/li&gt;
  &lt;li&gt;for each step do
    &lt;ul&gt;
      &lt;li&gt;Sample reward $r$, sample next state $s’$&lt;/li&gt;
      &lt;li&gt;Sample action $a’ ~ \pi_\theta (s’, a’)$&lt;/li&gt;
      &lt;li&gt;$\delta = r + \gamma Q_w(s’, a’) - Q_w(s, a)$ (This is the TD error)&lt;/li&gt;
      &lt;li&gt;$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) Q_w(s, a)$ (We replace with the approximation)&lt;/li&gt;
      &lt;li&gt;$w = w + \beta \delta \phi(s, a)$ (Update value function approximation model parameter)&lt;/li&gt;
      &lt;li&gt;$a = a’$, $s = s’$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 10 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcment-Learning-Lesson-6/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcment-Learning-Lesson-6/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 5</title>
        <description>&lt;p&gt;In this post, we are going to look into how can we solve the real world problem with a practical way. Think of the state value function $v(s)$ or the action value function $q(s, a)$ we mentioned before. If the problem has a really large state space, then it would take a lot of memory to store each value function. Instead of recording each value function, we can actually use a model to approximate the actual value function, which means given the current state, we want to predict the value of the state. There are three types of value function approximation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input current state, output the state value&lt;/li&gt;
  &lt;li&gt;Input current state and an action, out put the action value&lt;/li&gt;
  &lt;li&gt;Input current state, output all possible action’s action value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This can be reviewed as a classical supervised learning problem if we &lt;strong&gt;know the actual value function&lt;/strong&gt;, and more accurately speaking, its a regression problem. In the regression problem, we are trying to fit a model which will output some real number that matches the our input label as much as possible. In the regression problem, the loss is defined using mean-square error. In order to get a model, we need first to do some feature engineering and represent each state using the &lt;strong&gt;feature vector&lt;/strong&gt; $x(S)$, this is going to be the input into our model. And then we try to minimize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w) = E_{\pi}[(v_{\pi}(S) - v(S, w))^2]&lt;/script&gt;

&lt;p&gt;Here $w$ is our model’s parameter and is what we are going to improve. $v_{\pi}(S)$ is the actual value (label) and $v(S, w)$ is the output from our model (predict). In order to minimize this loss, we use stochastic gradient decrease to update $w$, which we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w = \alpha (v_{\pi}(S) - v(S, w)) \nabla_w v(S, w)&lt;/script&gt;

&lt;p&gt;Here $\alpha$ is a learning rate controlling how fast we improve $w$, and $\nabla_w v(S, w)$ is the derivate of our model toward the parameter, for example, if we choose a linear model, where $v(S, w) = x(S)^T * w$, then we would have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w = \alpha (v_{\pi}(S) - v(S, w))x(S)&lt;/script&gt;

&lt;p&gt;However, we could only obtain this update when we really &lt;strong&gt;know the actual value function&lt;/strong&gt;, which is the case of supervised learning. However, in reinforcement learning, we are lack of such information. So we have to use some target to replacement them. We can actually combining it with the algorithm we have introduced before. For example the MC algorithm, In each episode, we will get a series of the state and corresponding return $&amp;lt;S_t, G_t&amp;gt;$, we can actually use this return as our target and train our model on it. The process would be like use our model to compute the state value, and use some policy to go through the process, then we would have $&amp;lt;S_1, G_1&amp;gt;, &amp;lt;S_2, G_2&amp;gt;, …, &amp;lt;S_T, G_T&amp;gt;$. Then use these as our training data and update our model. This training is &lt;strong&gt;on-policy&lt;/strong&gt; (because we are learning as well as behaving) and &lt;strong&gt;incremental&lt;/strong&gt; (episode by episode). Similar things can be applied to TD(0) and TD($\lambda$), where we use TD target and $G_t^\lambda$. Good news to use TD target is that is needs less steps for model to converge (since TD target is less variance), but it might not converge in some cases, for example, if we choose Neural Network as our model, then the model will blow up.&lt;/p&gt;

&lt;p&gt;Besides the incremental method, there is also &lt;strong&gt;batch&lt;/strong&gt; method, which we record all experience of the agent in $D$, and sample from it to get the training sample, then we update our model parameter using the same method above. &lt;strong&gt;Batch&lt;/strong&gt; method is more sample efficient and tries to find the best fit of all value functions available. While in the &lt;strong&gt;incremental&lt;/strong&gt; one, we are generate training sample one by one which is not very efficient, and we only use it once after update the parameter. A more detailed example is Deep Q-Networks (DQN), you can think of it as using NN model along with Q learning method. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Take action $a_t$ according to $\epsilon$-greedy policy&lt;/li&gt;
  &lt;li&gt;Store transition $(s_t, a_t, r_{t+1}, s_{t+1})$ in replay memory $D$&lt;/li&gt;
  &lt;li&gt;Sample random mini-batch of transitions $(s, a, r, s^,)$ from $D$&lt;/li&gt;
  &lt;li&gt;Compute Q learning target with an old, fixed parameter $w^-$&lt;/li&gt;
  &lt;li&gt;Optimize MSE between Q target and Q learning Network&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_i(w_i) = E_{s,a,r,s^, ~ D}[(r + \gamma max_{a^,}Q(s^,,a^,; w^-) - Q(s, a; w_i))^2]&lt;/script&gt;

&lt;p&gt;The key method that stabilize the model is experience reply and Q target. For the experience reply, it helps decouple the relationship between each step since we are randomly sampling. For the Q target, we are using the model several steps ago, not the model we just updated. You can think of this as avoid oscillation.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lesson-5/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lesson-5/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 4</title>
        <description>&lt;p&gt;In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.&lt;/p&gt;

&lt;h4 id=&quot;mento-calro-policy-iteration&quot;&gt;Mento Calro Policy Iteration&lt;/h4&gt;
&lt;p&gt;In the &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-2&quot;&gt;Lesson 2&lt;/a&gt;, we mentioned how to solve a MDP when we have full information about the MDP. One method is called &lt;strong&gt;Policy Iteration&lt;/strong&gt;. It can be divided into two components: &lt;em&gt;policy iterative evaluation&lt;/em&gt; and &lt;em&gt;policy improvement&lt;/em&gt;. For the evaluation part, we can use the methods in &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-3&quot;&gt;last lesson&lt;/a&gt;, nominally MC and TD. However, we could not directly use the state value function, cause in the policy improvement step (e.g. greedy), we need to know the $R$ and $P$ to find the best action (recall the &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1&quot;&gt;Bellman Optimality Function&lt;/a&gt;). However, action value function does not need the model of the MDP while in greedy policy improvement:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_*(s) = argmax_a q(s, a)&lt;/script&gt;

&lt;p&gt;For the policy improvement part. If we stick to the greedy method, it will not be good for us to explore all possible states. So we use another method which is called $\epsilon$-greedy. We will have $1-\epsilon$ probability to perform greedily (choose the current best action), and have $\epsilon$ probability to random choose an action:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\pi(s|a) = \begin{cases}
\frac{\epsilon}{m} + 1 - \epsilon, &amp; \text{if $a^\star = argmax_a Q(s, a)$} \\
\frac{\epsilon}{m}, &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have the final Mento Calro Policy Iteration as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sample the kth episode $S_1, A_1, …, S_T$ from policy $\pi$&lt;/li&gt;
  &lt;li&gt;For each state $S_t$ and $A_t$ in the episode&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N(S_t, A_t) = N(S_t, A_t) + 1 \\
Q(S_t, A_t) = Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}((G_t - Q(S_t, A_t))) \\&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Update the $\epsilon$ and policy:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon = 1/k \\
\pi = \epsilon\text{-greedy}(Q)&lt;/script&gt;

&lt;h4 id=&quot;sarsa-algorithm&quot;&gt;Sarsa Algorithm&lt;/h4&gt;
&lt;p&gt;If we use the logic in TD for the evaluation part, then we would have the sarsa algorithm. The main difference is that, in original TD, we use the value state function of the successor state, however, we need the action value function right now. We can obtain that by run our current policy again (remember, TD does not need the complete sequence of experience, we can generate the state and action along the way). Following is the algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0&lt;/li&gt;
  &lt;li&gt;Repeat for each episode
    &lt;ul&gt;
      &lt;li&gt;Initialize $S$, choose $A$ from the current policy derived from $Q$&lt;/li&gt;
      &lt;li&gt;Repeat for each step in the episode until we hit terminal
        &lt;ul&gt;
          &lt;li&gt;Take action $A$, observe $R$ and $S^\prime$&lt;/li&gt;
          &lt;li&gt;Choose $A^\prime$ from $S^\prime$ from the current policy derived from $Q$&lt;/li&gt;
          &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;Q(S, A) = Q(S, A) + \alpha(R + \gamma Q(S^\prime, A^\prime) - Q(S, A))&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Update $S = S^\prime, A = A^\prime$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, we can also use Eligibility Trace for the sarsa algorithm and result in sarsa($\lambda$) algorithm. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0
    &lt;ul&gt;
      &lt;li&gt;Repeat for each episode&lt;/li&gt;
      &lt;li&gt;Initialize $E$ for each $s, a$ pair to 0&lt;/li&gt;
      &lt;li&gt;Initialize $S$, choose $A$ from the current policy derived from Q&lt;/li&gt;
      &lt;li&gt;Repeat for each step in the episode until we hit terminal
        &lt;ul&gt;
          &lt;li&gt;Take action $A$, observe $R$ and $S^\prime$&lt;/li&gt;
          &lt;li&gt;Choose $A^\prime$ from $S^\prime$ from the current policy derived from Q&lt;/li&gt;
          &lt;li&gt;Calculate $\delta = R + \gamma Q(S^\prime, A^\prime) - Q(S, A)$&lt;/li&gt;
          &lt;li&gt;Update $E(S, A) = E(S, A) + 1$&lt;/li&gt;
          &lt;li&gt;For each $s$ and $a$ pair&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(s, a) = Q(s, a) + \alpha\delta E(s, a) \\
E(s, a) = \gamma\lambda E(s, a)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Update $S = S^\prime, A = A^\prime$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;q-learning&quot;&gt;Q Learning&lt;/h4&gt;
&lt;p&gt;Both MC policy iteration and sarsa algorithm are &lt;strong&gt;online learning&lt;/strong&gt; method, which means that they are observing there own policy, learning along the process. There is another category which is called &lt;strong&gt;offline learning&lt;/strong&gt;, in which we learn from other policy, not the policy we are trying to improving. Example is that a robots learns walking by observing human. Q learning falls in this category. It is pretty similar to the sarsa algorithm, the only difference is that when we get the action for successor state, we replace the $\epsilon$-greedy to greedy policy. The Q learning method is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize Q for each state and action pair arbitrarily, set Q(terminate, *) to 0&lt;/li&gt;
  &lt;li&gt;Repeat for each episode
    &lt;ul&gt;
      &lt;li&gt;Initialize $S$, choose $A$ from the current policy derived from Q&lt;/li&gt;
      &lt;li&gt;Repeat for each step in the episode until we hit terminal
        &lt;ul&gt;
          &lt;li&gt;Take action $A$, observe $R$ and $S^\prime$&lt;/li&gt;
          &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;Q(S, A) = Q(S, A) + \alpha(R + \gamma max_a Q(S^\prime, a) - Q(S, A))&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Update $S = S^\prime$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 19 Aug 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lesson-4/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lesson-4/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 3</title>
        <description>&lt;p&gt;In this lesson, we will learn about what to do when we have no knowledge about the MDP. In the last lesson, we learnt about how to solve a MDP when we have full information about it (e.g. $P$, $R$). When we don’t have enough information, the Bellman Equation won’t work. The only way is to learn from experience, where we run the process once, and obtain a $S_1, R_1, …, S_T$ sequence and improve our value function with it. This is called model free. In this lesson, we learn about when given a policy $\pi$, how do we calculate the state value function (which is called model free predicting). And in the next one, we will learn how to come up with the policy (which is called model free control).&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-reinforcement-learning&quot;&gt;Monte-Carlo Reinforcement Learning&lt;/h4&gt;
&lt;p&gt;The first method is called Mento-Carlo Reinforcement Learning. The idea behind this method is to use empirical mean to measure the value. The algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $N(s)$ to all zero, copying the value function from last one&lt;/li&gt;
  &lt;li&gt;Given an episode $S_1, R_1, …, S_T$&lt;/li&gt;
  &lt;li&gt;For each $S_t$ with return $G_t$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N(S_t) = N(S_t) + 1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S_t) = V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))&lt;/script&gt;

&lt;p&gt;Here $N(S_t)$ counts the number of our visit to $S_t$. The update function is using running mean to update the value of the current state, by moving it towards the return in this episode (G_t) a little bit. Here we can replace $\frac{1}{N(S_t)}$ to a small number $\alpha$, this is functioning as a learning rate to control how quick we update our value function. When we increase the counter, we can increase it either by first visit within the episode or every visit within the episode.&lt;/p&gt;

&lt;p&gt;Mento-Carlo Reinforcement Learning can only works with episode experience, which means the MDP must has a terminate state and the experience must be complete.&lt;/p&gt;

&lt;p&gt;In this method, we are &lt;strong&gt;sampling&lt;/strong&gt; from the policy distribution because for each state, we are only considering one possible successor state. The learning method in last lesson is using dynamic programming, and it is not based on sampling, it actually takes all possible successor states into consideration.&lt;/p&gt;

&lt;h4 id=&quot;td0-learning&quot;&gt;TD(0) Learning&lt;/h4&gt;
&lt;p&gt;The second method is called Temporal Difference Learning. As its naming suggested, in this method we are not using the actual return in the episode but using an temporal estimation to update the value function. The algorithm is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For each $S_t$ within the episode&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S_t) = V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))&lt;/script&gt;

&lt;p&gt;Here, $R_{t+1} + \gamma V(S_{t+1})$ is called TD target, and $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ is called the TD error. The main logic here is bootstrapping, which means we are not directly making each value function to the most accurate value it should be given this episode. We are making it slightly better based on our current estimate on the successor state. The benefit of the doing so is that we can learn from incomplete experience, and MDP without a terminal state.&lt;/p&gt;

&lt;p&gt;In this method, we are also &lt;strong&gt;sampling&lt;/strong&gt; from the policy distribution, as well as bootstrapping. Dynamic programming also uses bootstrapping similar to TD(0) learning (recall the Bellman Equation).&lt;/p&gt;

&lt;h4 id=&quot;tdlambda-learning&quot;&gt;TD($\lambda$) Learning&lt;/h4&gt;
&lt;p&gt;In both MC and TD(0) Learning, we are looking forward to the future rewards. In MC, we are looking until we reach the end, while in TD(0) we only look at next step. Instead of looking forward, we can also looking backward. However, this involves how to assign the current timestamp rewards to pervious states. This is called credit assignment problem. And the method we overcome it is to use &lt;strong&gt;Eligibility Traces&lt;/strong&gt;, which fusion both assigning credit to the most recent state and most frequent states. Here we introduce the TD($\lambda$) algorithm (back view version):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize Eligibility Traces $E_0(s) = 0$&lt;/li&gt;
  &lt;li&gt;Given an experience, for each state $s$:&lt;/li&gt;
  &lt;li&gt;Update the Eligibility Traces by: $E_t(s) = \gamma \lambda E_{t-1}(s) + 1(S_t = s)$&lt;/li&gt;
  &lt;li&gt;Calculate the update step by: $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$&lt;/li&gt;
  &lt;li&gt;Update &lt;strong&gt;each&lt;/strong&gt; state by: $V(s) = V(s) + \alpha \delta_t E_t(s)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we use $\lambda = 0$, then the Eligibility Traces will fall to $1(S_t = s)$ and replace it in the update function, we will see that its the exact same update function as TD(0). If we choose $\lambda = 1$, then it is actually equals to every visit MC. We can prove it as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Suppose in our experience, $s$ is visited at timestamp $k$, then the $E_t(s)$ will be like&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E_t(s) = \begin{cases}
0, &amp; \text{if $t &lt; k$} \\
\gamma^{t - k}, &amp; \text{if $t \ge k$} \\
\end{cases} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The accumulated online update for $s$ is&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\sum_{t=1}^{T-1}\alpha\delta_t E_t(s) &amp; = \sum_{t=k}^{T-1}\gamma^{t-k}\delta_t \\
&amp; = \delta_k + \gamma\delta_{k+1} + ... + \gamma^{T-1-k}\delta_{T-1} \\
&amp; = R_{k+1} + \gamma V(S_{k+1}) - V(S_k) + \gamma R_{k+2} + \gamma^2 V(S_{k+2}) - \gamma V(S_{k+1}) + ... \\
&amp; + \gamma^{T-1-k} R_{T-1} + \gamma^{T-k} V(S_T) - \gamma^{T-1-k} V(S_{T-1}) \\
&amp; = R_{k+1} + \gamma R_{k+2} + \gamma^2 R_{k+3} + ... + \gamma^{T-1-k} R_{T-1} - V(S_k) \\
&amp; = G_k - V(S_k)
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Thus the update function for TD(1) is the same as the one in every visit MC (where we use $\alpha$ as a learning rate instead of the original one).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The good thing for TD($lambda$) is that it can learn with incomplete experience. And the update is performed &lt;em&gt;online&lt;/em&gt;, &lt;em&gt;step by step&lt;/em&gt; within the episode. MC is updated via offline, cause it needs to wait until the end and calculate the update for each state and update them in batch.&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Aug 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lession-3/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lession-3/</guid>
      </item>
    
  </channel>
</rss>
