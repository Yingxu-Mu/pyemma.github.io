<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yang Pei</title>
    <description>The effort a coding monkey paid when he managed to become a coding machine.</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>DQN In Practice</title>
        <description>&lt;p&gt;Recently I have been working on Deep-Q-Learning and apply it to some interesting AI games. In this post, I would like to give a brief introduction to how I implemented the Deep-Q-Learning, as well as lots of learning along the way.&lt;/p&gt;

&lt;h3 id=&quot;what-is-dqn&quot;&gt;What is DQN&lt;/h3&gt;
&lt;p&gt;To understand DQN, we need first know is prototype, Q-Leanring. Here is a pervious post about &lt;a href=&quot;https://pyemma.github.io/Reinforcement-Learning-Lesson-4/&quot;&gt;Q-Learning&lt;/a&gt;. Some core elements are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have a \( Q(s, a) \) to record for each state and action pair, what is the expected reward we can get from them&lt;/li&gt;
  &lt;li&gt;We update this estimation by finding what is the &lt;strong&gt;max&lt;/strong&gt; reward we can get from the next state leaded by our current state and action, update it by &lt;script type=&quot;math/tex&quot;&gt;Q(S, A) = Q(S, A) + \alpha(R + \gamma max_a Q(S^\prime, a) - Q(S, A))&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we have limited number of state and action, we can hold these information into a simple lookup table. However, in reality we usually deal with unlimited number of state and action. In this case, a lookup table is not scalable, we use a model to simulate this part: describe the state with some features, tell the model and the model will tell us what \( Q(s, a) \) would be, the model would be trained and updated along the way with the examples we have.&lt;/p&gt;

&lt;p&gt;Deep-Q-Leanring basically is a combination of the above two ideas. Apply the logic of Q-Learning, with a model measuring the \( Q(s, a) \). Here the &lt;em&gt;Deep&lt;/em&gt; comes from the fact that we usually use &lt;em&gt;Deep-Neural-Network&lt;/em&gt; as our model. However, there is another two important thing to stabilize the training of DQN:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Experience Replay&lt;/strong&gt;: Instead of directly using the most recent example, we keep a pool of past experience and sample a batch from this pool to update our model&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q-Target Network&lt;/strong&gt;: Instead of the max value output by our current model, we use the version of several steps ago. This is called the Q-Target model and this model will be frozen and not updated, but occasionally copied from our main model.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dqn-implementation&quot;&gt;DQN Implementation&lt;/h3&gt;
&lt;p&gt;Cool, as we have some highlight idea on what DQN is, let’s see how it is implemented. The code is &lt;a href=&quot;https://github.com/pyemma/tensorflow/blob/master/util/dqn.py&quot;&gt;here&lt;/a&gt;. Please not that this code is currently not generalized yet and only suitable for training &lt;em&gt;Cartpole&lt;/em&gt; game due to how we parsing the state. Making it generalized is WIP. However, that does not prevent us from understanding the main idea of DQN. Let me now illustrate some important component:&lt;/p&gt;

&lt;p&gt;Let’s first take a look at the main training logic:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsiode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Train the model
    Args:
        epsiode:        Number of epsiode to train
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_start&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsiode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_remember&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_to_copy_graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_copy_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_decay&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For each episode, we first initialize the state, and before the game terminate, we take a action based on our model and policy, then get the reward and next state for that action. We then put this as an experience into our memory pool. After the game is terminated, we update our model, and check if we should update q-target network. We also decrease the epsilon as we play. Here we are using \( \epsilon \)-greedy policy, and this parameter is the tradeoff between explore and exploit.&lt;/p&gt;

&lt;p&gt;Now lets take a look at how we train the model:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Use Experience Replay and Target Value Network to learn
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;q_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here, we sample a batch of experience from our memory pool. Then prepare it into the right format. Our goal is to train our model’s prediction (in this case, the prediction is the value of each action) is the same as the actual reward + q-target. In the code, we first get the model prediction for all actions. We also get q-target prediction for each action. We then update the value for the action we take to the target value. Then we train our model using this updated value. Since we only updated the value of action we took, the model will only learn from these updated value, all other is the same as before and model would not learn from them.&lt;/p&gt;

&lt;h3 id=&quot;dqn-in-practice&quot;&gt;DQN In Practice&lt;/h3&gt;
&lt;p&gt;During the implementation of this feature, I encountered lots of problem and would like to notice them down for further discussion:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Initially I updated the model &lt;strong&gt;after we take each action&lt;/strong&gt; instead of &lt;strong&gt;after each game&lt;/strong&gt;. This will dramatically increase the number of training we have and impact on the training time. However, getting more number of training is not always a good thing. I noticed that in my case, the training would be not stable.&lt;/li&gt;
  &lt;li&gt;Parameter tuning is really challenging. I tried different combination of batch size, memory pool size, learning rate, and model arch. I found that usually have a moderate memory pool size with a larger learning rate is beneficial.&lt;/li&gt;
  &lt;li&gt;The step to copy the q-target network is also hard to set. If we set is too small, then the training is less stabile; if too large, the training does not get improved.&lt;/li&gt;
  &lt;li&gt;I feel like the usage of the memory is not good enough, as there is not difference in terms of success experience and failure experience. From our common sense, we know that we learn more from our bad experience, maybe we should skew more onto the bad experience?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//DQN-In-Practice/</link>
        <guid isPermaLink="true">http://localhost:4000//DQN-In-Practice/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 2</title>
        <description>&lt;h3 id=&quot;evolving-search-recommendations-on-pinterest&quot;&gt;&lt;a href=&quot;https://medium.com/@Pinterest_Engineering/evolving-search-recommendations-on-pinterest-136e26e0468a&quot;&gt;Evolving search recommendations on Pinterest&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A post introducing the search work done in Pinterest.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initially they use a &lt;em&gt;Term-Query graph&lt;/em&gt; to generate candidates. In this graph, each term (a single word) is represent a node, as well as the query. Each term node is connected to the query, weighted by the reciprocal of the number of queries that term shows up in. Each query node is also connected to query node, weighted by the relativeness. Most visited queries are recommended.&lt;/li&gt;
  &lt;li&gt;They later changed to &lt;em&gt;Pixie&lt;/em&gt;, a graph based recommendation platform. The graph is build using query and pins. Compared with pervious solution, this solution will not break the query and thus keep the semantic information. &lt;strong&gt;To give better recommendation, semantic information is important.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;They have further work to utilize embeddings for queries. Is embedding based candidate generation works better than random walk based candidate generation?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;an-overview-of-multi-task-learning-in-deep-neural-networks&quot;&gt;&lt;a href=&quot;http://ruder.io/multi-task/index.html#introduction&quot;&gt;An Overview of Multi-Task Learning in Deep Neural Networks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A pretty good blog introducing what MTL is, who is works and some recent works. Here are some points I think most beneficial:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Using one sentence to explain MTL: “By sharing representation among related tasks(leveraging different domain-specific knowledge), the generalization of model is improved.”&lt;/li&gt;
  &lt;li&gt;Why MTL work:
    &lt;ul&gt;
      &lt;li&gt;Our training data will always contains some noise. Training a single goal on the data will easily get overfit. However, training on multiple tasks simultaneously will help cancel out this noise (&lt;strong&gt;Data Augmentation&lt;/strong&gt;).&lt;/li&gt;
      &lt;li&gt;By training on multiple tasks and sharing the same representation at the same time, the model will try to find a more general hypothesis that would work for all tasks (&lt;strong&gt;Regularization&lt;/strong&gt;).&lt;/li&gt;
      &lt;li&gt;Some feature combination might be pretty complex in one task and hard to let the model to capture, but easy in the other one. MTL will help sharing this info between tasks(&lt;strong&gt;Feature Engineering&lt;/strong&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are mainly two form of MTL:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Hard Parameter Sharing&lt;/strong&gt;: Different tasks will have several same layers at the lower level, and have their own layers at higher level. A common use-case is that, when can use the bottom layers in VGG, and then train our own layer on our task.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Soft Parameter Sharing&lt;/strong&gt;: Different tasks will have their own model, but each model can constraint each other to not differ too much.&lt;/li&gt;
      &lt;li&gt;Currently, &lt;strong&gt;Hard Parameter Sharing&lt;/strong&gt; is still very popular, but &lt;strong&gt;Soft Parameter Sharing&lt;/strong&gt; is more promising as it let the model to learn what to share.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;welcoming-the-era-of-deep-neuroevolution&quot;&gt;&lt;a href=&quot;https://eng.uber.com/deep-neuroevolution/&quot;&gt;Welcoming the Era of Deep Neuroevolution&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Uber AI Lab’s work on using Genetic Algorithm instead of SGD to optimize DNN on reinforcement learning tasks.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GA can produce comparatively similar result as SGD.&lt;/li&gt;
  &lt;li&gt;They purposed a method to smartly guide the mutation to put more attention on the sensitive feature, to solve the problem GA has when dealing with large networks.&lt;/li&gt;
  &lt;li&gt;They also purposed a method to enforce the exploration, which they try to have a population of candidates that act differently from each other as much as possible (unlikely to trapped in local minima).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;effective-modern-c&quot;&gt;[Effective Modern C++]&lt;/h3&gt;
&lt;p&gt;Mainly read the smart pointer part.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;unique_ptr&lt;/code&gt; performs similar to the old fashion raw pointer. It indicates a exclusive ownership to the object it manages, thus it can only be moved not be copied. We can specify a custom deleter to a unique pointer, and the deleter would become part of the unique pointer’s type. It is very convenient to covert a unique pointer to a shared pointer.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt; performs similar to the garbage collection in Java. It indicates a shared ownership to the object. The underlay mechanism is that each shared pointer will create a control block that would keep the reference count and other data. Since there is a separate object holding all extra info, the deleter we passed to shared pointer will not become a part of its type. Remember not using the raw pointer to initialize a shared pointer, as it is pretty dangerous and we might result in free the object multiple times. This is extremely the case when we are working with the &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; pointer. To be able to safely create a shared pointer from &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;enable_shared_from_this&lt;/code&gt; template.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;enable_shared_from_this&lt;/code&gt; uses &lt;em&gt;Curiously Recurring Template Pattern (CRTP)&lt;/em&gt; (to be add more details later).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;weak_ptr&lt;/code&gt; is like &lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;, but it does not effect the reference count on the object, and the object it is pointing to might be destroyed. The use case for &lt;code class=&quot;highlighter-rouge&quot;&gt;weak_ptr&lt;/code&gt; can be &lt;strong&gt;cacheing&lt;/strong&gt;, &lt;strong&gt;observer lists&lt;/strong&gt; and the prevention of &lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt; cycles.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 20 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//What-I-Read-This-Week-2/</link>
        <guid isPermaLink="true">http://localhost:4000//What-I-Read-This-Week-2/</guid>
      </item>
    
      <item>
        <title>读书有感 1</title>
        <description>&lt;p&gt;最近两天读了两本书，分别是武志红的《你就是答案》和史蒂芬柯维的《高效能人士的七个习惯》。这两本书都算是提升自我修养的书，但是有人可能这时候就嚷嚷说：“啊，这都是鸡汤，不要被鸡汤给灌糊涂了”。诚然这两本书的确有鸡汤的性质，但是里面的内容还是很在理的，即使说这些东西你其实早就知道，但是再读一读，还是能够进一步加深自己的认知和理解的。下面我就着重说几个给我印象最深刻的部分。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;生命是一个历程，一个整体，我们不能太在乎一时的得失，成长才是最重要的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这一点对我的触动是最大的。在现实生活中，我是一个得失心非常重的人，经常把眼前的结果看的非常非常重要。比如说最近的年底考核，我上个half每天加班工作，就是希望最后能得到升职的机会，然而事与愿违，今天跟老板meeting，说我没能升上去，打击很大。这就开始有点自暴自弃，不想再好好干活了，成了一个“自断经脉的打工族”。这正反应出，我自己并没有什么人生目标，认为自己的价值就是根据这种外界的物质来衡量的，所谓的“以荣誉为中心的生活”。我应该学会调整自己，拒绝以这种外界的物质来衡量自己，而是真正的明白自己想成为什么样的人，想做什么样的事情。我们要有一个好的人生规划，包括一个一以贯之的目标，以及从整体的角度看待生命。想找的目标不是一件容易的事情，我打算参考《高效能人士的七个习惯》中的“个人使命宣言”来逐渐认清自己，找到自己的人生目标。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;遇到挫折要做到对事不对人&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我发觉自己的“挫折商”不够强大，遇到点不顺心的事情，就开始夸大其后果，否定自己的能力，任由其影响自己。挫折商可以从四个方面来衡量，控制，归因，延伸以及忍耐。想要提升挫折商，试试下面的方法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;任何时候，都要问问自己，“我可以做什么”。&lt;/li&gt;
  &lt;li&gt;内向归因，问题发生时，问问自己该承担什么样的责任，而自己又可以如何去改变。&lt;/li&gt;
  &lt;li&gt;不把问题扩大。哦，有失败发生了，但它就是这么一件事情而已，这并不意味着我其他方面有问题。&lt;/li&gt;
  &lt;li&gt;如果你真的确定一件事可以做，它是有道理的，那么努力将其进行到底。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;跳好爱情的双人舞&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之前的好一段时间都被感情的事情困扰：要么就是自己心仪很久的女生追求不到，要么就是新认识的女生对自己爱答不理的。很自卑，一度有“注孤生”的感觉。但是其实这大可不必要，保持积极乐观的态度，坦然真诚的对待女生，相信自己值得被爱，总是能遇到合适的人的。除此之外，爱情需要深深的理解和接受，我不能太过于“外貌协会”，而要真正的去了解对方的内心世界，接纳真实的对方。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;第四代时间管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个是我从《高效能人士的七个习惯》中得到的最有价值的一点。这个第四代的时间管理有几个最主要的方面：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;按照自己所处的角色不同，设定不同的任务，然而这些任务需要以第二类任务为主（不紧急，重要）&lt;/li&gt;
  &lt;li&gt;以一周为计划来安排这些任务&lt;/li&gt;
  &lt;li&gt;每天跟进进度进行调整&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;比如下图就是我尝试制定的一个一周的计划表
&lt;img src=&quot;/assets/weekly_schedule.png&quot; alt=&quot;一周计划表&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样做的益处是可以帮助自己专注于第二类任务，从而使自己得到真正的提高，更加符合自己的“个人使命宣言”。&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F-1/</link>
        <guid isPermaLink="true">http://localhost:4000//%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F-1/</guid>
      </item>
    
      <item>
        <title>What I Read This Week 1</title>
        <description>&lt;h3 id=&quot;the-3-tricks-that-made-alphago-zero-work&quot;&gt;&lt;a href=&quot;https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef&quot;&gt;The 3 Tricks That Made AlphaGo Zero Work&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using &lt;strong&gt;better training sample&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;Use a single neural network to predict which &lt;strong&gt;move&lt;/strong&gt; to recommend &lt;em&gt;and&lt;/em&gt; which &lt;strong&gt;move&lt;/strong&gt; are likely to win the game (This is the idea of using &lt;a href=&quot;http://ruder.io/multi-task/index.html#introduction&quot;&gt;&lt;strong&gt;Multitask Learning&lt;/strong&gt;&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Use a upgrade version of neural network (from convolutional neural network to &lt;strong&gt;residual neural network&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intuitive-rl-intro-to-advantage-actor-critic-a2c&quot;&gt;&lt;a href=&quot;https://medium.com/@rudygilman/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752&quot;&gt;Intuitive RL: Intro to Advantage-Actor-Critic (A2C)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A very vivid introduction of &lt;strong&gt;Advantage-Actor-Critic&lt;/strong&gt; reinforcement learning.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;First we should keep in mind that &lt;strong&gt;Actor-Critic&lt;/strong&gt; is a blend of both value estimation and policy estimation reinforcement learning method, a.k.a we will try to learn value function, as well as policy from game play (This is different from pure value function based method and policy based method).&lt;/li&gt;
  &lt;li&gt;In &lt;strong&gt;Actor-Critic&lt;/strong&gt;, the &lt;strong&gt;Actor&lt;/strong&gt; will tries to optimize the parameter for policy and &lt;strong&gt;Critic&lt;/strong&gt; will tries to optimize the parameter for the value function of a state. This can be done by having a single model outputting both the value of the state, as will as the probability of action.&lt;/li&gt;
  &lt;li&gt;By jump into one state, taking action and get reward. We will get the training examples for our &lt;strong&gt;Critic&lt;/strong&gt;. The estimate for each state will become more and more accurate. In this way, we don’t need to wait until the end of the game to get the value of each state, which is high in variance.&lt;/li&gt;
  &lt;li&gt;In stead of simply policy gradient update the policy (which tries to avoid the action that lead to a state with low value), we use &lt;strong&gt;Advantage&lt;/strong&gt;, which is the relative improvement of the action take (e.g. current state is -100, and take action A we arrive in a state with -20, the improvement of the action is 80!). The idea behind this is that the action might be the result that result in a low value.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ai-and-deep-learning-in-2017--a-year-in-review&quot;&gt;&lt;a href=&quot;http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/&quot;&gt;AI and Deep Learning in 2017 – A Year in Review&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A really awesome post that summarize what is going on in deep learning in 2017. Some points that I enjoy most:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evolution Algorithm (e.g. Genetic Algorithm) is coming back again.&lt;/li&gt;
  &lt;li&gt;Lots of deep learning framework is available right now: PyTorch is pretty popular in academic, but personally I thing TensorFlow is still the bests to try out (It’s also my plan to be more familiar with TensorFlow and work on some side project).&lt;/li&gt;
  &lt;li&gt;A good online reinforcement learning algorithm to read: &lt;a href=&quot;https://github.com/openai/baselines&quot;&gt;&lt;em&gt;OpenAI Baseline&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A good online courses: https://stats385.github.io/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;effective-modern-c&quot;&gt;Effective Modern C++&lt;/h3&gt;
&lt;p&gt;Mainly read the chapter about lambda function, some take away is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Avoid using default capture&lt;/li&gt;
  &lt;li&gt;In C++14, we can use init capture to move data we would like to use into the closure class, or use some expression to initialize the data member&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)](){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_unique&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()](){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Use decltype on auto&amp;amp;&amp;amp; parameters to forward them&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[](&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;decltype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Another thing to remember is to prefer using &lt;strong&gt;alias&lt;/strong&gt; declarations than &lt;strong&gt;typedefs&lt;/strong&gt;, because &lt;strong&gt;alias&lt;/strong&gt; supports templates better:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyAllocList&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyAlloc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sun, 14 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//What-I-Read-This-Week-I/</link>
        <guid isPermaLink="true">http://localhost:4000//What-I-Read-This-Week-I/</guid>
      </item>
    
      <item>
        <title>2018 新年计划</title>
        <description>&lt;p&gt;&lt;em&gt;2018年的主题，就是要变得自信以及学会Deep Work!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;工作&quot;&gt;工作&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;升职加薪
    &lt;ul&gt;
      &lt;li&gt;跳出自己的安逸区，接受更具有挑战的项目&lt;/li&gt;
      &lt;li&gt;加深对C++和Python的理解&lt;/li&gt;
      &lt;li&gt;加深机器学习领域的研究，多读post, blog, paper，每两周写一篇blog总结&lt;/li&gt;
      &lt;li&gt;提高自己的见识，要多和高人交流，学习别人的经验&lt;/li&gt;
      &lt;li&gt;(bonus)发表一遍学术论文，参加一次学术会议&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;学习&quot;&gt;学习&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;完成一个side project
    &lt;ul&gt;
      &lt;li&gt;starcraft AI?&lt;/li&gt;
      &lt;li&gt;photo style transfer?&lt;/li&gt;
      &lt;li&gt;chatbot?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;参加kaggle的比赛&lt;/li&gt;
  &lt;li&gt;一门新的语言: GO&lt;/li&gt;
  &lt;li&gt;一门新的计算机技术&lt;/li&gt;
  &lt;li&gt;每个月至少读一本书，每本书写一篇读后感&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;生活&quot;&gt;生活&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;胸围再大一圈，练出腹肌
    &lt;ul&gt;
      &lt;li&gt;每天坚持去gym，坚持每次练一组腹肌强化&lt;/li&gt;
      &lt;li&gt;每周晨跑三次&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;日语
    &lt;ul&gt;
      &lt;li&gt;每周安排计划固定学习的时间&lt;/li&gt;
      &lt;li&gt;参加N2考试&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;口琴&lt;/li&gt;
  &lt;li&gt;出去旅游两次&lt;/li&gt;
  &lt;li&gt;喝遍湾区的咖啡店，逛遍湾区的书店&lt;/li&gt;
  &lt;li&gt;(bonus) 每周去尝试一家新的饭店&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 07 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2018-%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/</link>
        <guid isPermaLink="true">http://localhost:4000//2018-%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/</guid>
      </item>
    
      <item>
        <title>Deep Work Reading Note</title>
        <description>&lt;p&gt;“Deep Work” is the first book I read this year. I was pretty impressed by the idea and methods the author purposed to help you gain the ability to do “deep work”, which means how to be concentrate on the work that can really generate value. In this note, I would like to summarize the main points of author, together with my personal experience.&lt;/p&gt;

&lt;p&gt;Deep work is the work which can really generate value, compared with shallow work, which is more like some routines that don’t need to think about too much to finish. Only by doing deep work, can we really improve ourselves, broaden our vision and shape our skills. As my personal experience, learning a new machine learning algorithm, or build a new APP counts to deep work, but repeatedly doing some interview algorithm exercise is shallow work.&lt;/p&gt;

&lt;p&gt;There are mainly 4 different strategies for deep work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Monastic: This is the most crucial strategies, which means that you are going to disconnected with all around and only focusing on the work you are doing all the time. This strategy does not apply to me, as I have work to do everyday and could not totally block others.&lt;/li&gt;
  &lt;li&gt;Bimodal: This is less crucial then the above one. For this one, you are not going to disconnected with the world all the time, but periodically. For example, for each year, you might choose two or three month to disconnect; or for each month, you choose one week to disconnect. This also does not apply for me, if I’m absent for too long, my manager would go crazy about me.&lt;/li&gt;
  &lt;li&gt;Rhythmic: This maybe the most common way to do deep work, which is to schedule some time each day dedicatedly. For example, every morning from 7:00 am to 9:00 am do deep work. This is the most suitable strategy for me, as well as for most of us. Although the “deepness” is not as powerful as the above two, but it still works pretty well. My plan is to change my schedule, and leave some time before go to work and go to bed, do some deep work.&lt;/li&gt;
  &lt;li&gt;Journalistic: This is the most flexible version, which is just to do deep work occasionally across the day, whenever you have some time. But this really need have strong deep work ability, so that you can switch to deep work easily. Because the switch will consume lots of power-will, which is limited resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides these strategies, there are also some tips to help do deep work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Come up with some rules when you do deep work, e.g. where to work (red rock cafe?), how to work (ban internet, no wechat) and how to support you deep work (relax music and good coffee)&lt;/li&gt;
  &lt;li&gt;Go to some really really really expensive places :)&lt;/li&gt;
  &lt;li&gt;Collaboration with others to help inspire your idea, but still work alone to do deep work&lt;/li&gt;
  &lt;li&gt;Execute like a business:
    &lt;ul&gt;
      &lt;li&gt;Have a small number of clear ambitious. I usually fail this one, as I was always trying to do lots of thing together to maximums my parallelism.&lt;/li&gt;
      &lt;li&gt;Record to number of deep hour you spend on deep work&lt;/li&gt;
      &lt;li&gt;Keep a compelling scoreboard to track your progress&lt;/li&gt;
      &lt;li&gt;Spend sometime to introspect how well you are doing deep work, what you success and what you fail to do&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Take good rest, e.g. stop working after 8:00 pm&lt;/li&gt;
  &lt;li&gt;Divide the working hour into some large block (deep work block and shallow work block). This would be extreme helpful for me, as I was easily distracted when I was writing code. What I plan to do is to have three long (more than 1hr) deep working blocks to focusing on coding, reading/writing important document/posts. Also schedule every minutes of the work time.&lt;/li&gt;
  &lt;li&gt;Do some meditate productively. What I plan is to spend some time walking around the campus
    &lt;ul&gt;
      &lt;li&gt;Be wary of distraction and looping. When you start to distract from your original problem, remind yourself and come back. Also you need to avoid the looping question you have already thought about&lt;/li&gt;
      &lt;li&gt;Structure deep thinking&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do some memory exercise to help you improve your concentration&lt;/li&gt;
  &lt;li&gt;Quit social media (no Facebook, wechat :) )&lt;/li&gt;
  &lt;li&gt;Know how to tell what is deep work and what is shallow work, and prioritize deep work.&lt;/li&gt;
  &lt;li&gt;Talk with your manager about the percentage of shallow work spend on each day.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 04 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//Deep-Work-Reading-Note/</link>
        <guid isPermaLink="true">http://localhost:4000//Deep-Work-Reading-Note/</guid>
      </item>
    
      <item>
        <title>2017 年终终结</title>
        <description>&lt;p&gt;2017年一转眼就过去了，自己也从一个刚刚毕业的学生成了一个快上了两年班的上班族了。趁着年末好好回顾一下这一整年发生的事情，一方面是给自己留个纪念，另一方面也是好好总结一下这一年的得失，找到改进的方向。&lt;/p&gt;

&lt;h3 id=&quot;工作&quot;&gt;工作&lt;/h3&gt;
&lt;p&gt;前半年工作比较顺利，但是后半年工作比较艰辛。首先最开心的事情就是年初得升职了，然后上半年的表现的评级也还不错，算是对自己能力的一个肯定。但是下半年由于种种原因，主要负责的项目一直进展非常不顺利，而且最后没能发布出去，很郁闷。辛辛苦苦干了大半年，每天基本从早上9点工作到晚上10点才回家，每个周末也去公司加班，但是还是事与愿违。而且由于一直专注于这个项目，导致疏忽了其他的方面工作，例如roadmap啊，mentor新人啊之类的，和上半年比起来差不少。&lt;/p&gt;

&lt;p&gt;反思了一下，感觉还是太焦躁心急，不够沉稳。心里总是想着早点升职多赚钱什么的，然而往往事与愿违。尤其是自己的计划被打乱之后，整个人就会变得非常的消沉，郁闷。一个很好的例子就是今年回国H1B签证被check，导致在国内待了两个月。当时的心情是非常的郁闷，还经常自己吓唬自己万一拿不到签证回不去美国咋办，好好的工作没了，自己怎么就这么倒霉。现在回头看感觉自己当时真是好笑，本来两个月的时间完全可以出去逛逛玩玩当个长假来休息，但是由于自己太重视这份工作，使自己一直纠结于被check的这一无法改变的事实，在家愁了两个月。不过经历过这一出之后，自己的心态也有所改善，脾气基本上是给磨没了。现在遇到点事情能比当时更沉得住气一些了。然而离自己理想的境界还有很长的距离， 希望在今后的日子里能在“不以物喜，不以己悲”的修炼上更进一步。&lt;/p&gt;

&lt;p&gt;另一个就是要想办法挑战自己。最近在工作上一直处于自己的舒适区，只是在做一些自己已经很熟练的工作而不去做一些自己没有尝试过的更具有挑战性的事情。很大的程度上这是因为担心脱离舒适区的后果，比如想换组，但是万一换到别的组表现不如现在的好影响了自己的评级了咋办？风险自然是有的，磨难肯定也是不少，但是只有经历过这些，才能成长，才能有进一步的提高，才能有更广的可能性。套用钢之炼金术师里面的台词，“在经历过这些磨难之后，就能获得钢铁般的心”。最近刚刚看过了吴军老师对于职业发展的一个观点：“稀缺性”。现在自己的工作，换一个人来干一样能干，都是一些简单的写写代码，调调参数什么的。这说明自己的稀缺性还不够，想要增加自己的稀缺性，那么就要敢于去尝试一些别人不能做的事情，这种事情往往风险比较大，存在于舒适区之外。另一方面，就是要增加自己的见识，对自己的领域要有一些比较深入的看法和见解，因为这个东西，别人是不可能轻易学去的，当然培养这方面也很困难，需要大量的经验积累，而这种经验，如果一直处于舒适区，是很难获得的。&lt;/p&gt;

&lt;h3 id=&quot;学习&quot;&gt;学习&lt;/h3&gt;
&lt;p&gt;年初定下来的好多计划最终都没有坚持下来：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;想要学口琴，但是一点都没有练过&lt;/li&gt;
  &lt;li&gt;想要学习日语，但是没能坚持下来&lt;/li&gt;
  &lt;li&gt;想要学习GO语言，但是根本就没有开始&lt;/li&gt;
  &lt;li&gt;想要看一些开源的代码，结果没看过&lt;/li&gt;
  &lt;li&gt;想要做一些side project，但是一个也没做出来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是还是有一些坚持下来了：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;每个月读一本书&lt;/li&gt;
  &lt;li&gt;学习了TensorFlow并且写了些简单的算法&lt;/li&gt;
  &lt;li&gt;自学了Reinforcement Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;感觉自己在增长新技能这方面有点不够专注，什么都想沾一手，但是最终的结果是什么都没有学到，典型的“想得太多，做得太少”。而且我还另一个特点，开始干一件事情的阈值很高，但是只要开始了，我就会坚持下去，尽量做到完美，这也表现出我不是一个半途而废的人，那么关键就是怎么开始了。所以在2018年，打算缩减自己的计划，把精力集中在两三个事情上，强迫自己开始并坚持下去。&lt;/p&gt;

&lt;p&gt;另一个就是自己不太擅长记录，比如读过的书，看过就扔了，没有提炼出一些个性化的东西；学过的一些技术，也是没有好好的总结，过一阵子也就忘记了。之前尝试过写读书笔记以及技术博客，但是都没有坚持下来。在新的一年里要坚持更新自己的博客。&lt;/p&gt;

&lt;h3 id=&quot;感情&quot;&gt;感情&lt;/h3&gt;
&lt;p&gt;感情这一年就是很扎心了。先是跟前女友分手了，之后被一个妹子表白第二天就又被发卡，现在联系一个新认识的妹子结果一直不怎么受待见。这很可能跟我负能量比较多，不够自信有关。我这个人看待事物的时候有点消极，遇到什么事情总是喜欢把消极面放大而不会从积极的焦虑去考虑。然后自信也比较低，总感觉自己比不上别人。感觉这是自己性格方面的两个很重要的缺陷。为此我还买了本书叫《正能量》，想看看从心理学的角度怎么能改正一下。总而言之一句话就是表现的乐观就会感觉乐观，表现的自信就会感觉自信，即行为影响情绪。所以平时说话多用积极的字眼，经常保持微笑，不整天愁眉苦脸的，来帮助自己树立正能量和自信。除此之外，也更加深入的明白了一个道理，找女朋友这件事绝对不能将就，要找就找自己喜欢的，想一辈子过日子的。不能抱着说，“先谈着，慢慢培养感情”，这种思想。培养感情是要建立在一定的冲动之上，如果连一点喜欢的感觉都没有，强迫自己培养感情喜欢上对方，对方痛苦，你自己也痛苦，还浪费时间跟精力，实在不是明智之举。&lt;/p&gt;

&lt;p&gt;当然啦，平时跟基友也总吐槽自己单身，埋怨自己怎么找不到女朋友啥的，但是其实单身的生活还是比较享受的。每天想在公司加班就加班，想回家打游戏就打游戏，周末想去吃个日料就去吃个日料，想到咖啡店看书就去咖啡店看书。时间充分的自由支配也挺好的。&lt;/p&gt;

&lt;h3 id=&quot;生活&quot;&gt;生活&lt;/h3&gt;
&lt;p&gt;生活方面感觉是这一年取得的成就最大的，那就是健身整整坚持了一整年，而且小有成效，再也不用为买什么衣服而发愁了哈哈。从上半年的每天坚持跑步，到下半年每天和同事去健身房做力量训练，虽然过程很痛苦，但是心里充满了成就感。年初的时候参加了公司组织的10km的长跑比赛，跑进了1小时；卧推现在能举起200lb以上；体重曾经一度控制在70kg，这些都是曾经我不敢想象的，然而通过自己的努力都做到了，非常的欣慰。&lt;/p&gt;

&lt;p&gt;另一个感觉很欣慰的事情就是成了一个狂热的咖啡和日料的爱好者。每个周末都会抽个半天去各种各样的咖啡店品尝咖啡看看书，体验一下不同的咖啡店的环境和氛围；晚上也会去各种日料店吃日料，顺便发个美食攻略贴。这些算是给自己单调的生活填上了一些乐趣。&lt;/p&gt;

&lt;p&gt;俗话说得好，“读万卷书，行万里路”。新的一年里想多出去走走，领略一下不同城市的风景。&lt;/p&gt;

&lt;h3 id=&quot;结语&quot;&gt;结语&lt;/h3&gt;
&lt;p&gt;不管2017年怎么样，都已经成为过去，不必太纠结于这一整年的失落和痛苦，让那些成为以后酒桌上的笑谈。昂首挺胸，迎接崭新的一年！&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Dec 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</link>
        <guid isPermaLink="true">http://localhost:4000//2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 8</title>
        <description>&lt;p&gt;This is the last lesson for the entire reinforcement learning, and in this lesson we will learn something related to exploit and explore. In machine learning service, like recommendation service, there is always a trade off between exploit and explore. Exploit means we are always choosing the best given the current information we have, while explore means try something new we haven’t tried yet. An example is if you go to restaurant, you can always go to the one you enjoy most(exploit), while you can also try a new one(explore).&lt;/p&gt;

&lt;p&gt;This problem is usually formularized as multi bandit problem, which can be represented as \(&amp;lt;A, R&amp;gt; \). Here \( A \) is a set of action we can take, and \( R^a(r) = P[R=r, A=a] \) is an &lt;strong&gt;unknown&lt;/strong&gt; probability distribution over rewards. At each time, our agent is going to pick an action, and the environment will generate a reward. The goal is to maximize the cumulative reward.&lt;/p&gt;

&lt;h4 id=&quot;regret&quot;&gt;Regret&lt;/h4&gt;
&lt;p&gt;We can measure the goodness of our action use &lt;strong&gt;regret&lt;/strong&gt;. Suppose the action value is the mean reward for an action \( a \)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(a) = E[r|a]&lt;/script&gt;

&lt;p&gt;and the optimal value \( V^\star \) is the max mean reward we can get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^\star = Q(a^\star) = max_{a\in A}Q(a)&lt;/script&gt;

&lt;p&gt;Then maximize the cumulative reward is equivalent to minimize the total regret, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_t = E[\sum_{i=1}^t (V^\star - Q(a_i))]&lt;/script&gt;

&lt;h4 id=&quot;upper-confidence-bound&quot;&gt;Upper Confidence Bound&lt;/h4&gt;
&lt;p&gt;We can try to solve this problem in the face of uncertainty. The best action we should try is the one that would on one hand has a high mean reward, and on the other hand have a high uncertainty. We might get a higher reward, which is good. While we can also get a worse reward, but that does not matter, since we can reduce our uncertainty about that action, and prefer other action which might have higher reward. A more formal description is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Estimate an upper confidence \( \hat{U_t}(a) \) for each action value, which depends on the number of times \( a \) has been selected, the larger the times, the smaller the upper confidence&lt;/li&gt;
  &lt;li&gt;Such that \( Q(a) \le \hat{Q_t}(a) + \hat{U_t}(a) \) with high probability&lt;/li&gt;
  &lt;li&gt;Select action maximize Upper Confidence Bound (UCB)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} \hat{Q_t}(a) + \hat{U_t}(a)&lt;/script&gt;

&lt;p&gt;We need to come up with some method to calculate the upper bound. Here, we bring &lt;em&gt;Hoeffding’s Inequality&lt;/em&gt; for help&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Let \( X_1,…, X_t \) be i.i.d. random variables in \( [0, 1] \), and let \( \bar{X_t} = \frac{1}{i} \sum_{i=1}^t X_i \) be the sample mean. Then&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[E[X] &gt; \bar{X}_t + u] \le e^{-2tu^2}&lt;/script&gt;

&lt;p&gt;With this we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P[Q(a) &gt; \hat{Q_t}(a) + \hat{U_t}] \le e^{-2N_t(a)U_t(a)^2}&lt;/script&gt;

&lt;p&gt;where \( N_t(a) \) is the expected number of \( a \) is selected. We then can pick a probability \( p \) that true value exceeds UCB, and reduce $p$ as we observer more rewards, e.g. \( p = t^{-4} \). Then we could obtain the upper bound as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_t(a) = \sqrt{\frac{2logt}{N_t(a)}}&lt;/script&gt;

&lt;p&gt;And finally we have the UCB1 algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = argmax_{a\in A} (Q(a) + \sqrt{\frac{2logt}{N_t(a)}})&lt;/script&gt;
</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lesson-8/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lesson-8/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 7</title>
        <description>&lt;p&gt;In the pervious notes, we are all using &lt;strong&gt;model-free&lt;/strong&gt; reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1&quot;&gt;Lesson 1&lt;/a&gt; we know that a MDP can be represent by $&amp;lt;S, A, P, R&amp;gt;$, and our model is going to understand and simulate this. We will only introduce the simple version here, in which we assume that the $S$ and $A$ is known, and thus we only need to model $P$ and $R$. We can formulate it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{t+1} ~ P_\eta(S_{t+1}|S_t, A_t) \\
R_{t+1} = R_\eta(R_{t+1}|S_t, A_t)&lt;/script&gt;

&lt;p&gt;where the prediction of next state is a density estimation problem and the reward is a regression problem.&lt;/p&gt;

&lt;h4 id=&quot;integrated-architecture&quot;&gt;Integrated Architecture&lt;/h4&gt;
&lt;p&gt;In this architecture, we are going to consider two types of experience. &lt;strong&gt;Real experience&lt;/strong&gt; which is sampled from the environment, and &lt;strong&gt;Simulated experience&lt;/strong&gt; which is sampled from our model. In the past, we only use the real experience to learn value function/policy. Now, we are going to learn our model from real experience, then plan and learn value function/policy from both real and simulated experience. This is thus called integrated architecture (integration of real and fake), the &lt;strong&gt;Dyna Architecture&lt;/strong&gt;. Here is an picture to illustrate what the logic flow of Dyna is like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dyna.png&quot; alt=&quot;Dyna Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to the Dyna architecture, we can design many algorithm, here is an example of &lt;strong&gt;Dyna-Q Algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $Q(s, a)$ and $Model(s, a)$ for all $s$ and $a$&lt;/li&gt;
  &lt;li&gt;Do forever:
    &lt;ul&gt;
      &lt;li&gt;$S =$ current (nonterminal) state&lt;/li&gt;
      &lt;li&gt;$A = \epsilon - \text{greedy}(S, Q)$&lt;/li&gt;
      &lt;li&gt;Execute action $A$; observe result reward $R$, and state $S’$&lt;/li&gt;
      &lt;li&gt;$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$ (This is using real experience)&lt;/li&gt;
      &lt;li&gt;Update $Model(S, A)$ using $R, S’$&lt;/li&gt;
      &lt;li&gt;Repeat $n$ times: (This is using simulated experience to learn value function)
        &lt;ul&gt;
          &lt;li&gt;$S =$ random previously observed state&lt;/li&gt;
          &lt;li&gt;$A =$ random action previously taken in $S$&lt;/li&gt;
          &lt;li&gt;Sample $R, S’$ from $Model(S, A)$&lt;/li&gt;
          &lt;li&gt;$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;monte-carlo-tree-search&quot;&gt;Monte-Carlo Tree Search&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Monte-Carlo Tree Search&lt;/strong&gt; is a very efficient algorithm to plan once we have a model.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model $M_v$&lt;/li&gt;
  &lt;li&gt;Simulate $K$ episodes from current state $s_t$ using current simulation policy $\pi$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{s_t, A_t^k, R_{t+1}^k, S_{t+1}^k, ..., S_T^k} ~ M_v, \pi&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Build a search tree containing visited states and actions&lt;/li&gt;
  &lt;li&gt;Evaluate state $Q(s, a)$ by mean return of episodes from $s, a$&lt;/li&gt;
  &lt;li&gt;After search is finished, select current (real) action with maximum value in search tree&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In MCMT, the simulation policy $\pi$ improves. Each simulation consists of two phases (in-tree, out-of-tree):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree policy (improves): pick action to maximize $Q(S, A)$&lt;/li&gt;
  &lt;li&gt;Default policy (fixed): pick action randomly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Repeat (each simulation):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evaluate states $Q(S, A)$ by Mento-Carlo evaluation&lt;/li&gt;
  &lt;li&gt;Improve tree policy, e.g. by $\epsilon-\text{greedy}(Q)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are several advantages of MCMT:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Highly selective best-first search&lt;/li&gt;
  &lt;li&gt;Evaluates states dynamically&lt;/li&gt;
  &lt;li&gt;Uses sampling to break curse of dimensionality&lt;/li&gt;
  &lt;li&gt;Works for “black-box” models (only requires samples)&lt;/li&gt;
  &lt;li&gt;Computationally efficient, anytime&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 11 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcement-Learning-Lesson-7/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcement-Learning-Lesson-7/</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Lesson 6</title>
        <description>&lt;p&gt;In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_{\theta}(s ,a) = P[a|s, \theta]&lt;/script&gt;

&lt;p&gt;In this case, we are not going to have any value function. A slight variance of this method is called &lt;strong&gt;Actor-Critic&lt;/strong&gt;, in which both value function and policy are modeled and learnt.&lt;/p&gt;

&lt;p&gt;The advantage of &lt;strong&gt;Policy based RL&lt;/strong&gt; is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Better convergence properties&lt;/li&gt;
  &lt;li&gt;Effective in high-dimensional or continuous action spaces&lt;/li&gt;
  &lt;li&gt;Can learn stochastic policies&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h4&gt;
&lt;p&gt;Since we are going to learn $\pi_\theta (s, a)$ and find the best $\theta$, we need to first find a way to measure the quality of our policy. These are called &lt;strong&gt;policy objective function&lt;/strong&gt; and some we can use are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In episode environment we can use the start value&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_1(\theta) = V^{\pi_\theta}(s_1) = E_{\pi_\theta}[v_1]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;In continuous environment we can use average value or average reward pre time-step&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_{avV}(\theta) = \sum_{s}d^{\pi_\theta}(s)V^{\pi_\theta}(s) \\
J_{avR}(\theta) = \sum_{s}d^{\pi_\theta}(s)\sum_{a}\pi_\theta(s, a)R_s^a&lt;/script&gt;

&lt;p&gt;where $d^{\pi_\theta}(s)$ is stationary distribution of Markov chain for $\pi_\theta$.&lt;/p&gt;

&lt;p&gt;After we have the measurement of the policy quality, we are going to find the best parameter which gives us the best quality and this becomes an optimization problem. Actually, similar to the last post, we can also use stochastic gradient to help use here. Since we are trying to find the maximum value, we are going to use what is called gradient ascent to find the steepest direction to update our parameter (very similar to gradient decrease).&lt;/p&gt;

&lt;h4 id=&quot;score-function&quot;&gt;Score Function&lt;/h4&gt;
&lt;p&gt;In order to compute the policy gradient analytically, we introduced the &lt;strong&gt;score function&lt;/strong&gt;. Assume policy $\pi_{\theta}$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta (s, a)$. Then using some tricky we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_\theta \pi_\theta (s, a) &amp; = \pi_\theta (s, a) \frac{\nabla_\theta \pi_\theta (s, a) }{\pi_\theta (s, a)} \\
&amp; = \pi_\theta (s, a) \nabla_\theta log \pi_\theta (s, a)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here, $\nabla_\theta log \pi_\theta (s, a)$ is the &lt;strong&gt;score function&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;For any differentiable policy $\pi_\theta (s, a)$, for any of the policy objective functions $J_1, J_{avV}, J_{avR}$, the policy gradient is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log \pi_\theta (s, a) Q^{\pi_\theta} (s, a)]&lt;/script&gt;

&lt;h4 id=&quot;monte-carlo-policy-gradient&quot;&gt;Monte-Carlo Policy Gradient&lt;/h4&gt;
&lt;p&gt;Use return as an unbiased sample of $Q^{\pi_\theta} (s, a)$, the algorithm is as follow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $\theta$ arbitrarily
    &lt;ul&gt;
      &lt;li&gt;for each episode ${s_1, a_1, r_2, …, s_{T-1}, a_{T-1}, r_T} ~ \pi_\theta$ do
        &lt;ul&gt;
          &lt;li&gt;for $t = 1$ to $T - 1$ do
            &lt;ul&gt;
              &lt;li&gt;$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) v_t$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;end for&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;end for&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $\theta$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;actor-critic-policy-gradient&quot;&gt;Actor Critic Policy Gradient&lt;/h4&gt;
&lt;p&gt;The problem with Monte-Carlo Policy Gradient is that is has a very high variance. In order to reduce the variance, we can use a &lt;strong&gt;critic&lt;/strong&gt; to estimate the action value function. Thus in &lt;strong&gt;Actor Critic Policy Gradient&lt;/strong&gt;, we have two components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Critic&lt;/em&gt; updates action value function parameters $w$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Actor&lt;/em&gt; updates policy parameters $\theta$, in direction suggested by critic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example when we use linear value function approximation for the critic:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $s$, $\theta$&lt;/li&gt;
  &lt;li&gt;Sample $a ~ \pi_\theta$&lt;/li&gt;
  &lt;li&gt;for each step do
    &lt;ul&gt;
      &lt;li&gt;Sample reward $r$, sample next state $s’$&lt;/li&gt;
      &lt;li&gt;Sample action $a’ ~ \pi_\theta (s’, a’)$&lt;/li&gt;
      &lt;li&gt;$\delta = r + \gamma Q_w(s’, a’) - Q_w(s, a)$ (This is the TD error)&lt;/li&gt;
      &lt;li&gt;$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) Q_w(s, a)$ (We replace with the approximation)&lt;/li&gt;
      &lt;li&gt;$w = w + \beta \delta \phi(s, a)$ (Update value function approximation model parameter)&lt;/li&gt;
      &lt;li&gt;$a = a’$, $s = s’$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 10 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//Reinforcment-Learning-Lesson-6/</link>
        <guid isPermaLink="true">http://localhost:4000//Reinforcment-Learning-Lesson-6/</guid>
      </item>
    
  </channel>
</rss>
