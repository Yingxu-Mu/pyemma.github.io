<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Reinforcement Learning Lesson 7 - Coding Monkey</title>
<meta name="description" content="In the pervious notes, we are all using model-free reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey">
<meta property="og:title" content="Reinforcement Learning Lesson 7">
<meta property="og:url" content="http://localhost:4000/Reinforcement-Learning-Lesson-7/">


  <meta property="og:description" content="In the pervious notes, we are all using model-free reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.">







  <meta property="article:published_time" content="2017-09-11T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/Reinforcement-Learning-Lesson-7/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Pei",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/">Coding Monkey</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/" >Quick-Start Guide</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yang Pei</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a Coding Monkey
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Mountain View</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Reinforcement Learning Lesson 7">
    <meta itemprop="description" content="In the pervious notes, we are all using model-free reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.">
    <meta itemprop="datePublished" content="September 11, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Reinforcement Learning Lesson 7
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>In the pervious notes, we are all using <strong>model-free</strong> reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.</p>

<p>From <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1">Lesson 1</a> we know that a MDP can be represent by $&lt;S, A, P, R&gt;$, and our model is going to understand and simulate this. We will only introduce the simple version here, in which we assume that the $S$ and $A$ is known, and thus we only need to model $P$ and $R$. We can formulate it as:</p>

<script type="math/tex; mode=display">S_{t+1} ~ P_\eta(S_{t+1}|S_t, A_t) \\
R_{t+1} = R_\eta(R_{t+1}|S_t, A_t)</script>

<p>where the prediction of next state is a density estimation problem and the reward is a regression problem.</p>

<h4 id="integrated-architecture">Integrated Architecture</h4>
<p>In this architecture, we are going to consider two types of experience. <strong>Real experience</strong> which is sampled from the environment, and <strong>Simulated experience</strong> which is sampled from our model. In the past, we only use the real experience to learn value function/policy. Now, we are going to learn our model from real experience, then plan and learn value function/policy from both real and simulated experience. This is thus called integrated architecture (integration of real and fake), the <strong>Dyna Architecture</strong>. Here is an picture to illustrate what the logic flow of Dyna is like.</p>

<p><img src="/assets/dyna.png" alt="Dyna Architecture" /></p>

<p>According to the Dyna architecture, we can design many algorithm, here is an example of <strong>Dyna-Q Algorithm</strong>:</p>
<ul>
  <li>Initialize $Q(s, a)$ and $Model(s, a)$ for all $s$ and $a$</li>
  <li>Do forever:
    <ul>
      <li>$S =$ current (nonterminal) state</li>
      <li>$A = \epsilon - \text{greedy}(S, Q)$</li>
      <li>Execute action $A$; observe result reward $R$, and state $S’$</li>
      <li>$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$ (This is using real experience)</li>
      <li>Update $Model(S, A)$ using $R, S’$</li>
      <li>Repeat $n$ times: (This is using simulated experience to learn value function)
        <ul>
          <li>$S =$ random previously observed state</li>
          <li>$A =$ random action previously taken in $S$</li>
          <li>Sample $R, S’$ from $Model(S, A)$</li>
          <li>$Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)]$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="monte-carlo-tree-search">Monte-Carlo Tree Search</h4>
<p><strong>Monte-Carlo Tree Search</strong> is a very efficient algorithm to plan once we have a model.</p>
<ul>
  <li>Given a model $M_v$</li>
  <li>Simulate $K$ episodes from current state $s_t$ using current simulation policy $\pi$</li>
</ul>

<script type="math/tex; mode=display">{s_t, A_t^k, R_{t+1}^k, S_{t+1}^k, ..., S_T^k} ~ M_v, \pi</script>

<ul>
  <li>Build a search tree containing visited states and actions</li>
  <li>Evaluate state $Q(s, a)$ by mean return of episodes from $s, a$</li>
  <li>After search is finished, select current (real) action with maximum value in search tree</li>
</ul>

<p>In MCMT, the simulation policy $\pi$ improves. Each simulation consists of two phases (in-tree, out-of-tree):</p>
<ul>
  <li>Tree policy (improves): pick action to maximize $Q(S, A)$</li>
  <li>Default policy (fixed): pick action randomly</li>
</ul>

<p>Repeat (each simulation):</p>
<ul>
  <li>Evaluate states $Q(S, A)$ by Mento-Carlo evaluation</li>
  <li>Improve tree policy, e.g. by $\epsilon-\text{greedy}(Q)$</li>
</ul>

<p>There are several advantages of MCMT:</p>
<ul>
  <li>Highly selective best-first search</li>
  <li>Evaluates states dynamically</li>
  <li>Uses sampling to break curse of dimensionality</li>
  <li>Works for “black-box” models (only requires samples)</li>
  <li>Computationally efficient, anytime</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-09-11T00:00:00-07:00">September 11, 2017</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Reinforcement+Learning+Lesson+7%20http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-7%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-7%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-7%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-7%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="http://localhost:4000/Reinforcment-Learning-Lesson-6/" class="pagination--pager" title="Reinforcement Learning Lesson 6
">Previous</a>
    
    
      <a href="http://localhost:4000/Reinforcement-Learning-Lesson-8/" class="pagination--pager" title="Reinforcement Learning Lesson 8
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/assets/violet.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/What-I-Read-This-Week-4/" rel="permalink">What I Read This Week 4
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A visual proof that neural nets can compute any function
A really good writing on helping understand why neural network can approximate any functions, withou...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/assets/violet.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/What-I-Read-This-Week-3/" rel="permalink">What I Read This Week 3
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Explicit vs. Implicit Recommenders

  Never use RMSE (or other metrics only take explicit feedback (e.g. rating)) as measurement for your recommendation syst...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/assets/violet.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/DQN-In-Practice/" rel="permalink">DQN In Practice
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Recently I have been working on Deep-Q-Learning and apply it to some interesting AI games. In this post, I would like to give a brief introduction to how I i...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/assets/violet.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/What-I-Read-This-Week-2/" rel="permalink">What I Read This Week 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Evolving search recommendations on Pinterest
A post introducing the search work done in Pinterest.

  Initially they use a Term-Query graph to generate candi...</p>
  </article>
</div>
        
      </div>
    </div>
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Yang Pei. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="http://localhost:4000/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>








  </body>
</html>
