<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Reinforcement Learning Lesson 7 - Coding Monkey’s Blog</title>
<meta name="description" content="In the pervious notes, we are all using model-free reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey's Blog">
<meta property="og:title" content="Reinforcement Learning Lesson 7">
<meta property="og:url" content="https://pyemma.github.io/Reinforcement-Learning-Lesson-7/">


  <meta property="og:description" content="In the pervious notes, we are all using model-free reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.">







  <meta property="article:published_time" content="2017-09-11T00:00:00-07:00">





  

  


<link rel="canonical" href="https://pyemma.github.io/Reinforcement-Learning-Lesson-7/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Bayarea Coding Monkey",
      "url" : "https://pyemma.github.io",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->

<script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script
      type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<link href="https://pyemma.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey's Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://pyemma.github.io/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://pyemma.github.io/">Coding Monkey's Blog</a>
        <ul class="visible-links">
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Bayarea Coding Monkey</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a coding monkey, and I am proud of it. I have done lots of work in machine learning area, especially recommendation system and AutoML. This blog summarize my journey to become an expert monkey in distributed system and LLM.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Sunnyvale</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Reinforcement Learning Lesson 7">
    <meta itemprop="description" content="In the pervious notes, we are all using model-free reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.">
    <meta itemprop="datePublished" content="September 11, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Reinforcement Learning Lesson 7
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>In the pervious notes, we are all using <strong>model-free</strong> reinforcement learning method to find the solution for the problem. Today we are going to introduce method that directly learns from the experience and tries to understand the underlaying world.</p>

<p>From <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1">Lesson 1</a> we know that a MDP can be represent by \( &lt;S, A, P, R&gt; \), and our model is going to understand and simulate this. We will only introduce the simple version here, in which we assume that the \( S \) and \( A \) is known, and thus we only need to model \( P \) and \( R \). We can formulate it as:</p>

\[S_{t+1} ~ P_\eta(S_{t+1}|S_t, A_t) \\
R_{t+1} = R_\eta(R_{t+1}|S_t, A_t)\]

<p>where the prediction of next state is a density estimation problem and the reward is a regression problem.</p>

<h2 id="integrated-architecture">Integrated Architecture</h2>
<p>In this architecture, we are going to consider two types of experience. <strong>Real experience</strong> which is sampled from the environment, and <strong>Simulated experience</strong> which is sampled from our model. In the past, we only use the real experience to learn value function/policy. Now, we are going to learn our model from real experience, then plan and learn value function/policy from both real and simulated experience. This is thus called integrated architecture (integration of real and fake), the <strong>Dyna Architecture</strong>. Here is an picture to illustrate what the logic flow of Dyna is like.</p>

<p><img src="/assets/dyna.png" alt="Dyna Architecture" /></p>

<p>According to the Dyna architecture, we can design many algorithm, here is an example of <strong>Dyna-Q Algorithm</strong>:</p>
<ul>
  <li>Initialize \( Q(s, a) \) and \( Model(s, a) \) for all \( s \) and \( a \)</li>
  <li>Do forever:
    <ul>
      <li>\( S = \) current (nonterminal) state</li>
      <li>\( A = \epsilon - \text{greedy}(S, Q) \)</li>
      <li>Execute action \( A \); observe result reward \( R \), and state \( S’ \)</li>
      <li>\( Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)] \) (This is using real experience)</li>
      <li>Update \( Model(S, A) \) using \( R, S’ \)</li>
      <li>Repeat \( n \) times: (This is using simulated experience to learn value function)
        <ul>
          <li>\( S = \) random previously observed state</li>
          <li>\( A = \) random action previously taken in \( S \)</li>
          <li>Sample \( R, S’ \) from \( Model(S, A) \)</li>
          <li>\( Q(S, A) = Q(S, A) + \alpha[R + \gamma max_a Q(S’, a) - Q(S, A)] \)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="monte-carlo-tree-search">Monte-Carlo Tree Search</h2>
<p><strong>Monte-Carlo Tree Search</strong> is a very efficient algorithm to plan once we have a model.</p>
<ul>
  <li>Given a model \( M_v \)</li>
  <li>Simulate $K$ episodes from current state $s_t$ using current simulation policy \( \pi \)</li>
</ul>

\[{s_t, A_t^k, R_{t+1}^k, S_{t+1}^k, ..., S_T^k} ~ M_v, \pi\]

<ul>
  <li>Build a search tree containing visited states and actions</li>
  <li>Evaluate state \( Q(s, a) \) by mean return of episodes from \( s, a \)</li>
  <li>After search is finished, select current (real) action with maximum value in search tree</li>
</ul>

<p>In MCMT, the simulation policy \( \pi \) improves. Each simulation consists of two phases (in-tree, out-of-tree):</p>
<ul>
  <li>Tree policy (improves): pick action to maximize \( Q(S, A) \)</li>
  <li>Default policy (fixed): pick action randomly</li>
</ul>

<p>Repeat (each simulation):</p>
<ul>
  <li>Evaluate states \( Q(S, A) \) by Mento-Carlo evaluation</li>
  <li>Improve tree policy, e.g. by \( \epsilon-\text{greedy}(Q) \)s</li>
</ul>

<p>There are several advantages of MCMT:</p>
<ul>
  <li>Highly selective best-first search</li>
  <li>Evaluates states dynamically</li>
  <li>Uses sampling to break curse of dimensionality</li>
  <li>Works for “black-box” models (only requires samples)</li>
  <li>Computationally efficient, anytime</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-09-11T00:00:00-07:00">September 11, 2017</time></p>
        
      </footer>

      
  <nav class="pagination">
    
      <a href="https://pyemma.github.io/Reinforcment-Learning-Lesson-6/" class="pagination--pager" title="Reinforcement Learning Lesson 6
">Previous</a>
    
    
      <a href="https://pyemma.github.io/Reinforcement-Learning-Lesson-8/" class="pagination--pager" title="Reinforcement Learning Lesson 8
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="https://pyemma.github.io/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Bayarea Coding Monkey. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="https://pyemma.github.io/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "https://pyemma.github.io/Reinforcement-Learning-Lesson-7/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/Reinforcement-Learning-Lesson-7"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://pyemma.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>
