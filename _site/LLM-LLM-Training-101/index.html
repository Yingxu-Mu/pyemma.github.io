<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>LLM Training 101 - Coding Monkey’s Blog</title>
<meta name="description" content="这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey's Blog">
<meta property="og:title" content="LLM Training 101">
<meta property="og:url" content="https://pyemma.github.io/LLM-LLM-Training-101/">


  <meta property="og:description" content="这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备">







  <meta property="article:published_time" content="2024-09-01T00:00:00-07:00">





  

  


<link rel="canonical" href="https://pyemma.github.io/LLM-LLM-Training-101/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Bayarea Coding Monkey",
      "url" : "https://pyemma.github.io",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->

<script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script
      type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<link href="https://pyemma.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey's Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://pyemma.github.io/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://pyemma.github.io/">Coding Monkey's Blog</a>
        <ul class="visible-links">
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Bayarea Coding Monkey</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a coding monkey, and I am proud of it. I have done lots of work in machine learning area, especially recommendation system and AutoML. This blog summarize my journey to become an expert monkey in distributed system and LLM.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Sunnyvale</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="LLM Training 101">
    <meta itemprop="description" content="这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备">
    <meta itemprop="datePublished" content="September 01, 2024">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">LLM Training 101
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#概念性知识">概念性知识</a></li>
  <li><a href="#打算去学习了解的框架和技术">打算去学习了解的框架和技术</a></li>
  <li><a href="#一些比较主流的和重要的概念">一些比较主流的和重要的概念</a>
    <ul>
      <li><a href="#parallelism-strategy">Parallelism Strategy</a></li>
      <li><a href="#memory-optimization">Memory Optimization</a></li>
      <li><a href="#communication-optimization">Communication Optimization</a></li>
      <li><a href="#fault-tolerance">Fault Tolerance</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <p>这个是读完这篇综述 <a href="https://arxiv.org/pdf/2407.20018">Efficient Training of Large Language Models on Distributed Infrastructures - A Survey</a> 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备</p>

<blockquote>
  <p>PS: 后续会不定期的更新这篇 blog 来争取与时俱进，同时会有专栏来介绍这篇 blog 里面打算深入研究的项目</p>
</blockquote>

<h2 id="概念性知识">概念性知识</h2>

<ul>
  <li>LLM 训练的一些特点
    <ul>
      <li>模型架构的一致性，基本都是堆的 transformer, 虽然现在有一些不一样的尝试比如 Mamba 和 TTT, 但是主流的模型还是 transformer</li>
      <li>训练的规模和时间也是空前绝后的</li>
      <li>Specialized software, 比如 Megatron (这个听说过，去了解一下)</li>
      <li>LLM 训练的 pipeline 也发生了变化（这一点说的还蛮有道理，我在这个领域有比较多的经验，可以向这个 LLM 的方向研究一下看看有什么机会）。传统的机器学习都是针对某一个问题用对应的数据来训练（domain specific），但是现在 LLM 的主流是在大量的数据做自监督学习，然后再进行 fine-tuning, alignment 等</li>
      <li>在 LLM 训练的各项因素之中，Communication overhead 是一个主要痛点</li>
    </ul>
  </li>
  <li>LLM 训练的 infrastructure 相关的内容
    <ul>
      <li>PCIe 由于 bandwidth 的问题导致其不是很合适 LLM 的训练，现在更多的是使用专用的链接比如 NVLink 等，同时能使用不同的网络连接拓扑结构来进行进一步的优化，比如 cube-mesh 或者 switch-based fullly-connected</li>
      <li>The Clos network architecture, commonly known as a Fat-Tree topology, is widely used in LLM training clusters. In a Closbased cluster, each server, equipped with one or more NICs, is organized into racks connected to leaf switches. These leaf switches link to spine switches, providing inter-rack connectivity and forming a pod. The pods are further interconnected with core switches, facilitating any-to-any communication across servers within the cluster.</li>
      <li>Parallel file systems such as Lustre, GPFS, and BeeGFS are frequently deployed on leading high performance computing systems to ensure efficient I/O, persistent storage, and scalable performance. 听说过 distributed file system, 但是这个 parallel file system 是啥</li>
    </ul>
  </li>
</ul>

<h2 id="打算去学习了解的框架和技术">打算去学习了解的框架和技术</h2>

<ul>
  <li><strong>RDMA</strong>: 可以去学习了解一下 InfiniBand</li>
  <li><strong>DeepSpeed-Chat</strong>, parallel strategy
    <ul>
      <li><a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat">https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat</a></li>
      <li>uses Hybrid Engine to seamlessly switch model partitioning between training and inference, such as using tensor parallelism to improve throughput during inference and using ZeRO or LoRA to improve memory utilization during training, providing outstanding system efficiency for RLHF training</li>
    </ul>
  </li>
  <li><strong>HuggingFace TRL</strong>, parallel strategy
    <ul>
      <li><a href="https://huggingface.co/docs/trl/en/index">https://huggingface.co/docs/trl/en/index</a></li>
      <li>make full use of various parameter-efficient fine-tuning (PEFT) methods, such as LoRA or QLoRA, to save memory cost, and use a dedicated kernel designed by unsloth to increase the training speed of RLHF.</li>
    </ul>
  </li>
  <li><strong>FlashAttention</strong>, 内存优化
    <ul>
      <li><a href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a></li>
      <li>an IO-aware tiling algorithm is proposed to reduce the number of memory reads/writes between slow HBM and fast on-chip SRAM based on the online softmax. 看能不能自己实现一遍这个算法，网上应该有一些简化版的 kernel 教程，可以参考学习一下</li>
      <li>Selective-checkpointing selectively discards the activations of memory-intensive attention modules. FlashAttention fuses the attention module into a single kernel, and also employs selective-checkpointing to reduce memory consumption. 这个看一下具体是怎么做的</li>
    </ul>
  </li>
  <li><strong>FlashAttention 2</strong>: 内存优化, efficiently handles variable-length inputs by parallelizing the sequence length dimension inseparably
    <ul>
      <li>这个是怎么实现的，去学习一下代码</li>
    </ul>
  </li>
  <li><strong>FlashAttention 3</strong>: 内存优化, An interleaved block-wise GEMM and softmax algorithm is redesigned based on FlashAttention-2 to hide the non-GEMM operations in softmax with the asynchronous WGMMA instructions for GEMM. Besides, by leveraging the asynchrony of the Tensor Cores and Tensor Memory Accelerator (TMA), overall computation is overlapped with data movement via a warp-specialized software pipelining scheme. Blockwise Parallel Transformer (BPT) further reduces the substantial memory requirements by extending the tiling algorithm in FlashAttention to fuse the feedforward network
    <ul>
      <li>需要学习了解一下 WGMMA, Tensor Cores, Tensor Memory Accelerator, Blockwise Parallel Transformer</li>
    </ul>
  </li>
  <li><strong>Triton</strong>, 用来写 kernel, 计算优化，听说现在很多公司内部在大量的使用这个写 Kernel, 可以学习一下 #kernel #CUDA
    <ul>
      <li><a href="https://github.com/triton-lang/triton">https://github.com/triton-lang/triton</a></li>
    </ul>
  </li>
  <li><strong>ZERO</strong>, 通过 fully sharding 来进行内存优化, ZERO1, 2, 3
    <ul>
      <li><a href="https://arxiv.org/pdf/1910.02054">https://arxiv.org/pdf/1910.02054</a></li>
      <li>ZeRO-3 employs per-parameter sharding to shard the full model and utilizes All-Gather and ReduceScatter for unsharding and sharding communication, respectively</li>
      <li><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/"><strong>ZERO++</strong></a> 感觉也算是 ZERO 家族的一员，但是是一种 partial sharding 的办法，在 ZERO3 的基础之上, further introduces a secondary shard of parameters within subgroups of GPUs and uses quantization to compress parameters and gradients, effectively diminishing communication volume with a trade-off in accuracy</li>
      <li><a href="https://arxiv.org/pdf/2101.06840"><strong>ZeRO-Offload</strong></a> concentrates on multi-GPU training. It holds model parameters on GPU, and stores optimizer states and gradients on CPU memory. In addition, it offloads optimizer update computation to the CPU.</li>
    </ul>
  </li>
  <li><strong>Ring AllReduce</strong> 算法: <a href="https://github.com/baidu-research/baidu-allreduce">https://github.com/baidu-research/baidu-allreduce</a></li>
  <li><a href="https://arxiv.org/pdf/1802.05799"><strong>Horovod</strong></a>: replaced the Baidu ring-AllReduce implementation with NCCL and designed a user-friendly interface for distributed training</li>
  <li><strong>Pytorch DPP</strong>: fuse multiple sequential AllReduce communication operations into a single operation. This method avoids transmitting a large number of small tensors over the network by waiting for a short period of time and then combining multiple gradients into one AllReduce operation during the backward phase. 通信优化的一种办法，可以看看代码学习一下</li>
  <li><strong>FSDP</strong>: <a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html</a></li>
  <li><a href="https://arxiv.org/pdf/1811.06965"><strong>GPipe</strong></a> 是之前听说过的一种方法，貌似是目前比较流行的方法，但是仍然会在开始和结束的时候有大量的 bubble 出现
    <ul>
      <li><a href="https://github.com/kakaobrain/torchgpipe">https://github.com/kakaobrain/torchgpipe</a></li>
    </ul>
  </li>
</ul>

<h2 id="一些比较主流的和重要的概念">一些比较主流的和重要的概念</h2>

<h3 id="parallelism-strategy">Parallelism Strategy</h3>

<ul>
  <li><strong>Tensor parallelism</strong>: partitions the parameter tensors of each layer along multiple dimensions, effectively distributing the model parameters across the available GPUs. 感觉 tensor parallelism 没有 data/model parallelism 那么常见，在工作中没怎么看到用这种方法的
    <ul>
      <li>it is challenging to overlap the communication with computation, necessitating the use of high-bandwidth connections. Consequently, tensor parallelism is more commonly employed in a single GPU node.</li>
    </ul>
  </li>
  <li><strong>Pipeline parallelism</strong>: pipeline parallelism only necessitates the exchange of intermediate tensors at designated cutting points, resulting in less frequent communication requirements, pipeline parallelism 算是比较常用的东西了
    <ul>
      <li>但是 pipeline parallelism 也有两个问题，一个是 pipeline bubble, 一个是 memory consumption imbalance</li>
    </ul>
  </li>
  <li><strong>Sequence parallelism</strong>: It divides the input data into multiple chunks along the sequence dimension and each chunk is fed to one GPU for computation. 没怎么听说过这种方法，可以找来一些 code 来学习一下
    <ul>
      <li><a href="https://arxiv.org/pdf/1911.02150">MQA</a> 和 <a href="https://arxiv.org/pdf/2305.13245">GQA</a> 就是属于这个范畴, 可以好好的学习一下</li>
      <li><a href="https://arxiv.org/abs/2310.01889"><strong>Ring Self-Attention</strong></a> leverages sequence parallelism and calculates the self-attention with ring-style communication to scale up the context window of LLM training. It first transmits the key tensors among GPUs to calculate the attention scores in a circular fashion, and then calculates the self-attention output based on the attention scores and value tensors transmitted in a similar fashion</li>
    </ul>
  </li>
  <li><strong>MoE parallelism</strong>: MoE 的结构在目前主流的 LLM 里面都得到了大量的使用，可以看看下面的这几篇文章里面介绍的针对 MOE 的 parallel strategy 的方法 #MOE
    <ul>
      <li><a href="https://arxiv.org/abs/2006.16668"><strong>GShard</strong></a>: extends the idea of MoE to Transformers in distributed settings, where experts are distributed across different workers and collaborates with All-to-All communication</li>
      <li><a href="https://arxiv.org/abs/2201.05596"><strong>DeepSpeed-MOE</strong></a>: proposes a new distributed MoE architecture that applies shared experts in each worker and places more experts in deeper layers to balance communication costs with training accuracy</li>
    </ul>
  </li>
  <li>Since General Matrix Multiplications (GeMMs) require the size of all experts’ inputs to be consistent, existing MoE training frameworks often perform token dropping and padding to match the same expert capacity, which wastes computation.
    <ul>
      <li>General Matrix Multiplications (GeMMs) 的工作原理可以参考: <a href="https://spatial-lang.org/gemm">https://spatial-lang.org/gemm</a></li>
      <li>Token dropping and padding 的常用方法是什么？有没有具体的实现代码样例</li>
    </ul>
  </li>
  <li>针对 MOE parallel strategy 中 communication 的优化
    <ul>
      <li><a href="https://arxiv.org/abs/2206.03382"><strong>Tutel</strong></a>: divides the input tensors into groups along the expert capacity dimension and overlaps computation and communication among different groups to hide All-to-All overhead</li>
      <li><strong>Tutel</strong>: optimizes the All-to-All kernel implementation by aggregating small messages into a single large chunk inside the nodes before exchanging data among different nodes #Batching</li>
      <li><a href="https://www.usenix.org/system/files/atc23-li-jiamin.pdf"><strong>Lina</strong></a> analyzes the All-to-All overhead of MoE during distributed training and inference systematically and finds that All-to-All latency is prolonged when it overlaps with AllReduce operations. Lina proposes prioritizing All-to-All over AllReduce to improve its bandwidth and reduce its blocking period in distributed training
        <ul>
          <li>很有意思的发现，可以去学习一下原文里面是怎么发现这个问题的，然后应用在自己以后的工作中</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>This heterogeneity is also reflected in model architectures, particularly with Reinforcement Learning from Human Feedback (RLHF). Utilizing heterogeneous hardware and diverse model architectures has become essential for the efficient training of LLMs
    <ul>
      <li>再重新学习一下 RLHF，来理解这里面提到的 <strong>异构性</strong> 的特点</li>
    </ul>
  </li>
</ul>

<h3 id="memory-optimization">Memory Optimization</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2112.05682">Rabe</a> 这篇论文中证明了自注意力只需要 O(logn) 的内存就可以了，学习一下这篇论文里面的工作</li>
  <li>了解一下 FP16 和 BF16 的工作原理，内存优化</li>
  <li>LLM training 的过程中主要吃内存的部分
    <ul>
      <li>Model States: Model states encompass the memory consumed by the optimizer states, gradients, and model parameters</li>
      <li>Activations refer to the tensors generated during the forward pass</li>
      <li>Temporary Buffers: Temporary buffers are used to store intermediate results</li>
      <li>Memory Fragmentation: Memory fragmentation can lead to scenarios where memory requests fail despite having a large amount of available memory, 这个在 Pytorch 里面由于内存分配机制会出现这种问题，可以再找一些额外的资料详细的了解一下</li>
    </ul>
  </li>
  <li>Deep learning frameworks typically use a caching allocator with a memory pool to enable fast memory allocation and deallocation without requiring device synchronization.</li>
  <li>一个用来估算所需要的内存的简易办法
    <ul>
      <li>When training a model with Φ parameters,4Φ bytes are needed to store parameters and gradients. The 32-bit copies of the parameters, momentum, and variance each require 4Φ bytes, totaling12Φ bytes. Therefore, the overall memory requirement for storing model states is 16Φ bytes，这个再好好看一下理解一下</li>
    </ul>
  </li>
  <li>一些用来进行 Memory 优化的整体大方向
    <ul>
      <li>Activation re-computation strategies, which trade increased computation for reduced memory usage, 这个是现在最主流的方法之一，可以找一些代码来看看是如何实现的，这个方法的一个关键就是节省的内存和额外计算之间的 trade off</li>
      <li>Redundancy reduction methods that minimize data duplication across training processes</li>
      <li>Defragmentation techniques that optimize memory allocation and deallocation to reduce fragmentation and improve memory utilization</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/abs/2401.08156"><strong>GMLake</strong></a> and <a href="https://pytorch.org/docs/stable/notes/cuda.html"><strong>PyTorch expandable segments</strong></a> propose to mitigate fragmentation by utilizing the virtual memory management (VMM) functions of the low-level CUDA driver application programming interface. 可以看看 PyTorch 里面这个工作</li>
  <li>Swap and offload approaches that leverage CPU memory and NVMe SSDs to supplement GPU memory
    <ul>
      <li>CPU offloading: static/dynamic</li>
      <li>SSD offloading, 这个在之前的 GPU training paper 里面好像看到过</li>
    </ul>
  </li>
</ul>

<h3 id="communication-optimization">Communication Optimization</h3>

<p>一些和通信相关的优化</p>

<ul>
  <li>NVIDIA’s NCCL and AMD’s RCCL are highly optimized libraries that typically outperform MPI-based collective communication libraries on their respective AI accelerators. These libraries usually select pre-defined algorithms to perform collectives based on conditions such as network topology and input tensor size. 可以去学习一下 NCCL</li>
  <li>通信的不同算法: <strong>Ring, Tree, Hybrid</strong></li>
  <li>Conventional frameworks simultaneously perform gradient computation for both weights and outputs. <a href="https://github.com/mlsys-seo/ooo-backprop"><strong>Out-of-order backpropagation (ooo-backprop)</strong></a> decouples the gradient computations for weights and outputs, scheduling the weight gradient computations flexibly out of their original order. This allows more critical computations to be prioritized and scheduled accordingly. Consequently, ooo-backprop optimizes overall performance by scheduling communications based on this out-of-order computation strategy. 这个工作很有意思，把 activation 和 gradient 的 communication 拆开然后进行类似不同的 priority 的 communication</li>
  <li><strong>In-network aggregation (INA)</strong> uses the computational capabilities of network devices to perform aggregation operations like summing gradients of deep learning models.</li>
</ul>

<h3 id="fault-tolerance">Fault Tolerance</h3>

<p>Failure tolerance 主流的还是使用 checkpoint</p>

<ul>
  <li><strong>Synchronous checkpoint</strong></li>
  <li><a href="https://www.usenix.org/system/files/nsdi22-paper-eisenman.pdf"><strong>Check-N-Run</strong></a> decouples the snapshot and persist phases. It achieves atomic checkpointing by stalling training only during the snapshot phase and asynchronously persisting snapshots using dedicated background CPU processes.</li>
  <li><a href="https://web.cels.anl.gov/~woz/papers/DeepFreeze_2020.pdf"><strong>DeepFreeze</strong></a> applies both lightweight (snapshot) and heavy(persist) persistence strategies in the background, sharding checkpoints across data-parallel GPUs to distribute I/O workload.</li>
  <li><strong>Gemini</strong> proposes checkpointing to CPU memory for faster failure recovery, along with a checkpoint placement strategy to minimize checkpoint loss and a traffic scheduling algorithm to reduce interference with training.</li>
  <li><a href="https://www.usenix.org/system/files/fast21-pan.pdf"><strong>Tectonic</strong></a>: Meta’s distributed filesystem, enables thousands of GPUs to save and load model checkpoints simultaneously, providing efficient and scalable storage solutions for extensive training operations</li>
  <li>现在貌似主要用来对 checkpoint 用来存储的都是 object store, 这个可以去研究下看看各个公司都用啥（比如 AWS 是不是都上 S3）</li>
  <li>Live migration leverages the inherent redundancy present in distributed LLM training setups, particularly the model replicas across different data parallel pipelines, to restore model states in case of failure. 这个感觉其实有点类似使用 Cassandra 里 consistency hashing 里面的 hinted hand off</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://pyemma.github.io/tags/#distributed-training" class="page__taxonomy-item" rel="tag">distributed training</a><span class="sep">, </span>
    
      
      
      <a href="https://pyemma.github.io/tags/#llm-training" class="page__taxonomy-item" rel="tag">llm training</a><span class="sep">, </span>
    
      
      
      <a href="https://pyemma.github.io/tags/#llm" class="page__taxonomy-item" rel="tag">llm</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2024-09-01T00:00:00-07:00">September 01, 2024</time></p>
        
      </footer>

      
  <nav class="pagination">
    
      <a href="https://pyemma.github.io/Book-Pattern-of-Distributed-System/" class="pagination--pager" title="读书笔记 - Patterns of Distributed System
">Previous</a>
    
    
      <a href="https://pyemma.github.io/Flyte-How-Workflow-Get-Scheduled/" class="pagination--pager" title="How Workflow Get Scheduled via Plugins in Flyte
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="https://pyemma.github.io/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Bayarea Coding Monkey. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="https://pyemma.github.io/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "https://pyemma.github.io/LLM-LLM-Training-101/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/[LLM]-LLM-Training-101"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://pyemma.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>
