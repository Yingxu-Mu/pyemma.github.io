<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Reinforcement Learning Lesson 1 - Coding Monkey</title>
<meta name="description" content="This is the first post for the series reinforcement learning. The main source for the entire series is here. The post mainly focus on summarizing the content introduced in the video and slides, as well as some of my own understanding. Any feedback is welcomed.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey">
<meta property="og:title" content="Reinforcement Learning Lesson 1">
<meta property="og:url" content="https://pyemma.github.io/Reinforcement-Learning-Lesson-1/">


  <meta property="og:description" content="This is the first post for the series reinforcement learning. The main source for the entire series is here. The post mainly focus on summarizing the content introduced in the video and slides, as well as some of my own understanding. Any feedback is welcomed.">







  <meta property="article:published_time" content="2017-08-13T00:00:00-07:00">





  

  


<link rel="canonical" href="https://pyemma.github.io/Reinforcement-Learning-Lesson-1/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Pei",
      "url" : "https://pyemma.github.io",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="https://pyemma.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://pyemma.github.io/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://pyemma.github.io/">Coding Monkey</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/" >Quick-Start Guide</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yang Pei</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a Coding Monkey
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Mountain View</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Reinforcement Learning Lesson 1">
    <meta itemprop="description" content="This is the first post for the series reinforcement learning. The main source for the entire series is here. The post mainly focus on summarizing the content introduced in the video and slides, as well as some of my own understanding. Any feedback is welcomed.">
    <meta itemprop="datePublished" content="August 13, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Reinforcement Learning Lesson 1
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>This is the first post for the series reinforcement learning. The main source for the entire series is <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html">here</a>. The post mainly focus on summarizing the content introduced in the video and slides, as well as some of my own understanding. Any feedback is welcomed.</p>

<p>In this post, we will talk about Markov Decision Process (MDP), which is a pretty fundamental model in many reinforcement learning cases.</p>
<blockquote>
  <p>Almost all RL problems can be formalized as MDP</p>
</blockquote>

<h3 id="markov-process">Markov Process</h3>
<p>In order to learn about MDP, we need to first know what is Markov Process (MP). This introduces the following two concept:</p>
<ul>
  <li>Markov Property</li>
  <li>State Transition Matrix</li>
</ul>

<p>In the most simple word, <strong>Markov Property</strong> means that the future state is independent on the history given the current state. It can be formalized using following statement:</p>

<script type="math/tex; mode=display">\mathbb{P}[S_{t+1}|S_{t}] = \mathbb{P}[S_{t+1}|S_1, ..., S_t]</script>

<p>This means that the current state contains all they necessary information for the future, and we can discard all history information.</p>

<p><strong>State Transition Matrix</strong> contains the probability we go from on state to another one. Given a state \( s \) and its successor state \( s^\prime \), the probability from \( s \) goes to \( s^\prime \) is given by</p>

<script type="math/tex; mode=display">P_{ss\prime} = \mathbb{P}[S_{t+1}=s\prime|S_{t}=s]</script>

<p>And the State Transition Matrix is by</p>

<script type="math/tex; mode=display">% <![CDATA[
P =
\begin{Bmatrix}
P_{11} & ... & P_{1n} \\
\vdots & ... & \vdots \\
P_{n1} & ... & P_{nn}
\end{Bmatrix} %]]></script>

<p>From the above two concept, we can notice two things and these are also the constraint for MDP:</p>
<ul>
  <li>The state is finite (otherwise the definition of State Transition Matrix is problematic)</li>
  <li>The environment is fully observable, no hidden state exists</li>
</ul>

<p>We can obtain a definition for MP as a tuple \( &lt;S, P&gt; \):</p>
<ul>
  <li>\( S \) is a finite state set</li>
  <li>\( P \) is a state transition matrix</li>
</ul>

<p>An example of MP:</p>

<p><img src="/assets/mdp.png" alt="Markov Process" /></p>

<h3 id="markov-reward-process">Markov Reward Process</h3>
<p>Markov Process combined with values, then we have Markov Reward Process (MRP), defined by a tuple \( &lt;S, P, R, \gamma&gt; \):</p>
<ul>
  <li>\( S \) is a finite state set</li>
  <li>\( P \) is a state transition matrix</li>
  <li>\( R \) is a reward function,
<script type="math/tex">R_{s} = \mathbb{E}[R_{t+1}|S_{t}=s]</script></li>
  <li>\( \gamma \) is a discounting ratio</li>
</ul>

<p>As we have introduced reward, we can measure how many rewards we can get in each state. We define return \( G_t \) as the discounted rewards we can get from timestamp  \( t \), and state value function \( v(s) \) the expected return we can get starting from state \( s \):</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=1}^{\infty}\gamma^{k}R_{t+k+1}</script>

<script type="math/tex; mode=display">v(s) = \mathbb{E}[G_t|S_t=s]</script>

<p>Note that the return is accumulated with discounting. This is important is:</p>
<ul>
  <li>Avoid infinity loop that might exist in MDP (e.g. self loop)</li>
  <li>Model the uncertainty about the future</li>
  <li>From common sense, human prefer immediate reward than long term ones</li>
</ul>

<p>We can breakdown the state value function into two parts, immediate and long term. Using recurse, we have the Bellman Equation for MRP:</p>

<script type="math/tex; mode=display">v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t=s]</script>

<p>By expanding the above expectation and using sum to replace expectation operator, we have:</p>

<script type="math/tex; mode=display">v(s) = R_s + \gamma\sum_{s^\prime\in S}P_{ss^\prime}v(s^\prime)</script>

<h3 id="markov-decision-process">Markov Decision Process</h3>
<p>Adding the actions we can make among the state, we finally have the definition for MDP, which is \( &lt;S, A, P, R, \gamma&gt; \):</p>
<ul>
  <li>\( S \) is a finite state set</li>
  <li>\( A \) is a finite action set</li>
  <li>\( P \) is a state transition matrix,
<script type="math/tex">P_{ss^\prime}^a = \mathbb{P}[S_{t+1}=s^\prime|S_{t}=s, A_t=a]</script></li>
  <li>\( R \) is a reward function,
<script type="math/tex">R_{s}^a = \mathbb{E}[R_{t+1}|S_{t}=s, A_t=a]</script></li>
  <li>\( \gamma\) is a discounting ratio</li>
</ul>

<p>Notice the change on the state transition matrix, before we only have a single matrix, and now we have one for each action \( a \) we can think of in the pervious case, we have only single action). Under different action \( a \), the transition probability can be different between the two state. We can now regard the new state transition matrix as a tensor with three dimension.</p>

<p>As we have actions to now, we need to make decision how to take actions. A <strong>policy</strong> is a distribution over actions given a state:</p>

<script type="math/tex; mode=display">\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]</script>

<p>A policy fully determines how an agent would act, and it does not depend on the history. Similar to MRP, we have state value function for MDP as the expected return starting from \( s \), following the policy \( \pi \):</p>

<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s]</script>

<p>We can also define an action value function, which is the expected return we get starting from state \( s \), taking action \( a \) and following policy \( \pi \):</p>

<script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a]</script>

<p>These two functions have relationship and can be transformed to each other easily, to get state value function, we can summation over all the action value function for the action that state we could get, weight by the policy:</p>

<script type="math/tex; mode=display">v_\pi(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s, a)</script>

<p>And the action value function can be obtained by summation overall all state we can transition to, weighted by the state transition matrix:</p>

<script type="math/tex; mode=display">q_\pi(s, a) = R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a v(s^\prime)</script>

<p>Combine the above equations, we can obtain the Bellman Exception Equation as:</p>

<script type="math/tex; mode=display">v_\pi(s) = \sum_{a\in A}\pi(a|s)(R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a v(s^\prime))</script>

<script type="math/tex; mode=display">q_\pi(s, a) = R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a \sum_{a\in A}\pi(a|s^\prime)q_{\pi}(s^\prime, a)</script>

<p>Given a MDP, if we want to solve it (to know what’s the best performance we can get, e.g. What’s the maximum rewards we can get in the terminate state), we need to find the optimal value function for it. As long as we have obtain the optimal value function, we can compose an optimal policy easily:</p>

<script type="math/tex; mode=display">% <![CDATA[
\pi_{*}(a|s) =
\begin{cases}
1,  & \text{if $a = argmax_{a\in A} q(s, a)$} \\
0, & \text{otherwise}
\end{cases} %]]></script>

<blockquote>
  <p>There exists an optimal policy \( \pi_{*} \) that is better than or equal to all other policy for any MDP. All optimal policy achieve optimal state value function and optimal action value function</p>
</blockquote>

<p>Following this policy, we can change our Bellman Exception Equation to Bellman Optimality Equation:</p>

<script type="math/tex; mode=display">v_*(s)=max_{a}(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^a v_*(s^\prime))</script>

<script type="math/tex; mode=display">q_*(s, a)=R_s^a + \gamma\sum_{s^\prime\in S} argmax_{a} P_{ss^\prime}^a v_*(s^\prime)</script>

<p>Bellman Optimality Equation is non-linear, there is no closed form solution for it. However, we can solve it by some iterative methods (will introduce in later lectures):</p>
<ul>
  <li>Policy Iteration</li>
  <li>Value Iteration</li>
  <li>Q-learning</li>
  <li>Sarsa</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-08-13T00:00:00-07:00">August 13, 2017</time></p>
        
      </footer>

      
  <nav class="pagination">
    
      <a href="https://pyemma.github.io/Math-Equation/" class="pagination--pager" title="Math Equation
">Previous</a>
    
    
      <a href="https://pyemma.github.io/Reinforcement-Learning-Lesson-2/" class="pagination--pager" title="Reinforcement Learning Lesson 2
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="https://pyemma.github.io/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Yang Pei. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="https://pyemma.github.io/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "https://pyemma.github.io/Reinforcement-Learning-Lesson-1/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/Reinforcement-Learning-Lesson-1"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://pyemma.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>
