<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>What I Read This Week 1 - Coding Monkey</title>
<meta name="description" content="The 3 Tricks That Made AlphaGo Zero WorkThis post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:  Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using better training sample).  Use a single neural network to predict which move to recommend and which move are likely to win the game (This is the idea of using Multitask Learning).  Use a upgrade version of neural network (from convolutional neural network to residual neural network).">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey">
<meta property="og:title" content="What I Read This Week 1">
<meta property="og:url" content="https://pyemma.github.io/What-I-Read-This-Week-I/">


  <meta property="og:description" content="The 3 Tricks That Made AlphaGo Zero WorkThis post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:  Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using better training sample).  Use a single neural network to predict which move to recommend and which move are likely to win the game (This is the idea of using Multitask Learning).  Use a upgrade version of neural network (from convolutional neural network to residual neural network).">







  <meta property="article:published_time" content="2018-01-14T00:00:00-08:00">





  

  


<link rel="canonical" href="https://pyemma.github.io/What-I-Read-This-Week-I/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Pei",
      "url" : "https://pyemma.github.io",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="https://pyemma.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://pyemma.github.io/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://pyemma.github.io/">Coding Monkey</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/" >Quick-Start Guide</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yang Pei</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a Coding Monkey
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Mountain View</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="What I Read This Week 1">
    <meta itemprop="description" content="The 3 Tricks That Made AlphaGo Zero WorkThis post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:  Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using better training sample).  Use a single neural network to predict which move to recommend and which move are likely to win the game (This is the idea of using Multitask Learning).  Use a upgrade version of neural network (from convolutional neural network to residual neural network).">
    <meta itemprop="datePublished" content="January 14, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">What I Read This Week 1
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3 id="the-3-tricks-that-made-alphago-zero-work"><a href="https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef">The 3 Tricks That Made AlphaGo Zero Work</a></h3>
<p>This post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead to the supreme result:</p>
<ul>
  <li>Use the evaluations provided by MCTS to continually improve the neural network’s evaluations of the board position, instead of using human play (This is actually the idea of using <strong>better training sample</strong>).</li>
  <li>Use a single neural network to predict which <strong>move</strong> to recommend <em>and</em> which <strong>move</strong> are likely to win the game (This is the idea of using <a href="http://ruder.io/multi-task/index.html#introduction"><strong>Multitask Learning</strong></a>).</li>
  <li>Use a upgrade version of neural network (from convolutional neural network to <strong>residual neural network</strong>).</li>
</ul>

<h3 id="intuitive-rl-intro-to-advantage-actor-critic-a2c"><a href="https://medium.com/@rudygilman/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752">Intuitive RL: Intro to Advantage-Actor-Critic (A2C)</a></h3>
<p>A very vivid introduction of <strong>Advantage-Actor-Critic</strong> reinforcement learning.</p>
<ol>
  <li>First we should keep in mind that <strong>Actor-Critic</strong> is a blend of both value estimation and policy estimation reinforcement learning method, a.k.a we will try to learn value function, as well as policy from game play (This is different from pure value function based method and policy based method).</li>
  <li>In <strong>Actor-Critic</strong>, the <strong>Actor</strong> will tries to optimize the parameter for policy and <strong>Critic</strong> will tries to optimize the parameter for the value function of a state. This can be done by having a single model outputting both the value of the state, as will as the probability of action.</li>
  <li>By jump into one state, taking action and get reward. We will get the training examples for our <strong>Critic</strong>. The estimate for each state will become more and more accurate. In this way, we don’t need to wait until the end of the game to get the value of each state, which is high in variance.</li>
  <li>In stead of simply policy gradient update the policy (which tries to avoid the action that lead to a state with low value), we use <strong>Advantage</strong>, which is the relative improvement of the action take (e.g. current state is -100, and take action A we arrive in a state with -20, the improvement of the action is 80!). The idea behind this is that the action might be the result that result in a low value.</li>
</ol>

<h3 id="ai-and-deep-learning-in-2017--a-year-in-review"><a href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/">AI and Deep Learning in 2017 – A Year in Review</a></h3>
<p>A really awesome post that summarize what is going on in deep learning in 2017. Some points that I enjoy most:</p>
<ul>
  <li>Evolution Algorithm (e.g. Genetic Algorithm) is coming back again.</li>
  <li>Lots of deep learning framework is available right now: PyTorch is pretty popular in academic, but personally I thing TensorFlow is still the bests to try out (It’s also my plan to be more familiar with TensorFlow and work on some side project).</li>
  <li>A good online reinforcement learning algorithm to read: <a href="https://github.com/openai/baselines"><em>OpenAI Baseline</em></a>.</li>
  <li>A good online courses: https://stats385.github.io/</li>
</ul>

<h3 id="effective-modern-c">Effective Modern C++</h3>
<p>Mainly read the chapter about lambda function, some take away is:</p>
<ul>
  <li>Avoid using default capture</li>
  <li>In C++14, we can use init capture to move data we would like to use into the closure class, or use some expression to initialize the data member</li>
</ul>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="p">[</span><span class="n">pw</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">pw</span><span class="p">)](){</span> <span class="k">return</span> <span class="n">pw</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">();</span> <span class="p">};</span>
<span class="p">[</span><span class="n">pw</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">Widget</span><span class="o">&gt;</span><span class="p">()](){</span> <span class="k">return</span> <span class="n">pw</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">();</span> <span class="p">};</span></code></pre></figure>

<ul>
  <li>Use decltype on auto&amp;&amp; parameters to forward them</li>
</ul>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">auto</span> <span class="n">f</span> <span class="o">=</span> <span class="p">[](</span><span class="k">auto</span><span class="o">&amp;&amp;</span> <span class="n">param</span><span class="p">){</span>
  <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">));</span> <span class="p">}</span></code></pre></figure>

<p>Another thing to remember is to prefer using <strong>alias</strong> declarations than <strong>typedefs</strong>, because <strong>alias</strong> supports templates better:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
<span class="k">using</span> <span class="n">MyAllocList</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">MyAlloc</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span><span class="p">;</span></code></pre></figure>


        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-01-14T00:00:00-08:00">January 14, 2018</time></p>
        
      </footer>

      
  <nav class="pagination">
    
      <a href="https://pyemma.github.io/Deep-Work-Reading-Note/" class="pagination--pager" title="Deep Work Reading Note
">Previous</a>
    
    
      <a href="https://pyemma.github.io/What-I-Read-This-Week-2/" class="pagination--pager" title="What I Read This Week 2
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="https://pyemma.github.io/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Yang Pei. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="https://pyemma.github.io/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "https://pyemma.github.io/What-I-Read-This-Week-I/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/What-I-Read-This-Week-I"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://pyemma.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>
