<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Reinforcement Learning Lesson 4 - Coding Monkey</title>
<meta name="description" content="In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey">
<meta property="og:title" content="Reinforcement Learning Lesson 4">
<meta property="og:url" content="http://localhost:4000/Reinforcement-Learning-Lesson-4/">


  <meta property="og:description" content="In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.">







  <meta property="article:published_time" content="2017-08-19T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/Reinforcement-Learning-Lesson-4/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Pei",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/">Coding Monkey</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/" >Quick-Start Guide</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yang Pei</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a Coding Monkey
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Mountain View</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Reinforcement Learning Lesson 4">
    <meta itemprop="description" content="In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.">
    <meta itemprop="datePublished" content="August 19, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Reinforcement Learning Lesson 4
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.</p>

<h4 id="mento-calro-policy-iteration">Mento Calro Policy Iteration</h4>
<p>In the <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-2">Lesson 2</a>, we mentioned how to solve a MDP when we have full information about the MDP. One method is called <strong>Policy Iteration</strong>. It can be divided into two components: <em>policy iterative evaluation</em> and <em>policy improvement</em>. For the evaluation part, we can use the methods in <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-3">last lesson</a>, nominally MC and TD. However, we could not directly use the state value function, cause in the policy improvement step (e.g. greedy), we need to know the $R$ and $P$ to find the best action (recall the <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1">Bellman Optimality Function</a>). However, action value function does not need the model of the MDP while in greedy policy improvement:</p>

<script type="math/tex; mode=display">v_*(s) = argmax_a q(s, a)</script>

<p>For the policy improvement part. If we stick to the greedy method, it will not be good for us to explore all possible states. So we use another method which is called $\epsilon$-greedy. We will have $1-\epsilon$ probability to perform greedily (choose the current best action), and have $\epsilon$ probability to random choose an action:</p>

<script type="math/tex; mode=display">% <![CDATA[
\pi(s|a) = \begin{cases}
\frac{\epsilon}{m} + 1 - \epsilon, & \text{if $a^\star = argmax_a Q(s, a)$} \\
\frac{\epsilon}{m}, & \text{otherwise}
\end{cases} %]]></script>

<p>We have the final Mento Calro Policy Iteration as:</p>
<ul>
  <li>Sample the kth episode $S_1, A_1, …, S_T$ from policy $\pi$</li>
  <li>For each state $S_t$ and $A_t$ in the episode</li>
</ul>

<script type="math/tex; mode=display">N(S_t, A_t) = N(S_t, A_t) + 1 \\
Q(S_t, A_t) = Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}((G_t - Q(S_t, A_t))) \\</script>

<ul>
  <li>Update the $\epsilon$ and policy:</li>
</ul>

<script type="math/tex; mode=display">\epsilon = 1/k \\
\pi = \epsilon\text{-greedy}(Q)</script>

<h4 id="sarsa-algorithm">Sarsa Algorithm</h4>
<p>If we use the logic in TD for the evaluation part, then we would have the sarsa algorithm. The main difference is that, in original TD, we use the value state function of the successor state, however, we need the action value function right now. We can obtain that by run our current policy again (remember, TD does not need the complete sequence of experience, we can generate the state and action along the way). Following is the algorithm:</p>
<ul>
  <li>Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0</li>
  <li>Repeat for each episode
    <ul>
      <li>Initialize $S$, choose $A$ from the current policy derived from $Q$</li>
      <li>Repeat for each step in the episode until we hit terminal
        <ul>
          <li>Take action $A$, observe $R$ and $S^\prime$</li>
          <li>Choose $A^\prime$ from $S^\prime$ from the current policy derived from $Q$</li>
          <li>Update <script type="math/tex">Q(S, A) = Q(S, A) + \alpha(R + \gamma Q(S^\prime, A^\prime) - Q(S, A))</script></li>
          <li>Update $S = S^\prime, A = A^\prime$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Similarly, we can also use Eligibility Trace for the sarsa algorithm and result in sarsa($\lambda$) algorithm. The algorithm is as follow:</p>
<ul>
  <li>Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0
    <ul>
      <li>Repeat for each episode</li>
      <li>Initialize $E$ for each $s, a$ pair to 0</li>
      <li>Initialize $S$, choose $A$ from the current policy derived from Q</li>
      <li>Repeat for each step in the episode until we hit terminal
        <ul>
          <li>Take action $A$, observe $R$ and $S^\prime$</li>
          <li>Choose $A^\prime$ from $S^\prime$ from the current policy derived from Q</li>
          <li>Calculate $\delta = R + \gamma Q(S^\prime, A^\prime) - Q(S, A)$</li>
          <li>Update $E(S, A) = E(S, A) + 1$</li>
          <li>For each $s$ and $a$ pair</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">Q(s, a) = Q(s, a) + \alpha\delta E(s, a) \\
E(s, a) = \gamma\lambda E(s, a)</script>

<ul>
  <li>Update $S = S^\prime, A = A^\prime$</li>
</ul>

<h4 id="q-learning">Q Learning</h4>
<p>Both MC policy iteration and sarsa algorithm are <strong>online learning</strong> method, which means that they are observing there own policy, learning along the process. There is another category which is called <strong>offline learning</strong>, in which we learn from other policy, not the policy we are trying to improving. Example is that a robots learns walking by observing human. Q learning falls in this category. It is pretty similar to the sarsa algorithm, the only difference is that when we get the action for successor state, we replace the $\epsilon$-greedy to greedy policy. The Q learning method is as follow:</p>
<ul>
  <li>Initialize Q for each state and action pair arbitrarily, set Q(terminate, *) to 0</li>
  <li>Repeat for each episode
    <ul>
      <li>Initialize $S$, choose $A$ from the current policy derived from Q</li>
      <li>Repeat for each step in the episode until we hit terminal
        <ul>
          <li>Take action $A$, observe $R$ and $S^\prime$</li>
          <li>Update <script type="math/tex">Q(S, A) = Q(S, A) + \alpha(R + \gamma max_a Q(S^\prime, a) - Q(S, A))</script></li>
          <li>Update $S = S^\prime$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-08-19T00:00:00-07:00">August 19, 2017</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Reinforcement+Learning+Lesson+4%20http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-4%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-4%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-4%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2FReinforcement-Learning-Lesson-4%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="http://localhost:4000/Reinforcement-Learning-Lession-3/" class="pagination--pager" title="Reinforcement Learning Lesson 3
">Previous</a>
    
    
      <a href="http://localhost:4000/Reinforcement-Learning-Lesson-5/" class="pagination--pager" title="Reinforcement Learning Lesson 5
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/What-I-Read-This-Week-I/" rel="permalink">What I Read This Week 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">The 3 Tricks That Made AlphaGo Zero Work
This post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead t...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/2018-%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/" rel="permalink">2018 新年计划
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">2018年的主题，就是要变得自信以及学会Deep Work!

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/Deep-Work-Reading-Note/" rel="permalink">Deep Work Reading Note
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">“Deep Work” is the first book I read this year. I was pretty impressed by the idea and methods the author purposed to help you gain the ability to do “deep w...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/" rel="permalink">2017 年终终结
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">2017年一转眼就过去了，自己也从一个刚刚毕业的学生成了一个快上了两年班的上班族了。趁着年末好好回顾一下这一整年发生的事情，一方面是给自己留个纪念，另一方面也是好好总结一下这一年的得失，找到改进的方向。

</p>
  </article>
</div>
        
      </div>
    </div>
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Yang Pei. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="http://localhost:4000/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>








  </body>
</html>
