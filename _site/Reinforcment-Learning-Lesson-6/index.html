<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Reinforcement Learning Lesson 6 - Coding Monkey</title>
<meta name="description" content="In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Coding Monkey">
<meta property="og:title" content="Reinforcement Learning Lesson 6">
<meta property="og:url" content="http://localhost:4000/Reinforcment-Learning-Lesson-6/">


  <meta property="og:description" content="In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:">







  <meta property="article:published_time" content="2017-09-10T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/Reinforcment-Learning-Lesson-6/">







  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Pei",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Coding Monkey Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/">Coding Monkey</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/" >Quick-Start Guide</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yang Pei</h3>
    
    
      <p class="author__bio" itemprop="description">
        I am a Coding Monkey.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Mountain View</span>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Reinforcement Learning Lesson 6">
    <meta itemprop="description" content="In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:">
    <meta itemprop="datePublished" content="September 10, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Reinforcement Learning Lesson 6
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:</p>

<script type="math/tex; mode=display">\pi_{\theta}(s ,a) = P[a|s, \theta]</script>

<p>In this case, we are not going to have any value function. A slight variance of this method is called <strong>Actor-Critic</strong>, in which both value function and policy are modeled and learnt.</p>

<p>The advantage of <strong>Policy based RL</strong> is:</p>
<ul>
  <li>Better convergence properties</li>
  <li>Effective in high-dimensional or continuous action spaces</li>
  <li>Can learn stochastic policies</li>
</ul>

<h4 id="policy-objective-functions">Policy Objective Functions</h4>
<p>Since we are going to learn $\pi_\theta (s, a)$ and find the best $\theta$, we need to first find a way to measure the quality of our policy. These are called <strong>policy objective function</strong> and some we can use are:</p>
<ul>
  <li>In episode environment we can use the start value</li>
</ul>

<script type="math/tex; mode=display">J_1(\theta) = V^{\pi_\theta}(s_1) = E_{\pi_\theta}[v_1]</script>

<ul>
  <li>In continuous environment we can use average value or average reward pre time-step</li>
</ul>

<script type="math/tex; mode=display">J_{avV}(\theta) = \sum_{s}d^{\pi_\theta}(s)V^{\pi_\theta}(s) \\
J_{avR}(\theta) = \sum_{s}d^{\pi_\theta}(s)\sum_{a}\pi_\theta(s, a)R_s^a</script>

<p>where $d^{\pi_\theta}(s)$ is stationary distribution of Markov chain for $\pi_\theta$.</p>

<p>After we have the measurement of the policy quality, we are going to find the best parameter which gives us the best quality and this becomes an optimization problem. Actually, similar to the last post, we can also use stochastic gradient to help use here. Since we are trying to find the maximum value, we are going to use what is called gradient ascent to find the steepest direction to update our parameter (very similar to gradient decrease).</p>

<h4 id="score-function">Score Function</h4>
<p>In order to compute the policy gradient analytically, we introduced the <strong>score function</strong>. Assume policy $\pi_{\theta}$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta (s, a)$. Then using some tricky we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla_\theta \pi_\theta (s, a) & = \pi_\theta (s, a) \frac{\nabla_\theta \pi_\theta (s, a) }{\pi_\theta (s, a)} \\
& = \pi_\theta (s, a) \nabla_\theta log \pi_\theta (s, a)
\end{align} %]]></script>

<p>Here, $\nabla_\theta log \pi_\theta (s, a)$ is the <strong>score function</strong>.</p>

<h4 id="policy-gradient-theorem">Policy Gradient Theorem</h4>
<blockquote>
  <p>For any differentiable policy $\pi_\theta (s, a)$, for any of the policy objective functions $J_1, J_{avV}, J_{avR}$, the policy gradient is</p>
</blockquote>

<script type="math/tex; mode=display">\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log \pi_\theta (s, a) Q^{\pi_\theta} (s, a)]</script>

<h4 id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</h4>
<p>Use return as an unbiased sample of $Q^{\pi_\theta} (s, a)$, the algorithm is as follow:</p>
<ul>
  <li>Initialize $\theta$ arbitrarily
    <ul>
      <li>for each episode ${s_1, a_1, r_2, …, s_{T-1}, a_{T-1}, r_T} ~ \pi_\theta$ do
        <ul>
          <li>for $t = 1$ to $T - 1$ do
            <ul>
              <li>$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) v_t$</li>
            </ul>
          </li>
          <li>end for</li>
        </ul>
      </li>
      <li>end for</li>
    </ul>
  </li>
  <li>return $\theta$</li>
</ul>

<h4 id="actor-critic-policy-gradient">Actor Critic Policy Gradient</h4>
<p>The problem with Monte-Carlo Policy Gradient is that is has a very high variance. In order to reduce the variance, we can use a <strong>critic</strong> to estimate the action value function. Thus in <strong>Actor Critic Policy Gradient</strong>, we have two components:</p>
<ul>
  <li><em>Critic</em> updates action value function parameters $w$</li>
  <li><em>Actor</em> updates policy parameters $\theta$, in direction suggested by critic</li>
</ul>

<p>Here is an example when we use linear value function approximation for the critic:</p>
<ul>
  <li>Initialize $s$, $\theta$</li>
  <li>Sample $a ~ \pi_\theta$</li>
  <li>for each step do
    <ul>
      <li>Sample reward $r$, sample next state $s’$</li>
      <li>Sample action $a’ ~ \pi_\theta (s’, a’)$</li>
      <li>$\delta = r + \gamma Q_w(s’, a’) - Q_w(s, a)$ (This is the TD error)</li>
      <li>$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) Q_w(s, a)$ (We replace with the approximation)</li>
      <li>$w = w + \beta \delta \phi(s, a)$ (Update value function approximation model parameter)</li>
      <li>$a = a’$, $s = s’$</li>
    </ul>
  </li>
  <li>end for</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-09-10T00:00:00-07:00">September 10, 2017</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Reinforcement+Learning+Lesson+6%20http%3A%2F%2Flocalhost%3A4000%2FReinforcment-Learning-Lesson-6%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FReinforcment-Learning-Lesson-6%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2FReinforcment-Learning-Lesson-6%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2FReinforcment-Learning-Lesson-6%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="http://localhost:4000/Reinforcement-Learning-Lesson-5/" class="pagination--pager" title="Reinforcement Learning Lesson 5
">Previous</a>
    
    
      <a href="http://localhost:4000/Reinforcement-Learning-Lesson-7/" class="pagination--pager" title="Reinforcement Learning Lesson 7
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/What-I-Read-This-Week-I/" rel="permalink">What I Read This Week 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">The 3 Tricks That Made AlphaGo Zero Work
This post explains why AlphaGo Zero out-perform than it’s elder brother AlphaGo, summarizing in 3 points that lead t...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/2018-%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/" rel="permalink">2018 新年计划
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">2018年的主题，就是要变得自信以及学会Deep Work!

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/Deep-Work-Reading-Note/" rel="permalink">Deep Work Reading Note
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">“Deep Work” is the first book I read this year. I was pretty impressed by the idea and methods the author purposed to help you gain the ability to do “deep w...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/2017-%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/" rel="permalink">2017 年终终结
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">2017年一转眼就过去了，自己也从一个刚刚毕业的学生成了一个快上了两年班的上班族了。趁着年末好好回顾一下这一整年发生的事情，一方面是给自己留个纪念，另一方面也是好好总结一下这一年的得失，找到改进的方向。

</p>
  </article>
</div>
        
      </div>
    </div>
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Yang Pei. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="http://localhost:4000/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>








  </body>
</html>
