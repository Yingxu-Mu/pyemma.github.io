[ { "title": "How Workflow Get Scheduled via Plugins in Flyte", "url": "/Flyte-How-Workflow-Get-Scheduled/", "categories": "Distributed System", "tags": "open source, job scheduler", "date": "2024-09-12 00:00:00 -0700", "snippet": "Reading open source code has been a recommended approach for software engineers to learn. However, in my past 8 years career, I didn’t do a good job on that. After working in a startup for 1 year, ...", "content": "Reading open source code has been a recommended approach for software engineers to learn. However, in my past 8 years career, I didn’t do a good job on that. After working in a startup for 1 year, I accidentally foster the habit to read open source code XD. In this post, I would like to share one open source project I have been learning recently, and hope you would enjoy this journey as well.I have been working in ML pipelining for a long time in Meta Ads. However, I didn’t have a comprehensive understanding across the entire stack, especially on how the underlying infra schedule the model training job and execute it over a fleet of machines. Recently, I have been exposed to an open source project: Flyte, which is a orchestrator for ML pipeline built on top of Kubernetes. I think this might be a good opportunity for me to gain some deep understanding in this area.I have always been a believer of “Learning by Doing”. My ultimate goal on learning this open source code is to implement a simplified version of ML pipeline orchestrator on my own. Next, let’s see what problem we are going to discuss in this post.ProblemIn Flyte, we could use something called Plugin for distributed training, e.g. PyTorch Plugin. In this post, we would discuss how these plugins are getting invoked, so that the distributed training job we defined could get executed. In this post, I would simplify the discussion and only laser eye on the main flow, for other important topics such as storage, pipeline definition and compilation, availability and scalability, I plan to defer it to later posts.High level architectureThe key component that is responsible for scheduling and monitoring the workflow in Flyte is called FlytePropeller. It tries to push FlyteWorkflow, which is defined as a Custom Resource Definition in k8s, to the desired state leveraging k8s reconcile mechanism. The official document of Flyte has provided a pretty good high level architecture on FlytePropeller’s design, here is a list of the core components: Controller: overall brain of FlytePropeller WorkQueue/WorkerPoll: where worker lives and take jobs to do, a very classic design in job scheduling system WorkflowExecutor: responsible for high-level workflow operations, such as tracking the status of workflow NodeExecutor: responsible for process the node within the workflow and decide the action need to take NodeHandler: different type of handler to execute different type of node in the workflow, e.g. TaskHandler for execute Plugins and WorkflowHandler to execute embedded sub-workflowsKnowing what to do is one thing, and knowing how to do is another thing! Next, let’s jump into the code and see how these components are working internally and see how the logic defined within Plugin could be invoked.Components Deep DiveControllerLet’s get our journey starts with the controller. Controller is the starter for FlytePropeller, it is responsible for initializing other components: In the New function of controller, we would create workqueue here. And then, we would create workerpool here. Note that workerpool requires the workqueue we have created before as part of its initialization (because worker needs to consume the jobs from the queue), and one PropellerHandler notably, the PropellerHandler is initialized with WorkflowExecutor and the WorkflowExecutor is composed of NodeExecutor NodeExecutor requires a nodeHandlerFactory as part of the construction As of now, all the key components we have mentioned in the high level architecture is ready. We would go deeper into them to understand how are they getting invoked.Besides the New function, there is also a run function which plays a critical role on launching the controller. It launches things such as the workerpool, gc and metrics monitors. run function is called within another function Run, in Run, one interesting part is that it is going to leverage the leader election functionality provided by k8s and only let leader to trigger run function. We would discuss this topic more in details in a future post.As the controller would launch workerpool, let’s then move our view to workerpool and workqueue to understand how these 2 components work.WorkerPool/WorkQueueThe workerpool essentially is composed of workqueue and several workers, each are actually goroutines (this is also why Flyte could be pretty scalable on a single CPU, we would discuss this in the future). The Run function in workerpool is the most critical function, which is the one get invoked by controller. The main logic is the for loop here, where we launch multiple goroutines and each goroutine would make a call to runWorker function. The runWorker function is relatively simple, just an endless while loop to call processNextWorkItem function. processNextWorkItem function gets an item from the workqueue and then invokes the PropellerHandler we perviously passed in during initialization. As we could see, the key processing logic resides within PropellerHandler’s Handle function, which is defined as part of the interface here, then let’s move on and see how this Handle works.PropellerHandlerThe Handle function defined by the Propeller struct is the entry point of the reconcile process (Here Propeller has implemented the Handle interface, thus it could be considered as type Handler although there is no explicit inherit, this is how interface implementation works in Golang). The key logic is within this for loop, where we call streak function up to a max trial. The streak function would try to do a single mutation to workflow, and return the mutated workflow upon succeed, otherwise no update made if failed. The workflow here is the CRD FlyteWorkflow and the mutation operation is done via TryMutateWorkflow. TryMutateWorkflow makes calls to workflowExecutor’s HandleFlyteWorkflow function to see if we could reconcile the workflow towards it desired status. We left out other details, such as how to handle failure, how to handle aborted workflow etc. From the code in PropellerHandler, we could observer that the Handler is just doing some high-level logic and the actual workflow processing logic is delegated to workflowExecutor. Now, let’s move to workflowExecutor.WorkflowExecutorThe HandleFlyteWorkflow function called within PropellerHandler is a router function. It invokes other actual logic function based on the status of the workflow. For example, if the workflow status is in WorkflowPhaseRunning, then it would invoke handleRunningWorkflow function. In these functions, a common pattern is that they would setup the context, invoke nodeExecutor’s RecursiveNodeHandler function to get the new status and then update the status. The new status is passed back and used to transit the workflow’s status (which is the reconcile process). Notice that the FlyteWorkflow is passed as parameters for executors.DAGStructure and executors.NodeLookup, as well as the startNode.There is some different operation based on the new status RecursiveNodeHandler passed back. For example, if the new status is partial completed, then the workflow would be enqueue again and return running status.The WorkflowExecutor handles the operation of workflow and decided what action to take. In ML pipeline, we know that workflow is usually composed by several nodes, and these nodes encapsulate the actual computation. Let’s take a look at nodeExecutor, which is responsible for handling this part.NodeExecutorThe RecursiveNodeHandler function is one of the most important function in NodeExecutor. It is the entry point to execute a node within a workflow. It uses actor model and modified version of DFS to traverse the DAG and to execute non-blocked nodes. Based on different status queried based on the starter node passed from input, it applies different logic to proceed. For example, if the node status is already succeed, skipped or recovered, then it would invoke handleDownstream function; while if the node is in status that could be handled, then the key logic happens here: first, based on the node’s kind, a dedicated handler is retrieved from nodeHandlerFactory; then HandleNode function would be invoked to execute the node.The handleDownstream is where the aforementioned modified DFS implemented. The logic is relatively straightforward: starting from the input node, we retrieve all downstream nodes; then we iterate each node and invoke the RecursiveNodeHandler function on each of them, with self as the new input start node; keep the status to check if all downstream nodes have been processed, and return the status accordingly.The HandleNode function of nodeExecutor is also a router function, where different processing function is invoked based on the status of the current node. The most important functions are handleQueuedOrRunningNode and handleNotYetStartedNode: In the handleNotYetStartedNode, the most critic logic is the call to preExecute, where we check if the node could be queued to be further processed. The checking logic is relative simple, where we check the upstream nodes are all in succeed status or not In the handleQueuedOrRunningNode, we would first try to check if there are cached result given the current handler, and trigger the execute function if there is no cache hit. The core part of the execute function is to trigger the Handle function of the input NodeHandler, which is obtained from RecursiveNodeHandler and passed along the stack here, what a long journey!Now, we have hit the most underground part of FlytePropeller’s architecture. Next, we need to dive into NodeHandler to understand how the Handle function is implemented (here we would focus on how the handler used to fulfill the operations we need in distributed training).NodeHandlerFrom the section above, we know that we retrieve node handler from nodeHandlerFactory in RecursiveNodeHandler, through the GetHandler function. Here is a step by step explanation on how we trigger the logic defined within plugins: The GetHandler function returns node handler based on the type of the node. Most of training job is defined via @task, which is of Task type in Flyte Here is the setup of the node handler for Task type. In Flyte, all Task is treated as a dynamic node and handle through dynamic node handler. However, we would still pass a task node handler into dynamic node handler In dynamic node handler’s Handle function, by default, we would make a call to handleParentNode, and in this function, we would make a call to TaskNodeHandler interface’s Handle function The logic of task node handler’s Handle function is pretty complex. First, it tries to find the Plugin based on task type; then if there is no cache hit on result, it would invoke plugin Within invokePlugin function, the core part is to invoke the Handle function ResolvePlugin search plugins through pluginsForType, where we initialized within the Setup function; the initialization is essentially sweeping the enabledPlugins, and we get it from WranglePluginsAndGenerateFinalList In WranglePluginsAndGenerateFinalList function, we get all plugins related to k8s through PluginRegistryIface interface; in task node handler, there is a data member pluginRegistry of this type, and the construction is here, where we call the PluginRegistry function from pluginMachinery module For each k8s plugin, they would be wrapped within a PluginEntry, which is further wrapped in an object called NewPluginManagerWithBackOff All k8s plugin would use the RegisterK8sPlugin function within the module pluginmachinery.register to register them into the system. For example, the Pytorch Plugin is registered here. However, all of these plugins actual do not provide a Handle function, which should be called by the node handler. What happened? Actually, the Handle function is implemented within PluginManager. Since PluginManager implement all interface defined in pluginCore.plugin, we could treat PluginManager as a plugin to invoke (this is a class Strategy design pattern, where PluginManager defines the main logic and expressed via several step functions. And we could use composition to fulfill these step functions with different implementation) Within the Handle function in PluginManager, we would check the current status, if the status is not started, then we would call launchResource, otherwise we would call getResource and checkResourcePhase to obtain new transition information In launchResource function, we would call BuildResource function which is defined in the plugin. This function is used to construct a kubeflow job. Then it make a create request via kubeClient to create this resource In checkResourcePhase, we would call GetTaskPhase to get the current status of the job Here is the point where Flyte is leveraging kubeflow and k8s to request resource and start the training job; both kubeflow and k8s would be huge topics, and I plan to discuss more in details in separate blog Here, we reach the end of our journey and the remaining job is delegated to k8s. What a complex flow!SummaryIn this post, we focus our discussion on how Flyte would invoke the distributed training job which is defined through plugin, we could see some common practice that is adopted in the design, such as utilization of queue and multithreading for scalability; separation of workflow executor and node executor for single responsibility principle; factory design for extensibility, etc.In next topic, we would focus on the storage used in Flyte, which is also another critical component, as we need to store the status of the workflow, node and even intermediate result; as well as leveraging caching to speed up the execution by avoiding duplicated computation. Once we have a better understanding on the storage part, we could start to evaluate the availability, scalability and persistence of Flyte." }, { "title": "LLM Training 101", "url": "/LLM-LLM-Training-101/", "categories": "Distributed Training", "tags": "llm, llm training", "date": "2024-09-01 00:00:00 -0700", "snippet": "这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家...", "content": "这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备 PS: 后续会不定期的更新这篇 blog 来争取与时俱进，同时会有专栏来介绍这篇 blog 里面打算深入研究的项目概念性知识 LLM 训练的一些特点 模型架构的一致性，基本都是堆的 transformer, 虽然现在有一些不一样的尝试比如 Mamba 和 TTT, 但是主流的模型还是 transformer 训练的规模和时间也是空前绝后的 Specialized software, 比如 Megatron (这个听说过，去了解一下) LLM 训练的 pipeline 也发生了变化（这一点说的还蛮有道理，我在这个领域有比较多的经验，可以向这个 LLM 的方向研究一下看看有什么机会）。传统的机器学习都是针对某一个问题用对应的数据来训练（domain specific），但是现在 LLM 的主流是在大量的数据做自监督学习，然后再进行 fine-tuning, alignment 等 在 LLM 训练的各项因素之中，Communication overhead 是一个主要痛点 LLM 训练的 infrastructure 相关的内容 PCIe 由于 bandwidth 的问题导致其不是很合适 LLM 的训练，现在更多的是使用专用的链接比如 NVLink 等，同时能使用不同的网络连接拓扑结构来进行进一步的优化，比如 cube-mesh 或者 switch-based fullly-connected The Clos network architecture, commonly known as a Fat-Tree topology, is widely used in LLM training clusters. In a Closbased cluster, each server, equipped with one or more NICs, is organized into racks connected to leaf switches. These leaf switches link to spine switches, providing inter-rack connectivity and forming a pod. The pods are further interconnected with core switches, facilitating any-to-any communication across servers within the cluster. Parallel file systems such as Lustre, GPFS, and BeeGFS are frequently deployed on leading high performance computing systems to ensure efficient I/O, persistent storage, and scalable performance. 听说过 distributed file system, 但是这个 parallel file system 是啥 打算去学习了解的框架和技术 RDMA: 可以去学习了解一下 InfiniBand DeepSpeed-Chat, parallel strategy https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat uses Hybrid Engine to seamlessly switch model partitioning between training and inference, such as using tensor parallelism to improve throughput during inference and using ZeRO or LoRA to improve memory utilization during training, providing outstanding system efficiency for RLHF training HuggingFace TRL, parallel strategy https://huggingface.co/docs/trl/en/index make full use of various parameter-efficient fine-tuning (PEFT) methods, such as LoRA or QLoRA, to save memory cost, and use a dedicated kernel designed by unsloth to increase the training speed of RLHF. FlashAttention, 内存优化 https://github.com/Dao-AILab/flash-attention an IO-aware tiling algorithm is proposed to reduce the number of memory reads/writes between slow HBM and fast on-chip SRAM based on the online softmax. 看能不能自己实现一遍这个算法，网上应该有一些简化版的 kernel 教程，可以参考学习一下 Selective-checkpointing selectively discards the activations of memory-intensive attention modules. FlashAttention fuses the attention module into a single kernel, and also employs selective-checkpointing to reduce memory consumption. 这个看一下具体是怎么做的 FlashAttention 2: 内存优化, efficiently handles variable-length inputs by parallelizing the sequence length dimension inseparably 这个是怎么实现的，去学习一下代码 FlashAttention 3: 内存优化, An interleaved block-wise GEMM and softmax algorithm is redesigned based on FlashAttention-2 to hide the non-GEMM operations in softmax with the asynchronous WGMMA instructions for GEMM. Besides, by leveraging the asynchrony of the Tensor Cores and Tensor Memory Accelerator (TMA), overall computation is overlapped with data movement via a warp-specialized software pipelining scheme. Blockwise Parallel Transformer (BPT) further reduces the substantial memory requirements by extending the tiling algorithm in FlashAttention to fuse the feedforward network 需要学习了解一下 WGMMA, Tensor Cores, Tensor Memory Accelerator, Blockwise Parallel Transformer Triton, 用来写 kernel, 计算优化，听说现在很多公司内部在大量的使用这个写 Kernel, 可以学习一下 #kernel #CUDA https://github.com/triton-lang/triton ZERO, 通过 fully sharding 来进行内存优化, ZERO1, 2, 3 https://arxiv.org/pdf/1910.02054 ZeRO-3 employs per-parameter sharding to shard the full model and utilizes All-Gather and ReduceScatter for unsharding and sharding communication, respectively ZERO++ 感觉也算是 ZERO 家族的一员，但是是一种 partial sharding 的办法，在 ZERO3 的基础之上, further introduces a secondary shard of parameters within subgroups of GPUs and uses quantization to compress parameters and gradients, effectively diminishing communication volume with a trade-off in accuracy ZeRO-Offload concentrates on multi-GPU training. It holds model parameters on GPU, and stores optimizer states and gradients on CPU memory. In addition, it offloads optimizer update computation to the CPU. Ring AllReduce 算法: https://github.com/baidu-research/baidu-allreduce Horovod: replaced the Baidu ring-AllReduce implementation with NCCL and designed a user-friendly interface for distributed training Pytorch DPP: fuse multiple sequential AllReduce communication operations into a single operation. This method avoids transmitting a large number of small tensors over the network by waiting for a short period of time and then combining multiple gradients into one AllReduce operation during the backward phase. 通信优化的一种办法，可以看看代码学习一下 FSDP: https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html GPipe 是之前听说过的一种方法，貌似是目前比较流行的方法，但是仍然会在开始和结束的时候有大量的 bubble 出现 https://github.com/kakaobrain/torchgpipe 一些比较主流的和重要的概念Parallelism Strategy Tensor parallelism: partitions the parameter tensors of each layer along multiple dimensions, effectively distributing the model parameters across the available GPUs. 感觉 tensor parallelism 没有 data/model parallelism 那么常见，在工作中没怎么看到用这种方法的 it is challenging to overlap the communication with computation, necessitating the use of high-bandwidth connections. Consequently, tensor parallelism is more commonly employed in a single GPU node. Pipeline parallelism: pipeline parallelism only necessitates the exchange of intermediate tensors at designated cutting points, resulting in less frequent communication requirements, pipeline parallelism 算是比较常用的东西了 但是 pipeline parallelism 也有两个问题，一个是 pipeline bubble, 一个是 memory consumption imbalance Sequence parallelism: It divides the input data into multiple chunks along the sequence dimension and each chunk is fed to one GPU for computation. 没怎么听说过这种方法，可以找来一些 code 来学习一下 MQA 和 GQA 就是属于这个范畴, 可以好好的学习一下 Ring Self-Attention leverages sequence parallelism and calculates the self-attention with ring-style communication to scale up the context window of LLM training. It first transmits the key tensors among GPUs to calculate the attention scores in a circular fashion, and then calculates the self-attention output based on the attention scores and value tensors transmitted in a similar fashion MoE parallelism: MoE 的结构在目前主流的 LLM 里面都得到了大量的使用，可以看看下面的这几篇文章里面介绍的针对 MOE 的 parallel strategy 的方法 #MOE GShard: extends the idea of MoE to Transformers in distributed settings, where experts are distributed across different workers and collaborates with All-to-All communication DeepSpeed-MOE: proposes a new distributed MoE architecture that applies shared experts in each worker and places more experts in deeper layers to balance communication costs with training accuracy Since General Matrix Multiplications (GeMMs) require the size of all experts’ inputs to be consistent, existing MoE training frameworks often perform token dropping and padding to match the same expert capacity, which wastes computation. General Matrix Multiplications (GeMMs) 的工作原理可以参考: https://spatial-lang.org/gemm Token dropping and padding 的常用方法是什么？有没有具体的实现代码样例 针对 MOE parallel strategy 中 communication 的优化 Tutel: divides the input tensors into groups along the expert capacity dimension and overlaps computation and communication among different groups to hide All-to-All overhead Tutel: optimizes the All-to-All kernel implementation by aggregating small messages into a single large chunk inside the nodes before exchanging data among different nodes #Batching Lina analyzes the All-to-All overhead of MoE during distributed training and inference systematically and finds that All-to-All latency is prolonged when it overlaps with AllReduce operations. Lina proposes prioritizing All-to-All over AllReduce to improve its bandwidth and reduce its blocking period in distributed training 很有意思的发现，可以去学习一下原文里面是怎么发现这个问题的，然后应用在自己以后的工作中 This heterogeneity is also reflected in model architectures, particularly with Reinforcement Learning from Human Feedback (RLHF). Utilizing heterogeneous hardware and diverse model architectures has become essential for the efficient training of LLMs 再重新学习一下 RLHF，来理解这里面提到的 异构性 的特点 Memory Optimization Rabe 这篇论文中证明了自注意力只需要 O(logn) 的内存就可以了，学习一下这篇论文里面的工作 了解一下 FP16 和 BF16 的工作原理，内存优化 LLM training 的过程中主要吃内存的部分 Model States: Model states encompass the memory consumed by the optimizer states, gradients, and model parameters Activations refer to the tensors generated during the forward pass Temporary Buffers: Temporary buffers are used to store intermediate results Memory Fragmentation: Memory fragmentation can lead to scenarios where memory requests fail despite having a large amount of available memory, 这个在 Pytorch 里面由于内存分配机制会出现这种问题，可以再找一些额外的资料详细的了解一下 Deep learning frameworks typically use a caching allocator with a memory pool to enable fast memory allocation and deallocation without requiring device synchronization. 一个用来估算所需要的内存的简易办法 When training a model with Φ parameters,4Φ bytes are needed to store parameters and gradients. The 32-bit copies of the parameters, momentum, and variance each require 4Φ bytes, totaling12Φ bytes. Therefore, the overall memory requirement for storing model states is 16Φ bytes，这个再好好看一下理解一下 一些用来进行 Memory 优化的整体大方向 Activation re-computation strategies, which trade increased computation for reduced memory usage, 这个是现在最主流的方法之一，可以找一些代码来看看是如何实现的，这个方法的一个关键就是节省的内存和额外计算之间的 trade off Redundancy reduction methods that minimize data duplication across training processes Defragmentation techniques that optimize memory allocation and deallocation to reduce fragmentation and improve memory utilization GMLake and PyTorch expandable segments propose to mitigate fragmentation by utilizing the virtual memory management (VMM) functions of the low-level CUDA driver application programming interface. 可以看看 PyTorch 里面这个工作 Swap and offload approaches that leverage CPU memory and NVMe SSDs to supplement GPU memory CPU offloading: static/dynamic SSD offloading, 这个在之前的 GPU training paper 里面好像看到过 Communication Optimization一些和通信相关的优化 NVIDIA’s NCCL and AMD’s RCCL are highly optimized libraries that typically outperform MPI-based collective communication libraries on their respective AI accelerators. These libraries usually select pre-defined algorithms to perform collectives based on conditions such as network topology and input tensor size. 可以去学习一下 NCCL 通信的不同算法: Ring, Tree, Hybrid Conventional frameworks simultaneously perform gradient computation for both weights and outputs. Out-of-order backpropagation (ooo-backprop) decouples the gradient computations for weights and outputs, scheduling the weight gradient computations flexibly out of their original order. This allows more critical computations to be prioritized and scheduled accordingly. Consequently, ooo-backprop optimizes overall performance by scheduling communications based on this out-of-order computation strategy. 这个工作很有意思，把 activation 和 gradient 的 communication 拆开然后进行类似不同的 priority 的 communication In-network aggregation (INA) uses the computational capabilities of network devices to perform aggregation operations like summing gradients of deep learning models.Fault ToleranceFailure tolerance 主流的还是使用 checkpoint Synchronous checkpoint Check-N-Run decouples the snapshot and persist phases. It achieves atomic checkpointing by stalling training only during the snapshot phase and asynchronously persisting snapshots using dedicated background CPU processes. DeepFreeze applies both lightweight (snapshot) and heavy(persist) persistence strategies in the background, sharding checkpoints across data-parallel GPUs to distribute I/O workload. Gemini proposes checkpointing to CPU memory for faster failure recovery, along with a checkpoint placement strategy to minimize checkpoint loss and a traffic scheduling algorithm to reduce interference with training. Tectonic: Meta’s distributed filesystem, enables thousands of GPUs to save and load model checkpoints simultaneously, providing efficient and scalable storage solutions for extensive training operations 现在貌似主要用来对 checkpoint 用来存储的都是 object store, 这个可以去研究下看看各个公司都用啥（比如 AWS 是不是都上 S3） Live migration leverages the inherent redundancy present in distributed LLM training setups, particularly the model replicas across different data parallel pipelines, to restore model states in case of failure. 这个感觉其实有点类似使用 Cassandra 里 consistency hashing 里面的 hinted hand offIf you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee" }, { "title": "读书笔记 - Patterns of Distributed System", "url": "/Book-Pattern-of-Distributed-System/", "categories": "Distributed System", "tags": "system design, reading, 读书笔记", "date": "2024-07-05 00:00:00 -0700", "snippet": "最近读了一本和 distributed system 相关的书籍，介绍了在 distributed system 里面常用的一些 pattern. 这是一篇简要的读书笔记，把书中提到的几个 pattern 总结了下来; 我计划会经常更新这篇 blog, 把我新学习到的或者总结出来的一些 pattern 记录在这里; 希望能起到一个引导性的作用，给大家提供一个提纲挈领的思路PatternsWr...", "content": "最近读了一本和 distributed system 相关的书籍，介绍了在 distributed system 里面常用的一些 pattern. 这是一篇简要的读书笔记，把书中提到的几个 pattern 总结了下来; 我计划会经常更新这篇 blog, 把我新学习到的或者总结出来的一些 pattern 记录在这里; 希望能起到一个引导性的作用，给大家提供一个提纲挈领的思路PatternsWrite Ahead Log把命令存储到一个 append only file 里面去，当挂了之后可以重新读 WAL 来 rebuild 内部的 state #Message-Queue #KV-Store #持久化 Flushing 来保证命令真的写到 physical media，好处是 persistent，代价就是 performance; 可以使用 batching 等方法来进行优化 #Batching CRC record 来防止 corrupted entry #CRC Log 里面可能有 duplication，每一个 request 需要一个 unique identifier 来进行区分 #Deduplication 可以用来实现 transaction，用来保证原子性 #Transaction 工业界里面的具体例子 #RocksDB #Kafka #Cassandra Key/Value pairs that needs atomic store, write into a batch, and then batch is add into data store; the data store first create a WAL entry for the entire batch, once log is created successfully, the batch is added into datastore Segmented Log把单一的 log file 切分成更多的 log 从而方便对老的数据进行 cleanup; 当数据超过一定的阈值之后就 rollover 到一个新的 log file 里面去, 业界的例子 #Kafka #Cassandra #RaftLow Water Mark帮助保证 log 的大小不会无限制的增长，通过 low water mark 这样的一个 index，对 log 进行压缩 (通常是一个 background job 在进行这个操作) Snapshot-based #Raft Time-based #KafkaLeader and Follower使用单一的 server 来 coordinate 多个 servers 的 replication #Replication Small cluster: leader election, #Zab #Raft Large cluster: consistent core, 需要的几个核心功能 #Zookeeper #etcd compareAndSwap to set a key atomically heartBeat to expire the key if no heartBeat from leader, and trigger new election notification mechanism to notify all servers if key is expired Heartbeat 可以使用 separated thread 来异步发送 heartbeats #Consul 在 large cluster 里面，1-to-1 的 heartbeat messaging 效率太低了，这个时候一般可以考虑使用 Gossip Protocol #Gossip-Protocol 两种主流的实现方式，Phi Accrual failure detector 和 SWIM #Cassandra #Consul Majoruty QuorumFlexible quorum, 我们可以通过动态的调整读写的 quorum size 来提高性能，只要能保证读写之间会有一个交集就行; 比如说一共有 5 个 node，然后我们有 90% 的读和 10% 的写，那么我们可以要求读只需要 2 个 quorum, 写需要 4 个 quorum #Quorum #CassandraGeneration Clock也可以叫做 Term, Epoch, 这个是 Lamport Clock 的一个具体样例 #Lamport-Clock Each process maintains an integer counter, which is incremented after every action the process performs. Each process also sends this integer to other processes along with the messages processes exchange. The process receiving the message sets its integer counter by choosing the maximum between its own counter and the integer value of the message. This way, any process can figure out which action happened before the other by comparing the associated integers. The comparison is possible for actions across multiple processes as well, if the messages were exchanged between the processes. Actions which can be compared this way are said to be causally related. 工业界的例子, Cassandra 里面的 server 在 restart 的时候会自增 1, 这样在 gossip 的 message 里面其他的 server 会知道这个 server restart 了，从而会把关于这个 server 的 stale 的 data drop 掉，然后要新的; Kafka 里面的 epoch number 会存在 Zookeeper 里面，每次一个新的 controller 被 elect 的时候，就会增加这个 epoch number; 同时 leader 也会 maintain 一个 Leader Epoch 来看是否有 follower 太落后了 #Cassandra #Kafka High Water Mark也被称作是 CommitIndex #Replication #Raft #Kafka Client 最多只能读到这里，因为在 high water mark 之后的 entry 都还没有被 confirm 已经 replicate 了 这个在 stream 里面处理 delayed event 时候也叫这个，只不过那个 high water mark 是多等一段时间Paxos这个太难了，等以后专门开一个总结一下吧 #Paxos #Consensus-Algorithm #2PC #Quorum #Lamport-Clock We can ensure liveness or safety, but not both. Paxos ensure safety first 工业界的具体应用: Google Spanner 使用的是 multi-paxos, which is implemented as a replicated log; Cassandra uses basic Paxos to implement lightweight transactions #Spanner #CassandraReplication Log 在 MongoDB 中，每一个 partition 会有一个自己的 replication log #MongoDB #Partition 在 Kafka 的 Raft 实现中，使用的是 pull 模式，也就是 follower 从 leader 那里 pull replication log #Kafka #Push-Pull Read request optimization via bypassing the replication log, 可以使用两种不同的方法, 一个是 leader 再发送一个 heartbeat 然后看能不能得到 majority 的回复，来确认自己仍然是 leader; 另一个是使用 leader lease #Read-Optimization #Lease #etcdIdempotent Receiverclient 可能会 retry request, server 端需要进行 deduplication, 这个在多种系统中都很常见 #Event-Aggregation #Payment 给每个 client 一个 unique id, 在 server 端进行注册，注册之后 client 才能开始给 server 发送 request; 这个数据也需要被 replicated 从而保证高可用性 Expiration of saved request, request number, next request only when received response, number of max in-flight request with request pipeline #KafkaSingular Update Queue一种用来高效处理 concurrent request 的方法，向比较使用 lock 的话效率更高；具体的实现方法就是实现一个 work queue, concurrent 的 request 都放到 queue 里面，但是只有一个 worker thread 来处理 queue，从而实现 one-at-a-time 的保证 #Concurrency #Message-Queue #Coordination 工业界的例子有 Zookeeper, etcd, Cassandra 可能会用到这个思想的 system design: Booking, Google doc (OT)Request Waiting List一个 node 可能要和其他的 node 进行异步的 communication 之后才能返回 request, 保存一个 waiting list 来 map 一个 key 和一个 callback function #Concurrency #异步 工业界例子: Kafka 里面的 purgatory 来保存 pending request #KafkaFollower Reads也就是大名鼎鼎的 read replica; 即使是在用 Raft 这种 consensus 算法来进行 replication 的系统中也会有 replication lag, 因为 leader 需要一个 additional 的 network call 来让所有的 follower 都 commit; read your own write，可以使用 lampart lock 来解决，写了之后传回去一个 version number，再读的时候要带着这个 version number 来看 read replica 上面的 value 是不是已经是更新的了Version NumberTo store versioned key values, a data structure that allows quick navigation to the nearest matching version is used, such as a skip list, 之前在 lucene 里面也看到了这个 skip list，需要研究一下 #数据结构在 RocksDB 里面，一个重要的原因需要把 key sorted 的是因为它们 underlaying 存储的都是 bytes array, its important to keep keys sorted when they are serialized into byte arraysVersion Vector在 Cassandra 里面，除了 value 以外，还把 timestamp 也当做一个 column 来存储了，从而实现了 LWW，但是代价就是 Cassandra 的 cluster 需要正确的设置 NTP, 否则的话 latest value 仍然可能被 old value 给 overwrite 掉 如果每一个 cluster client 有一个 unique id 的话，那么我们也可以使用 client id 来存 version vector (但是这样的话怎么进行 conflict resolve 呢) 一篇 Riak 里面讲针对使用 client id 还是使用 server id 来存储 version vector 的文章Fixed Partition先 create logic shard，然后再把 logic shard map 到 physical shard 上面去; 这些 metadata 都可以通过一个 coordination service 来负责 (分 partition 和 存储相应的 metadata); 另外一种做法是每个 physical node 上面的 partition 数量是固定的，也就是 propositional to number of nodes Kafka 里面的每一个 topic 就是一个 fixed size partitionsClock Bound WaitWhile reading or writing, cluster node wait until the clock values on every node in the cluster are guaranteed to be above the timestamp assigned to the value Google TrueTime, AWS Time Sync Service, 使用 atomic clock 和 GPS 来确保 clock drift across their cluster node is kept below a few milliseconds 这个概念有点复杂，需要再找一个好的资料学习理解一下这里的思想LeaseUse time-bound lease for cluster nodes to coordinate their activities, 这个在 GFS 里面就使用到了, 在 Facebook 的 Memcache 里面也有涉及到 Lease 的思想, Lease 一般可以通过一个 coordination core 来实现，由 leader 来进行 lease 的 replication 和 checkState Watch可以参考一下是怎么实现的, 在 server 端我们需要存储下来 event 和 client 的 connection, 在 client 端我们要存储 event 和对应的 handlerEmergent Leader直接用在整个 cluster 里面最老的那个 node 作为 coordinator node, 相比较 consistency core 所采用的 leader election 的方法， favor availability over consistencySingle Socket Channel在 follower 和 leader 之间保持一个能够支持 retry 而且能够保证 message order 的通讯，可以通过 TCP 来实现; 在 Kafka 和 Zookeeper 里面使用了这种方式 #KafkaRequest Batch把多个 request 放在一起从而提高带宽的利用率; 在 client 端可以 maintain 一个 queue 来维护 request, 然后再放在一个 batch 里面一起发过去 (这个其实跟之前写的 batch commit logger 是一样的)Request Pipelineserver 在发出去 request 之后不需要等待 response, 又另外一个 thread 来负责接受和处理 response (有点像是 webhook 的思路); 为了防止 request overwhelming, 一般会有一个 upper bound on max in-flight request; 同时针对 retry 和 out-of-order 的 request 也需要针对性的处理 (比如 assign unique request id 等)Reference Patterns of distributed system" }, { "title": "那些年，我们追过的 Feature", "url": "/Features-in-Recommendation-System/", "categories": "Machine Learning", "tags": "machine learning design, feature", "date": "2024-06-24 00:00:00 -0700", "snippet": "在今天的 blog 里面，我将结合我前一阵子面试 machine learning engineering 的经验，跟大家唠唠在 ML design 里面的 feature engineering 相关的问题。在 ML design 里面，我们可能会被问到可以使用什么样的 feature, 以及具体一些 feature 可以被怎么处理以及怎么使用在模型之中。由于我之前主要做的是推荐和广告相关...", "content": "在今天的 blog 里面，我将结合我前一阵子面试 machine learning engineering 的经验，跟大家唠唠在 ML design 里面的 feature engineering 相关的问题。在 ML design 里面，我们可能会被问到可以使用什么样的 feature, 以及具体一些 feature 可以被怎么处理以及怎么使用在模型之中。由于我之前主要做的是推荐和广告相关的内容，所以我在这里将主要介绍一下在设计推荐系统的时候，围绕 feature 可以聊的一些点，来给大家提供一些思路，帮大家更好的准备面试Feature 的种类在推荐系统里面，我们经常要处理的场景是 给定一个用户和一个物品（以及一些可能的 context），预测用户会喜欢（或者其他的 action）这个物品的概率基于上面这个简单化的概括（我们在之后会对这个问题进行适当的展开），我们其实可以比较容易的归纳出 feature 的种类 User Side Feature Item Side Feature User-item Interaction Feature Context Feature除此之外，还有一些相对比较特殊的 feature, 我们之后会单独介绍User Side Feature这种类型的 feature 有时候也会被称作 request level feature, 针对推荐系统来说，一个 request 基本上就代表了一次用户请求，所以这么叫没什么毛病。在 user side feature 里面，有下面几种比较常见的形式 Demographic feature, 也就是常说的一些基本用户属性，比如用户的年龄，性别，职业，local 等等; 这些 feature 是最最常用的 feature 了，不过由于现在针对 Machine Learning fairness 查的严，很多这种 demographic feature 被禁止用于推荐了 User behavior feature, 也就是用户的一些行为特征，比如用户过去一个月买过的物品 id，用户过去一周 follow 别人的 account 的个数，用户的历史 CTR 等等 这里其实列举了三种不同的“数据类型”，也是在推荐系统中 feature 的常见形式: id feature (比如用户 like 过的 post id, 用户之前 follow 过的用户的 id 等等), count feature (比如 follow 的个数，click 的个数等等), ratio feature (比如 CTR, CVR 等等) 一般针对 user behavior feature, 我们都会有一个窗口来进行 aggregation (比如过去一周的 sum 或者 average; 在工程实现的时候，经常是多个窗口一起实现了，比如 7D, 14D 等，因为可以通过一个 pipeline 就全部搞出来，比较方便高效); 随着模型技术的进步，现在有很多尝试开始用 user sequential behavior modeling 来取代这种 feature engineering 的方法了, 比如 DIN 直接拿用户的 behavior 数据和 candidate item 做 attention 处理 User 的 behavior 多种多样，这部分需要针对具体的推荐系统所解决的 business problem 来进行处理，是最需要 domain knowledge 的部分; 比如在给用户推荐其他用户来 drive growth 的推荐系统中，你用用户过去点击过的 post id 可能效果就差一些，但是如果用点击过的 post 的 author id 可能就是一个不错的选择 User embedding, 或者叫 user profiling, 也是现在非常主流的一种 user feature, 当在面试中提到这部分的时候，基本上一个 follow up 是如何计算出来 user embedding, 以及如何在 inference 的时候进行 serving, 一般的 user embedding 的计算方法有这么几种: 直接用 item 本身，或者通过一些 heuristic rule 来表示: 比如说，我们可以把所有的 post 分类成 1024 个 category, 那么根据用户曾经看过的 post, 我们就可以用一个 1024 维度的向量来表示用户, 如果用户看过 GUNDAM 类型的 post, 那么 GUNDAM 类别在 1024 向量中的值就设置成 1, 否则就是 0; 这种方法其实非常类似 NLP 里面以前经常使用的 bag of words 的方法来做 sentence embedding; 这种方法的好处就是实现起来比较简单，但是缺点就在于捕捉到的信息比较粗糙，只能根据我们定义的 heuristic rule 来决定，不能根据用户跟物品的交互学习出一些 semantic 的信息（比如喜欢 GUNDAM 的用户可能对于游戏也感兴趣） 一个这对上面的方法提高的方法，是基于 item 的 embedding 去学习一个 user embedding, 这个方法在 Pinterest 的这篇论文中得到利用，他们基于 PinSage 出来的 item embedding, 结合用户的 sequence engagement 数据，利用一个模型来学习 user embedding 通过其他的一些方法来训练出 user embedding, 比如利用全体用户的 interest follow graph 作为数据，利用 collaborative filtering, dedicated model (比如 graph neural network) 来计算出 user embedding, 然后这个 embedding 可以被用作输入送进其他的模型; 比如 Facebook 就采取了用一个专门的模型来对大量的 user feature 进行学习，然后用这个 dedicated model 去生成 user embedding 然后给下游的模型使用，详情可以参考这篇论文; 这种方法的好处在于能够学习更具有表征能力的 user embedding, 但是缺点在于 user embedding 需要进行单独的额外维护，以及不能很好的基于 downstream 的 task 来进行微调; 将用户 id 作为一个 spares feature 输入到模型里面，然后通过一个 embedding lookup table 来 convert 成一个 dense vector, 然后再和其他的 feature 输入 concat 在一起; 这个就是一个经典的如何在 Neural Network 里面使用 sparse feature 的方法，比如 LinkedIn 的这篇 blog; 这种方法的一个好处是在于 user embedding 和模型是一起训练的，所以 embedding 可以理解为针对某个问题的专门优化，那么代价也显而易见，一反面是模型训练的参数量上升，另一方面对于 user embedding 的 reuse 也不是那么的方便了 Item Side FeatureItem side feature 其实和 user side feature 本质上是类似的，比如我们也可以有一些 item 的属性信息，比如物品, 产地，物品价格等等，或者 item 的一些历史交互数据，比如过去一周的 impression 数量，过于一周的 CTR 等等；同时我们也可以使用和 user embedding 类似的方法来生成 item embedding 然后用作模型输入; Item side feature 的一个特点是 feature value 变化不是很频繁，因此这部分的 feature 一般都会采用一些类似 pre-compute 的方法来提高性能 一种常见的方法是使用双塔模型来对 user 和 item embedding 进行训练，然后 user tower 可以在线的去 server request 来实时的计算 user embedding, 但是 item tower 可以离线的把所有的 candidate 的 embedding 计算好然后缓存起来，等需要的时候直接读 pre-compute 好的而不用再实时的计算了 相比较 user feature, item 可能也会有一些其他形式的数据可以利用，比如物品的图片，广告语等等，这些数据都可以经过专门的处理然后用作模型输入，比如利用 pre-train model 来把 image 转换成 embedding; 或者利用 object detection 模型来 predict 图片中包含物品的种类信息; 利用 BERT 来做 sentence embedding 等等Item side feature 比较特殊的一种 feature，是可以利用一些 Owner 的信息，而不单单是物品本身的信息: 比如说我们要推荐广告，那么我们可以拿 campaign level 或者 advertiser level 的信息(这些信息相比较 ads id 本身可能会更加的 stable, 这个是 ads 里面的一个问题，因为 ads 本身的 TTL 都相对比较短); 如果我们推荐 post, 那么我们可以拿 post owner 的一些信息，比如 post owner 过去一周看过的 post id 等，作为 additional signal 来做推荐User-item Interaction FeatureUser-item interaction feature 就是具体到 user-item pair 的 signal。 在我们上面所描述的 feature 中，都是根据 user 或者 item 进行了 aggregation; user-item pair 可以理解为是 aggregation 之前的信息，能够给我们提供最 fine-granularity 的 signal， 比如，我们可以看 user_a 跟 item_a 过去 24 小时的 view count, time spent 等等; 或者 user_a 跟广告商 advertiser_a 过去 24 个小时的 impression 的数量, CTR 等等这一类的 feature 的 serving 是最困难的，原因就在于其 cardinality 非常的大（约等于 O(num of user) x O(num of item) 的数量级），因此我们很少在 early stage (这里其实涉及到了推荐系统里面的多层架构设计，可以理解为我们在层层做 filtering 从而来减少需要考虑的 candidate 的数量) 大量的使用这种类型的 featureContext Feature这种类型的 feature 相对于前三种讨论到的而言不是那么的常见，context feature 有两种不同的理解形式 一种是在 server 端我们能够拿到的一些 context feature，比如现在很多的推荐系统开始 adopt list ranking, 也就是考虑所推荐的物品彼此之间的影响，把彼此当做 context 来进行优化（彼此都是彼此的缺口 XD） 另外一种是在 server 端无法获取，只有在 device 上才能拿到的一些信息，比如用户当前的网络环境， 用户当前手机的电量等等，这些 context feature 也会对用户的行为产生影响，比如如果我的手机没什么电了，我可能会先收藏几个视频等之后再看 针对上面说的这两种情况，在快手的这篇论文里面都有提及，详情可以参考阅读一下原文其他 Feature 的碎碎念除了上面提到的 feature, 还有一些比较特殊的 feature 承担着一些特殊的使命 Privileged feature, 这种类型的 feature 是属于在 training 的时候 available 但是在 serving 的时候不 available 的，一个例子是在做 CVR 的预测的时候，用户在一个物品上面的逗留时间会是非常 powerful 的一个 signal, 但是我们在给用户展示物品的时候，是需要先预测 CVR 出来对物品排序，然后再给用户展示，所以在 serving 的时候这个逗留时间是不知道的，但是在 training 的时候我们可以根据 client 端的 logging 知道这个信息。在淘宝的这篇论文中，他们采用了 teacher-student knowledge distillation 的方法来学习这类 feature Position feature, 这个是在推荐系统中非常常考的一个 follow up. 在推荐系统中，存在这一种系统性的 bias, 也就是 position bias: 用户点击了某个物品，可能并不是因为我们的推荐做的有多么好，而是单纯的因为这个物品被排在了前面，导致有更多的 impression 引来更多的用户 action. 传统的解决办法是把物品的 position 当做一个 feature 放在模型中跟其他的 feature 一起训练学习，从而能够让模型学习出 position bias, 从而起到一定的 calibration 的作用; 在 Youtube 的 recommend next video to watch 的论文中，position feature 被单独的加到模型的一个 tower 里面来进行专门的学习，从而提高模型针对 position bias 的解决能力常见的 Feature 处理办法上面的章节我们主要说了一些不同类型的 feature, 在介绍的时候我们也稍微提及了一下 feature 的不同的数据类型，这里我们再稍微复盘一下 numeric feature, 我们也俗称 dense feature; 这里面也细分成两类，一类就是最普通的 float number, 比如 ratio/count 等等；另一类是 categorical feature, 比如性别，国别等等。这两类 feature 最主要的一个区别在于他们的 scale 是否具有意义 id feature, 我们也俗称 spares feature; 主要就是一些 entity id, 比如 post id, ads id 等等；具体的表示情况也有两种，一种就是单独的一个 id list, 另外一种则是给不同的 id 一个 weight, 比如说我们可以根据用户跟这个 entity 交互的时间来做一个 weight decay, 从而实现一种简单的 recency 性质的 sparse feature, 让更近的 id 有更高的 weight, 一般我们管这个叫做 id score list; 所有的这种 id feature 都需要通过一个 lookup table 来转化成 dense vector embedding feature, 一般就是其他模型输出的一些 dense vector; 一般整体直接使用，很少有只使用其中的某一些维度的情况在这几种类型的 feature 当中, id feature 和 embedding feature 的处理办法一般都比较的统一，比如使用 lookup table 来转化 spares feature 到 dense format, 或者使用一些 clipping 来防止 embedding 里面的某些值过大; 针对 numeric feature 的处理比较常见，我们下面就主要聊一聊这些Normalization这个是针对 numeric feature 来说最常见的一种处理方式了，主要的目的就是让所有的 numeric feature value 的 scale 尽可能是统一的, 比如 ratio feature 的 scale 一般都是 0 ~ 1, 但是房价这个 feature 的 value 就可能 scale 在 0 ~ 10M, 这样会让 ratio feature 的 weight 跟房价 feature 的 weight 不在同一个数量级上，从而导致了 “vanish” 情况的发生。针对这种情况，一般我们可以采用 min-max scale 的方式来 normlize另外的一种情况，是 feature 本身的 distribution 是 highly skew 的，在 ML 里面，我们都是尽可能希望 feature 的分布尽可能的接近正态分布（为什么呢）。针对这种 data skewness 的情况，我们可以采用一些 transformation 比如 log-transform 或者 cox-box transform 来进行处理，从而让 value 尽可能符合正态分布One-hot/multi-hot Encoding这种处理办法是 categorical feature 的一种常见处理办法。在这种方法中，针对 categorical feature，我们会把它展开成一个 sparse vector，里面只有一个或者几个 element 是 1, 其他的位置上全都是 0。One-hot 和 multi-hot 的主要区别在于在这个 sparse vector 里面能有几个位置上有 1，one-hot 只有 1 个，而 multi-hot 可以有多个Feature transformation除了上面的处理办法，针对 feature 还有一些额外的变换，从而增加模型能够学习的 signal 的数量 在 Youtube 的这篇论文中，他们把一些 feature 用上了 sqrt, pow 等变换，作为新的 feature 和原本的 feature 一起送入模型进行学习 我们还可以把不同的 feature 彼此之间做 bi-gram，从而显形的增加 feature interaction (2nd order interaction, 这个也是之前很多研究的一个重点，因此退出了诸如 DCN 等模型架构，不过在这些模型里面， feature interaction 是通过模型表征，而不是在输入上面做文章) 另外一种比较常见的做 feature transformation 的办法是给原本的 feature 添加 breakdown, 比如说我们算一类 item 的 CTR 数据，那么可以根据用户的性别添加一个 breakdown (说个题外话，这种 breakdown 现在也被用于处理用户数据上，从而实现 privacy preserve); 更加放飞自我的一种办法，我们可以先 train 一个 gbdt, 然后把 gbdt 的 leave node 作为一个 feature 输入模型（可以考虑作为一个 binary sparse vector，或者带着 leave node 上的 weight）; 在 Facebook 的早期 ads 模型里面，大量的使用了这种方法，详情参考这篇论文 We found that boosted decision trees are a powerful and very convenient way to implement non-linear and tuple transformations of the kind we just described. 针对这一点，我在和 reddit 的 ads ranking 面试的时候，跟 hiring manager 有一个比较激烈的讨论（可能也是因为这样她把我拒了把吧）; HM 一开始没有很理解使用 gbdt 来做 feature transformation 的思路，认为这不可行；在被我 convenience 这样做可行之后，又抛出来认为这样做没必要，因为可以调整模型架构来直接学习 tree 本身所 capture 到的一些复杂的 feature interaction; 针对这一点我是同意的，而且我也有幸参加到了 ads ranking 的 gbdt deprecation 的工作之中; 不够我们当时之所以 deprecate gbdt 更多的是从 maintenance cost 和 simplify tech stack 的角度，gbdt 本身仍然能够带来很不错的 model gain, 这也是为什么 deprecation 整了很长时间也没有完全取得胜利 Missing/Sentinel Value如何处理 feature 的 missing value 或者 sentinel value 也是作为 machine learning engineer 我们经常要实际面对的问题。针对 missing value，大部分的时候我们会直接用 0 来取代，但是如果 0 也是一个合法值的话，会给模型造成困扰; 这个使用可以考虑使用 sentinel value 来表示 value 是否 missing, 把这个 sentinel value 做成一个新的 categorial feature 用在模型里面; 这样一来，原本的 feature 和这个新的 categorical feature 结合着使用，模型就能区分开来 feature value 是真的等于 0 还是单纯的 missingFeature Optimization这一部分一般属于 bouns point, 在实际的面试中，很少有机会会真的讨论到这里，我之前在 Facebook 的时候带领很多这方面的项目，因此也借着这个机会给大家简单的介绍一下Feature Importance当我们有大量的 feature 之后，一个头痛的问题就是我们要选用哪些 feature 进入到模型里面; 由于 infra 的一些 limitation, 把所有的 feature 都放入到模型中开销过于巨大，无法支持，因此一般我们会选择一个 subset feature 来放到 production 模型中，而选取 feature 的一个重要指标，就是 feature importance 这里插入一些题外话，除了由于 infra limitation 导致我们不能使用所有的 feature 以外，另外一个考虑的因素是 feature coorelation，我们通过 empirical 的研究发现，把 correlated feature 放入到模型里面不会给模型带来任何效果上的提升，而且反而可能会降低模型的效果；不过 correlated feature 倒是有助于提高模型的 robustness，所以这也是一个实际中我们需要 make 的 trade off; 在 Facebook ads 我们是尽可能 minimize feature correlation，从而把尽可能多的有 additional gain 的 signal 放入到模型中Feature importance 可以从算法层面和工程层面两个角度讨论，工程层面的话主要就是涉及到各种 feature metadata 的管理，如何保证 feature candidate pool 是正确的（在复杂的系统中，feature serving 也是一个被高度优化的部分，导致有些 feature 只有在特定的环境下才可用），如何保证 feature 有足够的 coverage 等等；从算法层面，可以用来计算 feature importance 的方法有: shuffling algorithm, SHAP, integrated gradient 以及 binary stochastic neuron 等，这里就不再展开讨论了当通过上面提到的方法得到 feature importance score 之后，我们就可以使用 top k 的方法来选取 feature，除了这种简单的 selection strategy 之外，我们还可以引入其他的一些 metrics，比如 feature serving cost, feature storage cost 等等来进行 joint optimizationHash Size Tuning我们在前讨论 id feature 的时候，大量的提到了 embedding lookup table 这样一个重要的 component. 它有两个很重要的 hyper parameter，embedding dim 和 cardinality, 可以理解为 embedding lookup table 的的行数和列数. 由于 id space 巨大，如果给每一个 id 都提供一个单独的 embedding 的话，模型的 paraemter 会过于巨大，因此我们通常采用 hash trick, 也就是通过一个 hash function 把原本的 id 映射到另外一个空间上面，然后这个空间的 cardiality 就是我们 embedding lookup table 的列数，一般我们也把这个 cardinality 称作 hash size. 如果 hash size 设置的太大，那么会浪费空间，如果太小，又会导致太多的 collision 从而影响性能（尤其是对 tail id 来说），因此如何找到一个比较好的 hash size 也是一个玄学 这里再插入一个题外话，目前主流的一些 hash function 基本上是 semantic-less 的，也就是说很有可能一些非常 popular 的 id, 比如大V们的 post 和一些 tail id 比如我的 post 是被 hash 到同一个 bucket 里面去了，这会导致这个 embedding 会被这些大V们的 post overwhlem，而不会有太多的我的 post 的 representation. 这其实是一个 known issue, 业界也有一些尝试用更复杂的 hash function, 比如基于 culster 的 hash 来处理有两种 hash size 的方法可以使用，一种是在 traning dataset 上 sample 一部分数据之后做分析，看不同的 hash size 情况下 collision 的情况，然后动态的调整（比如使用二分搜索）使得 hash size 能够满足一定的 collision rate 的阈值需要；另外一种方法是先给一个很大的 hash size，然后我们在模型训练的时候同时保存一个 counter 来记录各个 column 的 hit rate, 然后再模型训练完毕之后根据这个 hit rate 来做一个 post-processing 针对 embedding dimension, 目前主流的做法就是用一个 single dimension, 但是有一些研究尝试用 variable dimension, Google 曾经有一篇 paper 但是我现在找不到了，之后找到了再 update 过来 If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeAcknowledgemment感谢 Yuan 大佬，Bing 姐和 Yuzhong 大佬提供的修改和补充意见，让这篇 blog 更加的完善Reference Deep Interest Network for Click-Through Rate Prediction Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta Enhancing homepage feed relevance by harnessing the power of large corpus sparse ID embeddings User Action Sequence Modeling for Pinterest Ads Engagement Modeling Evolution of Ads Conversion Optimization Models at Pinterest Real-time Short Video Recommendation on Mobile Devices Privileged Features Distillation at Taobao Recommendations Practical Lessons from Predicting Clicks on Ads at Facebook How we use AutoML, Multi-task learning and Multi-tower models for Pinterest Ads PinnerFormer: Sequence Modeling for User Representation at Pinterest" }, { "title": "How to Design Auction System", "url": "/How-to-design-auction-system/", "categories": "Distributed System", "tags": "system design, auction, realtime system", "date": "2024-04-06 00:00:00 -0700", "snippet": "In this post, let’s discuss a little bit how to design an auction system similar to the one on eBay, where owner could list their items in the system and others could place a bid on it. User with h...", "content": "In this post, let’s discuss a little bit how to design an auction system similar to the one on eBay, where owner could list their items in the system and others could place a bid on it. User with highest bid would be the winner of this auction and could buy it.In a real world auction system, there are lots of components involved, such as the search (user could search active auction based on their interest), payment (winner need to make the payment) and inventory (owner could add new items). We would not dive deep into these components, but would only focus on the auction service itself. For search and payment, I plan to have other posts to discuss them in depth.In this post, we would discuss 2 different ways to design the auction system, stateful and stateless, and see what would be their pros and cons. In reality, stateless is more common, while stateful design still play a critical role in different use cases, e.g. stream processing.Functional RequirementWe would assume the following functional requirement to be offered by our system User could start an auction User could view the active auction, and place a bid in the auction; user could also get realtime update on the current highest bid Auction is closed when there is no higher bid for 1 hour Winner of the auction would receive notification and has 10 minutes to make the paymentNon Functional Requirements High availability High scalability Low latency Eventual consistency is acceptable for live bidding part (we could discuss for higher consistency level), but when determine the winner of the auction, it needs strong consistency 1B DAU, 100k auctions per-day, on average 10% of user place 1 bid per day, assume 10:1 read:write ratioSome questions to clarify What if there are multiple bids with the same price, who would be the winner? The first bidder would be the winner Do we allow a bidder to place multiple bids within the same auction? No, each bidder could only place 1 bid, but they could increase their bid if their original one is not winner Do we need to record all bids that user placed during the auction? This is great question, let’s keep all bid that users have placed instead of just winners What shall we do if there is no bid for certain auction, do we need user to provide a TTL? Let’s simplify the problem as of now and assume there is no TTL required High level designNewton has said that If I have been able to see further, it was only because I stood on the shoulders of giantsIn this design, we would also stand on the shoulders of giants, which is live comment and cron job scheduler.Auction creationThis part is relative simple. We have Auction Service to handle the creation HTTP request from user. The auction service would write a new entry to the auction_table within Auction DB and update to cache. Below is an example schema of our auction table. Besides the regular metadata such as owner_id, item_id and created_at, there are 2 important fields, status and expire_at, which is critical for us to manage the transition of auction and handle the payment.When we create a new auction, we would also update it into the cache and mark it as a ACTIVE auction. This design choice actually makes our auction service stateless: it does not need to maintain any data on the server regarding the auction. If it needs to know the status of an auction, it would query the cache and then do the necessary processing. The cache is primarily used to help us improve the read performance regarding the highest bid for a given auction. If DB write or cache update fails, we would return failure to client and client would retry the creation.There might be issue that the status in cache and in Auction DB are inconsistent, we would dive deeper into this topic in Cache and Auction DB consistency section.Auction Bid Place and UpdateFor this part, there are 2 key problems we need to answer: the connection mechanism between client and our service the mechanism to route highest bid to users who are viewing the current auctionFor the first problem, we would use a combination of HTTP request and server sent event (SSE): to place a bid, we issue an HTTP request to Auction Service; while to receive highest bid from others, we leverage SSE connection with Bid Update Service. Other connection options are HTTP long polling and websocket. HTTP long polling is relative less efficient because client needs to repeatedly query the backend for new bids. Websocket is a little bit over killing in our scenario as we don’t expect each user viewing the auction actively place bids, thus a single direction connection is sufficient. However, websocket might also be applicable in some cases. A more detailed comparison between websocket and SSE is available in Websocket vs SSE.For the second problem, one naive approach is to write all bids into DB and let the Bid Update Service to poll the DB to see if there are new bids. This approach works if there is not much traffic, but is less efficient in our scale and would put too much pressure on DB (# of auction x 60 / # of granularity QPS from a single Bid Update Service). Here we would leverage a hierarchy fan-out mechanism to route the bids.When user first navigate to an auction page, we would retrieve the information about the auction through Auction Service via regular HTTP request. If the auction is still in ACTIVE status, user would build a SSE connection with one Bid Update Service (Load Balancer could randomly pick one). The Bid Update Service bus1 would update its in-memory subscription table to record that a user u1 is viewing auction a1. Also, this server would also make a request to Dispatcher specifying that itself is listening to a1 and Dispatcher would also update its in-memory subscription table.# bus1 subscription table{ 'a1': ['u1', 'u2'],}# dispatcher subscription table{ 'a1': ['bus1'], 'a2': ['bus2'],}When user make a bid, client would send a HTTP request to Auction Service, the node that handle the request would also make a request towards Dispatcher. The Dispatcher would check its internal subscription table to figure out which Bid Update Service (in this case bus1) needs this update. Once bus1 receives the request, it would also check its internal subscription table to figure out which connected user it needs to send this update.In the version we just described, Dispatcher is a stateful service because it needs to maintain the subscription table. If it is down, we won’t able to forward bid update anymore and thus making it highly available is critical to our system. The following options could be considered: Adopt write ahead log and snapshot to rebuild the state after failure Replicate the state to external storage (e.g. KV store) so that other nodes could pick it up Active standby node to be promoted to primary once original one failsAnother consideration here is that we might be able to remove dispatcher, and just use coordination service or a distributed kv store to maintain the subscription table. Bid Update Service would directly make update to coordination service, and Auction Service directly query it to figure out the Bid Update Service it needs to send update to.There are pros and cons of both approach Dispatcher pros: simplify Auction Service’s responsibility (SRP), could scale individually, handoff on retry cons: slightly more complex overall architecture Without Dispatcher pros: simpler architecture, less maintenance cost cons: Auction Service needs to handle forwarding and retry If we would like to achieve higher consistency, such as each update needs to be sent to all users that is within the same auction. We could enable ACK among the services. For example, if certain Bid Update Service does not reply ACK to Dispatcher, Dispatcher would retry the request. It is possible that on the client side we receive duplicated events, but it is pretty simple to dedup as we only need to keep the highest bid.It is still possible that certain bid update is lost during the transmission and it might not a big duel. The reason is that: During normal active auction, there would always new bids coming out, which overwrite the pervious one; so certain data loss on client side would not make a big issue. The only critical one is the miss of highest bid, which would be the last bid on the current auction. We could set a timer on the client side, and if it has been 10mins since we receive last update on bid, we could issue a hard pull to Auction Service to get the latest bid information.Having discussed about how bids are routed to other users, let’s take a look how we maintain the current highest bid. When user make a bid, one instance of Auction Service is going to handle the request. It first check if the auction exists in cache or not, and see if the status of the auction is still ACTIVE status. If there is a cache miss, it reads Auction DB to check the status of the auction (this could happen but should be some corner case). If auction is still ACTIVE, then Auction Service write the bid into the bid table in append pattern, which is great for write throughput. This choice would result in multiple bids for a single user given an auction, and we would use the latest one as user’s final bid (latest could be determined by insertion time, or we could have client side request id which would be more robust). Once DB write is done and if the new bid is higher than the current one in cache, we would also update the information in cache and Auction Service would also send request to Dispatcher to deliver this new update to all clients.It is possible that the DB write is failed or the cache update is failed. We would retry the request if is some transitional issue.In the cache, we would store the following metadataauction_id: (status, highest_bid, highest_bidder_id, updated_at, expire_at)status, highest_bid and highest_bidder_id is relative straightforward. updated_at is used to record the staleness of the cached entry, expire_at is used as timer to trigger the auction execution (see Auction Bid Execution). This state works because in our FR we assume that the same user could only modify his bidder to higher price instead of lower. If we allow user to bid lower, then we need to store all user’s bid or top 100 bid.Since we cache auction state by auction_id, we could suffer from hotspot issue. For example, Wing Gundam Zero is so popular that everyone tries to bid it and we have lots of concurrent update to the cache. Below are some options that we could consider To deal with high volume of concurrent write request, we could use lease to coordinate the update to avoid potential stale update. The downside is that the update might need to retry multiple times to succeed. If we choose quorum as our replication strategy, we could potentially set write commit to 1 to increase the write throughput and have customized conflict resolve (relative simple as larger-is-winner). This works because in our FR we assume that the same user could only place higher bid but not lower.Auction Bid ExecutionTo execute the winner’s bid after 1 hour, we have a Fulfillment Service. This service is similar to a cron job scheduler that it periodically scan the state we have in cache and see if there is any bid that needs to be executed by checking the status and expire_at. Once it identify one bid that needs to be executed, it would also send a request to Auction DB to double check if this is indeed the winner bid we need to execute: If not, it would make a write to cache to correct the information in cache. This is similar to read repair in quorum replication. If confirmed, then Fulfillment Service would update the status of the auction to be PAYMENT_PENDING in both DB and cache. The expired_at field in auction_table would be set based on the policy (e.g. 10mins in our case). The winner_id, winner_bid_id, winner_price would also be populated all together. And then send request to notification system to send a payment notification to the winner. This event update would also be sent via the Dispatcher to all live users in this auction.The actual payment would be handled by another dedicated system which we won’t discuss too much in details. But once the payment is done, the payment service would update the auction status to SUCCEED.The Fulfillment Service would also periodically check the auction that is in PAYMENT_PENDING status and see if there is any auction that exceeds the deadline but still not SUCCEED yet, and move them to FAILED status.Notice that in our design, the Fulfillment Service depends on the cache to trigger the bid execution. This requires us to have cache to be highly available (through strategy such as different replication mechanism). Another option is to directly have the Fulfillment Service to query the Auction DB where our ground truth data exists. It needs to perform a relative complex query to join auction_table with bid_table to find the wining bid of each ACTIVE auction and check if they need to be executed or not. This is one tradeoff we need to consider: use cache, pros is reduced latency, cons is potential inconsistency issue which cause missed execution directly read db, pros is accurate and no missed execution, cons is high latency and more pressure on DBFinal Stateless ArchitectureIn the final design, we also introduce a Reconcile Service which help us to detect certain abnormal situation. For example, the payment has succeed but the auction status is not correctly updated.Stateful ChoiceThe discussion above is mainly on the stateless design. In this section, we discuss a little bit about the stateful design and see how it would be different from the stateless one.We would make Auction Service stateful, which means that it would maintain all bid related data for an auction. Once owner create an auction, it would be randomly assigned to a Auction Service and all bid for this auction would be handled through this instance. To minimize the latency, we could make the state maintained in memory. But similar to Dispatcher, we still need to make it highly available. WAL + snapshot or rebuilding from Auction DB are available options.If user make a bid, we would leverage the load balancer to route this request to the right Auction Service instance to handle it (service discover). We don’t need another cron job scheduler to check if there is any bid needs to be executed, all these information is already available within the instance and it could handle that correctly.We could take a simple comparison between stateful and stateless   stateful stateless consistency easier to achieve high consistency as all data related to an auction is handled by a single server, for example we don’t need a separate fulfillment service to check if there is a bid to be executed more challenging because there could be concurrent data write on the same auction handled by different servers availability more challenging to achieve as we need to replicate the stateful data easier to handle as the server is stateless and all state data is handled by external storage scalability more challenging to scale, especially hotspot easier to scale as we could add more machines and evenly balance the traffic Additional DiscussionIn this section, we discuss about several additional points about the design.High AvailabilityDuring the high level design discussion, we have touched a little bit about how to achieve high availability in each component. In this section, we summarize the key points and add some additional ones. Auction Service in the stateless design, there is not much concern here, if the node is down before response back to client, the client would just retry and another node would help server the request. There might be duplicated write/update but it is fine in our case the auction creation could use upsert and check if there is the same user_id and item_id combination within a time rage for dedup the bid is designed to be in appends and only the last one (by request id or injection time) would be used as the user’s final bid the update to cache is fine regarding duplicated ones in the stateful design, we need to replicate the service state, by having follower node or snapshot the state to external storage Dispatcher: this is also a stateful service and the strategy is similar to the stateful Auction Service Cache, Auction DB, KV Store: different replication strategy could be discussed here, such as single leader, multi leader and quorum based. If you are not familiar with these concepts, please refer to the video below learn more details Bid Update Service: even though these service are also stateful because they need to maintain the connection with client, we don’t need to replicate them nor persistent the information similar to other stateful service. The reason is that: 1. this stateful information is coupled with the liveness of this service, if the node is down, the connection has to be rebuild with other nodes; 2. the stateful information is not shareable with other nodes Fulfillment Service: this is also stateless and we could have a active standby to take the work once the primary one is doneHigh ScalabilityWe didn’t talk too much about how the system could scale. Auction Service in the stateless design, it is pretty easy to scale as we could add more nodes to improve the request that we could handle in the stateful design, we could scale it via sharding by owner_id; sharding by auction_id is an option if we have a separate id generator to assign it upon the creation request Dispatcher: the size of the subscription_table is manageable (# of bid update server x 100k x 8 bytes ~ GB level), thus a single sever should be sufficient; however, the size of data is only one dimension we need to consider when scale the system, the QPS would also be a factor that we need to consider. For Dispatcher, it needs to deal with pretty high volume of request, thus we could add read replica to improve the throughput (sync replication for stronger consistency or async for eventual consistency), or we could also shard it by auction_id Cache, Auction DB, KV Store: different sharding strategy could be discussed here, such as partition by auction_id (which offers good co-locate property for the auction_table and bid_table but has the downside of hotspot); or partition by user_id (which might better distribute the write as is it relative rare for someone that becomes a hotspot and they could be rate limited) Bid Update Service: it is also easy to scale by adding more nodes because they only keep in-memory subscription_table Fulfillment Service: we could shard it by auction_id to evenly distribute the processing to more nodesCache and Auction DB consistencyIn our stateless design, we store all data into Auction DB, and also store highest bid related information for each auction in a cache. We adopted something similar to write through, in which we write DB first and then update the cache; another option to consider is write back, in which we update cache first, and then at sometime later right back to DB. Write back could be used if we decided to in real time update the winning bid into the auction_table to reduce the volume of write request.It is possible that we write to DB success but failed to update cache. For example, the request to update cache is failed or the node is down before try to update the cache. Retry could be used here, but it could still possible that the update to cache is failed after several retry. But since our Fulfillment Service reads the cache to execute the bid, it might read some outdated data because of the above potential failure. That is also why we have updated_at field to track if we should read from DB again to see if the data is up-to-date. Also upon serving request from client on pulling highest bid, we leverage updated_at to do a read repair to fix the potential out of date.Websocket and SSEWebsocket and SSE are 2 common way we build a connection with backend and keep it live to send/receive data; instead of repeatedly creating new request and sent it over. Below is a simple comparison of these 2 approach   Websocket SSE communication bi-direction single direction support most modern browser already supported limited browser support failure could not reconnect and need to establish a new one could reconnect data type support both text and binary data text data only application realtime messaging, online gaming stock monitor, live comment In our current design, we are establish a new SSE whenever user navigate to a new auction. Another design choice here is to let user establish a new connection upon login to our application. And keep a websocket connection. Whenever user navigate to another auction, it would send this event over the websocket so that the Bid Update Service could update the subscription table. Depends on the pattern of how general users are interacting with our system, we could optimize the choice of the connection mechanism. If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeReference Streaming a Million Likes/Second: Real-Time Interactions on Live Video How we designed Dropbox ATF: an async task framework Design Data-Intensive Applications Scaling Stateful Service Difference between Websockets and Server Sent Events Scaling Memcache at Facebook Stream Processing with Apache Flink DDIA Chapter 6 PartitionAcknowledgementThanks Rita and Celia for the great discussion and lots of idea." }, { "title": "How to use LLM for recommendation task", "url": "/How-to-use-GPT-for-recommendation-task/", "categories": "Machine Learning, Recommendation System", "tags": "llm", "date": "2023-12-12 00:00:00 -0800", "snippet": "Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would li...", "content": "Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey.Some key take away: LLM internally has encapsulated lots of knowledge about the world and it could leverage these knowledge to do some general recommendation (such as Movie) In context learning is a powerful technique to inject various information into promote to provide more context for LLM, such as user profile and user past interaction history Use training data that specifically constructed for recommendation task to fine tune LLM could further improve the performance of LLM We could directly use LLM to output candidate, or use LLM output as additional signal to inject into existing recommendation models PS: due to the rapid change of this area, the paper I read might have been outdated. Please feel free to leave comments on the latest work/idea in this domain. Also I’m reading the latest paper from arxiv and will potentially have a new series of post on summarizing the latest work in LLM and ML area, stay tuned!PPS: I would primarily summarize my understanding without to much technical terms and mathematic formula; the main goal is to grasp the high level idea of the paperContextIn classical recommendation system, we usually adopt a 2-stage architecture. In first stage, we adopt heuristic rule, or leverage some simple model to quickly identify some promising candidates from the entire eligible population (actually, there is indexing step before here as well, but for simplicity, let’s skip that). This first stage is called candidate retrieval, which we usually optimize for recall. In the second stage, we would rank the candidates we retrieved in the first stage, via more signals and more powerful model. This stage is usually called rerank, which optimize for precision.Pairwise Ranking via LLMIn paper “Large Language Model Are Effective Text Rankers With Pairwise Ranking Prompting”, the author proposed a new format of prompt that let LLM to rank a pair of candidates given a query, which outperforms the point-wise and list-wise format. The format of the prompt is as follow:f\"\"\"Given a query {query}, which of the following two passage is more relevant to the query?Passage A: {description of A}Passage B: {description of B}Output Passage A or Passage B\"\"\"For each pair of candidates, we use the above prompt to let LLM output the choice, and compute the final scores as\\[s_{i} = 1 * \\sum_{j \\neq i} I_{d_{i} &gt; d_{j}} + 0.5 * \\sum_{j \\neq i} I_{d_{i} = d_{j}}\\]and rank the document accordingly.Enrich the information for LLM to recommendPersonalized recommendation is critical to improve the conversion rate. Use profiling, user past’s item interaction history bring valuable signal for recommendation. In this section, we will take a look some idea on how to inject such information into prompt to let LLM “learn” the flavor of user and provide better personalized result.In “Is ChatGPT a Good Recommender? A Preliminary Study”, the authors proposed different type of prompt of different type of tasks. These prompt could be decomposed as task descriptor, user-specific injection, formatting restrictions. User-specific injection is the part where we add user’s past item interaction info. The format for sequential recommendation is as follow (content in bracket is comment)f\"\"\"Requirement: you must choose 10 items for recommendation and sort them in order of priority, from hightest to lowest. [task descriptor]Output format: a python list. Do not explain the reason for include any other words. [formatting restrictions]Given user's interaction history in chronological order: {[i_1, i_2, i_3, ..., i_n]}, the next interaction item is {i_n+1}. [In context learning]Now, if the interaction history is updated to {[j_1, j_2, j_3, ..., j_n]} and the user is likely to interact again, recommend the next item. [user-specific injection]\"\"\"In this prompt, a common technique, which is called in context learning, or few shot prompting , is used. By showing LLM some examples to follow in the prompt, we could change the underlying distribution of LLM model and bias it to generate the output conditionally on the examples we have given. This stanford blog is a great source to learn more on how in context learning works. In short words, the additional example we provided helps LLM to better locate concept internally, and thus more aligned. A Bayesian inference view on that is as follow, which is pretty easy to understand\\[p(output|prompt) = \\int_{concept}p(output|concept, prompt)p(concept|prompt)d(concept)\\]In “PALR: Personalization Aware LLMs for Recommendation”, author adopted similar approach to integrate users’ past interaction into prompt. One novel idea in this paper is to leverage LLM to generate user profile, which leverages the summarization capability of LLM. The prompt is as follow (use MovieLens-1M as example)f\"\"\"Input: Your task is to use two keywords to summarize user's preference based on history interactions.The output is an itemized list based on importance. The output template is:{KEYWORD_1: \"HISTORY_MOVE_1\", \"HISTORY_MOVE_2\"; KEYWORD_2: \"HISTORY_MOVE_2\"}The history movies and their keywords\"MOVIE_1\": KEYWORD_1, KEYWORD_2\"MOVIE_2\": KEYWORD_1, KEYWORD_3\"MOVIE_3\": KEYWORD_4\"MOVIE_4\": KEYWORD_1, KEYWORD_3, KEYWORD_4\"\"\"Then the user profile is also input into the prompt to let LLM recommend items from the candidate set.In context learning is a technique that I widely used during my project. It is much cheaper compared to fine-tune LLM, and the performance is also pretty good as long as you have high quality data. From my experience, formatting control is pretty challenge and sometimes could not be 100% solved by explicit instructions or few shot. Sometimes, we need to have some dedicated business code to do some postprocessing on LLM output to parse the part we interested most out.Go beyond In-Context Learning: Fine-tune LLM for recommendation taskIn context learning is a powerful technique, however, due to the fact that LLM is trained on NLP task instead of recommendation task, its performance is still sometime limited. Using some training data that is specifically constructed for recommendation to fine-tune LLM could help LLM to learn more for recommendation task.In TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation, the author proposed a 2-stage fine-tuning framework. In first stage, they leverage Alpaca Tuning to improve LLM’s generalization ability, and then in 2nd stage, they use recommendation training data to do rec tuning. The format of the training data is as followf\"\"\"Task instruction: Given the user's historical interactions, please determine whether the userwill enjoy the target new movie by answering \"Yes\" or \"No\".Task input: - User's liked items: GodFather. - User's disliked items: Star Wars. - Target new movie: Iron Man.Task output: No\"\"\"A high level flow is as followWork with existing Recommendation modelsBesides directly let LLM to output the recommendation from the candidates, we could also use LLM together with existing recommendation models. Use the output of one model as input to another model has been a widely adopted practice in the ranking world, e.g. using the GBDT leave as feature in NN. You could think of that we leverage model to do some compression and preprocessing on the signals, which is similar to traditional feature engineering.In LLM-Rec: Personalized Recommendation via Prompting Large Language Models, the author used different prompt to generate various text description from the original content, and then embedding them as additional signals and feed into MLP for ranking together with the original descriptions. Below is a high level architecture of their model" }, { "title": "How to Design Webhook", "url": "/How-to-Design-Webhook/", "categories": "Distributed System", "tags": "system design, webhook", "date": "2023-12-03 00:00:00 -0800", "snippet": "Today, let’s discuss about how to design a system that could let customer to register webhook and send webhook requests to destination.Let’s first align on some terms that we are going to use acros...", "content": "Today, let’s discuss about how to design a system that could let customer to register webhook and send webhook requests to destination.Let’s first align on some terms that we are going to use across this post: webhook provider: the platform that let customer to register webhook and send the webhook request webhook customer: they provide the endpoint they would like the provider to send the webhook request toWhat is WebhookFor readers that are not familiar with Webhook, it is a type of notification mechanism that communicates in one direction. This is a technique widely used in SaaS platform (e.g. Shopify, Strip, Slack) for external applications to receive data when some events they interested in happened on these platform.For example, codingmonkey.com is a website that I’m running (hosted on AWS maybe), and I have a shop on Shopify that sells awesome keyboards. I could register a Webhook on Shopify so that whenever there are some Shopify users purchase awesome keyboards, a purchase event would be sent to an endpoint that is hosting on my server to process (e.g. store it in database, issue an invoice to purchaser, or send a thank you email).Sounds similar? Yeah, it sounds pretty like a notification system. The difference here is that customer need to register webhook to express which event they would like to listen to and which endpoint URL the data need to be send to. There are also some other difference, such as we need to know if the webhook request is successfully received by codingmonkey.com or not, and additional security check to protect the data we are sending. Excited to learn more? Let’s dive deep and see how we could build such a system.Functional RequirementI didn’t find very crystal requirements on this, the following is some FR I summarized from the industrial examples Customer could register webhook and they could register multiple webhook Support retry of webhook and minimize lost webhook as much as possible Provide observability to customersNon Functional Requirements 1B events pre day, which is equivalent to 10k qps for webhook trigger and request sending High availability The design should scale SecuritySome questions to clarify Do we allow event loss? No, we should avoid event loss as much as possible. What delivery semantic do we provide? At least once, at most once or exactly once? At least once If we resend webhook, could we resume the endpoint to be idempotent? Yes, but we need to provide necessary info to achieve that High Level DesignI would skip the API design and the back envelop estimation for the sake of sanction of this post. We would start simple to first meet the functional requirements, and then improve the availability, scalability of our system.Webhook RegistrationWe need to be able to let user to register webhook in our system. Below is a simple design of this partThe design is pretty simple, which we have web server to handle request from client and store the information in the webhook metadata database. This metadata database is going to be used by the webhook delivery flow to figure out where to send the webhook request to.For each webhook, we would generate a unique webhook_id as the unique identifier of each webhook. Besides that, we also need to store the event_type that this webhook listen to, as well as the owner_id. The event_type is a list of per-defined events that are available on our platform, which could be revealed via API document provided to customer. Besides that, we also need to store the url and secret_token in the database, to know where we should send the request to, as well as sending the request safely. The secret_token could be used for authentication and encryption for sending the webhook requests. In this schema, customers could register multiple webhook within the system.One challenge here is that how to verify that customers have ownership on the urls they have provided. One common solution here is to send a test event to the endpoint they are providing, and ask them to verify they have received it; or by including a “challenge” in the request that the endpoints need to echo back (e.g. Dropbox webhook).Webhook DeliveryNext, let’s take a look at the webhook request deliver flow, which is the meaty part of the entire system. As mentioned earlier, the entire system is similar to notification system, and thus I use notification system as a template for this design. Below is a high level design of this flowIn this design, we adopted a single responsibility strategy and separate the delivery logic into several components Webhook Controller is responsible for processing the events (that are generated on our platform) and figuring out which endpoint we should send the data, as well as constructing the payload of the request. Here we assume that the events generated from our platform contains the event_type and owner_id information (because we don’t want the event that happened in our shop to be delivered to others’ endpoints). With event_type and owner_id, controller could retrieve the record from the metadata database and construct a webhook request task. Once the task is constructed, controller would write an entry into Webhook Delivery Log database to persistent this information, and set the request_status to PENDING, which we could leverage later for different retry strategy. Message Queue is adopted to store the webhook request task, which worker would consume. Using message queue bring the following benefits, which outperform the additional complexity they bring: controller don’t need to wait for the current webhook request to be delivered to process the next one (it is async okay). This not only saves resource, but also increases robustness (e.g. if worker failed, controller could still make progress and put job onto the queue instead of being blocked). if there is a burst of events come in, message queue could help buffer the increased volume of task so that worker won’t be throttling. Webhook Worker is responsible for consume webhook request task from the queue, and send the actual HTTP POST request to the endpoint. The payload of the HTTP POST request could be something like this{ \"id\": str \"event_type\": str \"created\": int, \"data\": { \"field_1\": value_1, \"field_2\": value_2, ... }}Worker would need to wait for the response from the endpoints, to know if the request has been successfully received. If received, then worker could update the record’s request_status in the Webhook Delivery Log database to SUCCEED; otherwise, different strategy of retry could be adopted to resent the webhook request. Supporting retry also means that we are providing at least once semantic, which could result in duplicated request sent to endpoints. We expect these endpoints need to be idempotent, which is doable with the id sent along with the HTTP POST request.Webhook Retry StrategyOne critical consideration for webhook system is the retry mechanism in case HTTP POST returns 4xx or 5xx code, or timeout. There are different retry strategies: Retry immediately upon failure within a time range repeatedly, or until max retry limit Exponential backoff within a time range (e.g. 24hrs), or until max retry limitFor example, Strip would attempts to delivery webhook up to 3 days with an exponential backoff. Option 1 is easy to implement, but the issue is that: if endpoint is returning error code, then it might take some time to mitigate the issue; immediate retry is likely to hit the same error, try again later time would be a better option.In order to achieve exponential backoff retry mechanism, we would use a cron version Webhook Controller, which dose not consume the events from upstream, but scan the Webhook Delivery Log database to identify the webhook requests that are still in PENDING status and have not exceed the max retry. For each of such request, the controller would bump their retry_count or retry_timestamp, and publish a new task into message queue.The addition of this cron version Webhook Controller could also help mitigate worker failure issue. For example, if one webhook http request is consumed and removed from the message queue by a worker, but suddenly the worker failed; since the task is already removed from the queue, other worker won’t able to get it and process it again. However, the cron controller would notice in from the log that there is one PENDING request and schedule it to retry. Another option is that if message queue provide the capability to persistent messages, worker could commit the position of the message in the queue they have processed, and if worker failed, it could resume from its last committed position and process the message againIf for some endpoints, the failure is consistent for a certain time and over the threshold, we could temporarily mark the endpoints as disabled in the metadata’s status field to prevent new events from further deliver to them. And we could send alert email to customers to have them investigate into the issue. Once the issue is mitigated, the status could be changed back, and we could consumer the delivery log to resume the webhook request; or use other channel, such as dump the entire data that need to be delivered during this time and send it over to customer.ObservabilitySince we have already log the status of each webhook request in Webhook Delivery Log database, it is easy to support the observability. This could be implemented via having web application server to send a query to the database to aggregate the data and render it as a dashboard for customers. They could know how many webhook request have been sent, what’s the failure rate, etc.SecuritySecurity is especially important in webhook system. In webhook registration section, we authenticate that the endpoints belongs to users, we also need to authenticate ourself that the HTTP request is from us.One common approach is to use HMAC to sign the request with a shared secret with the user and sent the signature along with the request(e.g. Strip uses this approach) and user could verify the signature with the shared secret. This shared secret could be auto generated upon user register webhook in our system, and show them to user in their monitor dashboard. This approach could also help us prevent replay attack, by including a timestamp used to expire webhook request.Another approach, which is less common, is to get a token from the consumer and add it to the Authorization header for validation. For example, if the owner of the endpoint has authorization server, then before sending webhook request, we could first obtain a JWT token and store it within our metadata table secret_token and use it each time we need to send webhook request.Besides the authentication problem, we also need to prevent the data we are sending could be read by others. There are also several options with different trade off: Avoid send sensitive information in the webhook payload. Instead, we could only send some entity id which is totally meaningless and ask customer to pull data again via other API. Pros is that this is the most safe approach, and the cons is that customer experience is worst Another option is to encrypt the data with a shared secret key, which is only known between webhook provider and webhook consumer. A follow up of this question is how could we share this secret key safely between customer and provider over the unsafe network? Here we could use RSA encryption. (This is a general practice, RSA is safe, since only yourself know the private key; but the amount of data could be transferred via RSA is limited. So it makes since to use RSA to send another secret key, which is used for encryption/decryption of large volume of data) Sending data with HTTPS and certificate pinning is also an option to safely transfer sensitive data, but this would have some performance hurt and require customer to have HTTPS setup such as CAHigh AvailabilityLet’s see if there is any single point of failure in our current design. What comes to us first is the database and message queue. There are multiple replication strategy here we could use, each comes with different trade off: For Webhook Metadata Database, we could adopt single leader strategy, and have 2 followers. The followers could use synchronized replication, which provides good consistency, but the write throughput on the leader would be low; while if we use async approach, leader could handle more write request while could lead to consistency issues among leader and followers. If we are building for a geo webhook system, we might also consider multi-leader strategy, with better write request severing based on location and annoy of write conflict. For Webhook Delivery Log Database, besides the aforementioned strategy, we could also consider the quorum based replication, which provides the best write throughput and eventual consistency is acceptable in this case. (Q: what would be the worst case here). For Message Queue, similar to the database, we could also have replica setup so that the message is written to multiple node instead of single one. Also, even if we only have a single node queue and it failed. Since we are storing all scheduled webhook request in the Webhook Delivery Log Database, the webhook controller (corn) would identify the abnormal ones and try to reschedule them.For other components such as web app server, webhook controller and webhook worker, they could be stateless. If a node fails, there would be other nodes available to continue the work.ScalabilityFor scalability, we could horizontally scale web app server, webhook controller and webhook worker by adding more nodes into the cluster. For database, we could shard it to scale if the total volume of data is too large to fit onto a single machine. Message queue could also be horizontally sharded by increase the number of partitions.There could be hotspot. For example, my awesome keyboard is so popular that lots of customer is visiting my shop and vast amount of events are triggered. To handle the hotspot, we could use a dynamic config to redirect the traffic of hotspot to specific cluster of machines, instead of starving the quote with other customers; or we could further shard the hotspot by some approach such suffix with numbers.Other optimizationThere are couple of other optimizations we could add to our system to make it more robust we could have load balances in front of webhook controller to route based on machine utilization; also we could integrate the rate limit here to prevent abuse of the system (such as bot triggered events) we could add a layer of cache to reduce the amount of read to metadata we could add a rate limiter to help control the http request we send to customers; for some customers that have high security requirement, they might only trust http request sent from specific IPs, we could have dedicated VPC to support that needs for observability, we could add some pre-compute mechanism to reduce the volume of data that the query need to scan; for example T-1 snapshot + on demand query on THere is our final design If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeReference Building Webhooks Into Your Application: Guidelines and Best Practices Strip Webhook Doc Dropbox Webhook Doc Shopify Webhook Best Practices Add Webhooks to Your API the Right Way Phoenix Architecture" }, { "title": "DDIA Chapter 11 Stream Processing Part I", "url": "/DDIA-Stream-Processing-I/", "categories": "Distributed System, DDIA", "tags": "system design, message queue, realtime system", "date": "2023-11-21 00:00:00 -0800", "snippet": "In this post, we would introduce stream processing. Since it is a large topic, we would break it down into 2 part, and in the first part, we would focus on the component that is related to the “flo...", "content": "In this post, we would introduce stream processing. Since it is a large topic, we would break it down into 2 part, and in the first part, we would focus on the component that is related to the “flow” of stream, a.k.a, delivery of message.What is EventStream is composed by sequence of event, which we also use message as an alternative term. Here is a quote from confluent on describing what is event An event is any type of action, incident, or change that’s identified or recorded by software or applications. For example, a payment, a website click, or a temperature reading, along with a description of what happened.Take the payment as an example, a payment event, could be User A paid X dollars to User B, for the purchase of an item C, on date X. This event would be recognized by our system to trigger the necessary processing (e.g. record in database, make third-party API call).How to deliver messageHow could we deliver message from machine A to machine B? There are multiple options.Direct connectionThe most straight-forward approach is to build a direct connection between A and B via network. Once the connection is published, B could receive the message from A in 2 different patterns Proactively asking A if there is new message with some intervals in between these ask Passively wait until A notify that there are some message for B to readThese 2 different patterns, more formally speaking, pull and push, is common approach on how message is delivered, or how consumer (B in our example) would receive the message.Direct connection works, but what would happen if B somehow offline for a period of time $T$? B would miss all the message A plans to deliver during $T$. One potential solution is to add the capability of storing the message temporarily within A, but that would increase the responsibility of A and make it more complexity. We need some sort of dedicated component to help us, this lead to message broker, or message queue, which is really good at this job.Message QueueMessage queue could be treated as some type of buffer in between of the message sender, a.k.a producer, and message receiver, a.k.a consumer. Producer would publish message to message queue, message queue would do some “necessary” processing on the message and hold it. Consumer could retrieve these message from message queue, by subscribing to some queue. Since message is buffered in message queue, it is okay that B is offline when A tries to send message, message queue would hold that message, and when B comes online, the message is not lost and could be consumed.When to use Message QueueMessage queue is pretty good to be used when the business involves certain async property, which means that user don’t expect an immediate response from the application, but could retrieve the result sometime in the future. Some typical case including: Job scheduler: user schedule a job (e.g. project building, model training) and expect it to finish sometime in the future Youtube video encoding: when user upload a video, the encoding job would be pushed onto a queue and be processed by some worker in the future Notification: a job to send some customer SMS/Email would be placed on queue and be sent in the futureIn the later section, we would see some more concrete example from industry on how message queue is being used in practice.Everything has two sides. The benefits of using message queue is that: 1. improve overall robustness of the system be decoupling different components; 2. balance the workload for upstream/downstream system (e.g. in case of burst of traffic). The downside of message queue is that, it would increase the complexity of the overall system (e.g. how to handle duplicated events gracefully).Industry practiceRabbitMQ &amp; KafkaRabbitMQ and Kafka is 2 commonly adopted message queue in industry. For a deeper dive into these 2 message queue, we would put it into another post. Here we would first summarize some highlight of them:   RabbitMQ Kafka Message Persistent control by request parameter persistent Message Delivery pull push Message Ack auto-ack or explicit ack no ack, consumer commit offset Scalability vertical horizontal Availability single node in general leader-follower replication Order Guarantee FIFO in general, special case: priority, sharded queue, multi consumer FIFO on partition level Consumer Load Balance priority or round robin different strategy specified by consumer group DoorDashIn this engineering blog, DoorDash introduced how they are using message queue in their business and why they migrate from RabbitMQ to Kafka. Several business task in DoorDash is done in async, such as order checkout, merchant order transmission and dasher location processing DoorDash use Celery + RabbitMQ as their initial async task processing infra. However, they identified several pain points: Availability is low. RabbitMQ would easily down during peak traffic. Traffic control needs to be enabled to prevent the issue that task consumption could not keep up with task publishing, which cause serious network lagging. Scalability is low. They are running the largest RabbitMQ node already (vertical scale). And they are using the primary-secondary HA mode, which also prevent them from scale (the down time could easily goes to 20mins to recover) They migrate RabbitMQ to Kafka to achieve better availability (partition replicated) and scalability (partitioned topic) They also mentioned on improvement on dealing with “straggler”: using one dedicated thread to read message from topic partition, and use multi-threading to process the message. Thus, if one message takes long time to process, then only one thread would be blocked, while other thread could continues to process the messages RobinhoodIn this blog from Robinhood, the author introduced how they are using Kafka to build their clearing service (which is one critical service to make sure the inside and outside account information is insync). Clearing service is not on the critical path of users (users don’t need to be aware of this), and thus they decided to build it as an async service. In their initial design, they use a monolith consumer, which contains a giant transaction to make update to several tables. This raise the contention issue and the efficiency is low. In their new design, they breakdown the original transaction into several smaller transaction to update only 1 ~ 2 tables. They also adopt the event source pattern that, once one job is done (e.g. user table update finished), it would fire one event to a Kafka topic, and one downstream consumer would consume the event and to the necessary update (e.g. update account table), and then fire another event. The benefit of this reduction in contention and overall throughput improvement But what if one consumer in the middle failed, how to resume and avoid duplicated write? Use Kafka commit log to resume where left When do the DB write, first update the lookup table, then the duplicated write would be no-op If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee" } ]
