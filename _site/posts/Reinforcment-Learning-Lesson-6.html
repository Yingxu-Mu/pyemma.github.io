<!DOCTYPE html>
<html>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Reinforcement Learning Lesson 6</title>
        <meta name="viewport" content="width=device-width">
        <meta name="description" content="A coding monkey, who is passionate in algorithm and machine learning, hoping one day to become a coding machine." />
        <link rel="canonical" href="http://localhost:4000/posts/Reinforcment-Learning-Lesson-6/" />

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

    </head>

    <body>

    <header class="site-header">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

  <div class="wrap">

    <a class="site-title" href="/">Coding Monkey</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        <a class="page-link" href="/about">About</a>
        <a class="page-link" href="/projects">Projects</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Reinforcement Learning Lesson 6</h1>
    <p class="meta">Sep 10, 2017</p>
  </header>

  <article class="post-content">
  <p>In the pervious we use a model to approximate the state value/action value function. In this post, we are going to learn how to directly parameterize a policy, which means we would directly get the probability of each action given a state:</p>

<script type="math/tex; mode=display">\pi_{\theta}(s ,a) = P[a|s, \theta]</script>

<p>In this case, we are not going to have any value function. A slight variance of this method is called <strong>Actor-Critic</strong>, in which both value function and policy are modeled and learnt.</p>

<p>The advantage of <strong>Policy based RL</strong> is:</p>
<ul>
  <li>Better convergence properties</li>
  <li>Effective in high-dimensional or continuous action spaces</li>
  <li>Can learn stochastic policies</li>
</ul>

<h4 id="policy-objective-functions">Policy Objective Functions</h4>
<p>Since we are going to learn $\pi_\theta (s, a)$ and find the best $\theta$, we need to first find a way to measure the quality of our policy. These are called <strong>policy objective function</strong> and some we can use are:</p>
<ul>
  <li>In episode environment we can use the start value</li>
</ul>

<script type="math/tex; mode=display">J_1(\theta) = V^{\pi_\theta}(s_1) = E_{\pi_\theta}[v_1]</script>

<ul>
  <li>In continuous environment we can use average value or average reward pre time-step</li>
</ul>

<script type="math/tex; mode=display">J_{avV}(\theta) = \sum_{s}d^{\pi_\theta}(s)V^{\pi_\theta}(s) \\
J_{avR}(\theta) = \sum_{s}d^{\pi_\theta}(s)\sum_{a}\pi_\theta(s, a)R_s^a</script>

<p>where $d^{\pi_\theta}(s)$ is stationary distribution of Markov chain for $\pi_\theta$.</p>

<p>After we have the measurement of the policy quality, we are going to find the best parameter which gives us the best quality and this becomes an optimization problem. Actually, similar to the last post, we can also use stochastic gradient to help use here. Since we are trying to find the maximum value, we are going to use what is called gradient ascent to find the steepest direction to update our parameter (very similar to gradient decrease).</p>

<h4 id="score-function">Score Function</h4>
<p>In order to compute the policy gradient analytically, we introduced the <strong>score function</strong>. Assume policy $\pi_{\theta}$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta (s, a)$. Then using some tricky we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla_\theta \pi_\theta (s, a) & = \pi_\theta (s, a) \frac{\nabla_\theta \pi_\theta (s, a) }{\pi_\theta (s, a)} \\
& = \pi_\theta (s, a) \nabla_\theta log \pi_\theta (s, a)
\end{align} %]]></script>

<p>Here, $\nabla_\theta log \pi_\theta (s, a)$ is the <strong>score function</strong>.</p>

<h4 id="policy-gradient-theorem">Policy Gradient Theorem</h4>
<blockquote>
  <p>For any differentiable policy $\pi_\theta (s, a)$, for any of the policy objective functions $J_1, J_{avV}, J_{avR}$, the policy gradient is</p>
</blockquote>

<script type="math/tex; mode=display">\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log \pi_\theta (s, a) Q^{\pi_\theta} (s, a)]</script>

<h4 id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</h4>
<p>Use return as an unbiased sample of $Q^{\pi_\theta} (s, a)$, the algorithm is as follow:</p>
<ul>
  <li>Initialize $\theta$ arbitrarily
    <ul>
      <li>for each episode ${s_1, a_1, r_2, …, s_{T-1}, a_{T-1}, r_T} ~ \pi_\theta$ do
        <ul>
          <li>for $t = 1$ to $T - 1$ do
            <ul>
              <li>$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) v_t$</li>
            </ul>
          </li>
          <li>end for</li>
        </ul>
      </li>
      <li>end for</li>
    </ul>
  </li>
  <li>return $\theta$</li>
</ul>

<h4 id="actor-critic-policy-gradient">Actor Critic Policy Gradient</h4>
<p>The problem with Monte-Carlo Policy Gradient is that is has a very high variance. In order to reduce the variance, we can use a <strong>critic</strong> to estimate the action value function. Thus in <strong>Actor Critic Policy Gradient</strong>, we have two components:</p>
<ul>
  <li><em>Critic</em> updates action value function parameters $w$</li>
  <li><em>Actor</em> updates policy parameters $\theta$, in direction suggested by critic</li>
</ul>

<p>Here is an example when we use linear value function approximation for the critic:</p>
<ul>
  <li>Initialize $s$, $\theta$</li>
  <li>Sample $a ~ \pi_\theta$</li>
  <li>for each step do
    <ul>
      <li>Sample reward $r$, sample next state $s’$</li>
      <li>Sample action $a’ ~ \pi_\theta (s’, a’)$</li>
      <li>$\delta = r + \gamma Q_w(s’, a’) - Q_w(s, a)$ (This is the TD error)</li>
      <li>$\theta = \theta + \alpha \nabla_\theta log \pi_\theta (s, a) Q_w(s, a)$ (We replace with the approximation)</li>
      <li>$w = w + \beta \delta \phi(s, a)$ (Update value function approximation model parameter)</li>
      <li>$a = a’$, $s = s’$</li>
    </ul>
  </li>
  <li>end for</li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">Coding Monkey</h2>

    <div class="footer-col-1 column">
      <ul>
        <li>Yang Pei</li>
        <li><a href="mailto:pyemma1991@gmail.com">pyemma1991@gmail.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/pyemma">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">pyemma</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/pyemma">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">pyemma</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">A coding monkey who wants to become a coding machine</p>
    </div>

  </div>

</footer>

    </body>
</html>
