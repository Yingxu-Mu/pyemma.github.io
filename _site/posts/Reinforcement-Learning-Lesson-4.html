<!DOCTYPE html>
<html>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Reinforcement Learning Lesson 4</title>
        <meta name="viewport" content="width=device-width">
        <meta name="description" content="A coding monkey, who is passionate in algorithm and machine learning, hoping one day to become a coding machine." />
        <link rel="canonical" href="http://localhost:4000/posts/Reinforcement-Learning-Lesson-4/" />

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

    </head>

    <body>

    <header class="site-header">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

  <div class="wrap">

    <a class="site-title" href="/">Coding Monkey</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        <a class="page-link" href="/about">About</a>
        <a class="page-link" href="/projects">Projects</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Reinforcement Learning Lesson 4</h1>
    <p class="meta">Aug 19, 2017</p>
  </header>

  <article class="post-content">
  <p>In this lecture, we learn how to solve an unknown MDP. In the last lecture, we introduced how to calculate the value function given a policy. In this one, we will try to find the optimize policy by ourselves.</p>

<h4 id="mento-calro-policy-iteration">Mento Calro Policy Iteration</h4>
<p>In the <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-2">Lesson 2</a>, we mentioned how to solve a MDP when we have full information about the MDP. One method is called <strong>Policy Iteration</strong>. It can be divided into two components: <em>policy iterative evaluation</em> and <em>policy improvement</em>. For the evaluation part, we can use the methods in <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-3">last lesson</a>, nominally MC and TD. However, we could not directly use the state value function, cause in the policy improvement step (e.g. greedy), we need to know the $R$ and $P$ to find the best action (recall the <a href="http://pyemma.github.io/posts/Reinforcement-Learning-Lesson-1">Bellman Optimality Function</a>). However, action value function does not need the model of the MDP while in greedy policy improvement:</p>

<script type="math/tex; mode=display">v_*(s) = argmax_a q(s, a)</script>

<p>For the policy improvement part. If we stick to the greedy method, it will not be good for us to explore all possible states. So we use another method which is called $\epsilon$-greedy. We will have $1-\epsilon$ probability to perform greedily (choose the current best action), and have $\epsilon$ probability to random choose an action:</p>

<script type="math/tex; mode=display">% <![CDATA[
\pi(s|a) = \begin{cases}
\frac{\epsilon}{m} + 1 - \epsilon, & \text{if $a^\star = argmax_a Q(s, a)$} \\
\frac{\epsilon}{m}, & \text{otherwise}
\end{cases} %]]></script>

<p>We have the final Mento Calro Policy Iteration as:</p>
<ul>
  <li>Sample the kth episode $S_1, A_1, â€¦, S_T$ from policy $\pi$</li>
  <li>For each state $S_t$ and $A_t$ in the episode</li>
</ul>

<script type="math/tex; mode=display">N(S_t, A_t) = N(S_t, A_t) + 1 \\
Q(S_t, A_t) = Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}((G_t - Q(S_t, A_t))) \\</script>

<ul>
  <li>Update the $\epsilon$ and policy:</li>
</ul>

<script type="math/tex; mode=display">\epsilon = 1/k \\
\pi = \epsilon\text{-greedy}(Q)</script>

<h4 id="sarsa-algorithm">Sarsa Algorithm</h4>
<p>If we use the logic in TD for the evaluation part, then we would have the sarsa algorithm. The main difference is that, in original TD, we use the value state function of the successor state, however, we need the action value function right now. We can obtain that by run our current policy again (remember, TD does not need the complete sequence of experience, we can generate the state and action along the way). Following is the algorithm:</p>
<ul>
  <li>Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0</li>
  <li>Repeat for each episode
    <ul>
      <li>Initialize $S$, choose $A$ from the current policy derived from $Q$</li>
      <li>Repeat for each step in the episode until we hit terminal
        <ul>
          <li>Take action $A$, observe $R$ and $S^\prime$</li>
          <li>Choose $A^\prime$ from $S^\prime$ from the current policy derived from $Q$</li>
          <li>Update <script type="math/tex">Q(S, A) = Q(S, A) + \alpha(R + \gamma Q(S^\prime, A^\prime) - Q(S, A))</script></li>
          <li>Update $S = S^\prime, A = A^\prime$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Similarly, we can also use Eligibility Trace for the sarsa algorithm and result in sarsa($\lambda$) algorithm. The algorithm is as follow:</p>
<ul>
  <li>Initialize $Q$ for each state and action pair arbitrarily, set $Q(terminate, *)$ to 0
    <ul>
      <li>Repeat for each episode</li>
      <li>Initialize $E$ for each $s, a$ pair to 0</li>
      <li>Initialize $S$, choose $A$ from the current policy derived from Q</li>
      <li>Repeat for each step in the episode until we hit terminal
        <ul>
          <li>Take action $A$, observe $R$ and $S^\prime$</li>
          <li>Choose $A^\prime$ from $S^\prime$ from the current policy derived from Q</li>
          <li>Calculate $\delta = R + \gamma Q(S^\prime, A^\prime) - Q(S, A)$</li>
          <li>Update $E(S, A) = E(S, A) + 1$</li>
          <li>For each $s$ and $a$ pair</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">Q(s, a) = Q(s, a) + \alpha\delta E(s, a) \\
E(s, a) = \gamma\lambda E(s, a)</script>

<ul>
  <li>Update $S = S^\prime, A = A^\prime$</li>
</ul>

<h4 id="q-learning">Q Learning</h4>
<p>Both MC policy iteration and sarsa algorithm are <strong>online learning</strong> method, which means that they are observing there own policy, learning along the process. There is another category which is called <strong>offline learning</strong>, in which we learn from other policy, not the policy we are trying to improving. Example is that a robots learns walking by observing human. Q learning falls in this category. It is pretty similar to the sarsa algorithm, the only difference is that when we get the action for successor state, we replace the $\epsilon$-greedy to greedy policy. The Q learning method is as follow:</p>
<ul>
  <li>Initialize Q for each state and action pair arbitrarily, set Q(terminate, *) to 0</li>
  <li>Repeat for each episode
    <ul>
      <li>Initialize $S$, choose $A$ from the current policy derived from Q</li>
      <li>Repeat for each step in the episode until we hit terminal
        <ul>
          <li>Take action $A$, observe $R$ and $S^\prime$</li>
          <li>Update <script type="math/tex">Q(S, A) = Q(S, A) + \alpha(R + \gamma max_a Q(S^\prime, a) - Q(S, A))</script></li>
          <li>Update $S = S^\prime$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">Coding Monkey</h2>

    <div class="footer-col-1 column">
      <ul>
        <li>Yang Pei</li>
        <li><a href="mailto:pyemma1991@gmail.com">pyemma1991@gmail.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/pyemma">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">pyemma</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/pyemma">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">pyemma</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">A coding monkey who wants to become a coding machine</p>
    </div>

  </div>

</footer>

    </body>
</html>
