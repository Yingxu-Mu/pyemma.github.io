<!DOCTYPE html>
<html>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Reinforcement Learning Lesson 1</title>
        <meta name="viewport" content="width=device-width">
        <meta name="description" content="A coding monkey, who is passionate in algorithm and machine learning, hoping one day to become a coding machine." />
        <link rel="canonical" href="http://localhost:4000/posts/Reinforcement-Learning-Lesson-1/" />

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

    </head>

    <body>

    <header class="site-header">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

  <div class="wrap">

    <a class="site-title" href="/">Coding Monkey</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        <a class="page-link" href="/about">About</a>
        <a class="page-link" href="/projects">Projects</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Reinforcement Learning Lesson 1</h1>
    <p class="meta">Aug 13, 2017</p>
  </header>

  <article class="post-content">
  <p>This is the first post for the series reinforcement learning. The main source for the entire series is <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html">here</a>. The post mainly focus on summarizing the content introduced in the video and slides, as well as some of my own understanding. Any feedback is welcomed.</p>

<p>In this post, we will talk about Markov Decision Process (MDP), which is a pretty fundamental model in many reinforcement learning cases.</p>
<blockquote>
  <p>Almost all RL problems can be formalized as MDP</p>
</blockquote>

<h4 id="markov-process">Markov Process</h4>
<p>In order to learn about MDP, we need to first know what is Markov Process (MP). This introduces the following two concept:</p>
<ul>
  <li>Markov Property</li>
  <li>State Transition Matrix</li>
</ul>

<p>In the most simple word, <strong>Markov Property</strong> means that the future state is independent on the history given the current state. It can be formalized using following statement:</p>

<script type="math/tex; mode=display">\mathbb{P}[S_{t+1}|S_{t}] = \mathbb{P}[S_{t+1}|S_1, ..., S_t]</script>

<p>This means that the current state contains all they necessary information for the future, and we can discard all history information.</p>

<p><strong>State Transition Matrix</strong> contains the probability we go from on state to another one. Given a state $s$ and its successor state $s^\prime$, the probability from $s$ goes to $s^\prime$ is given by</p>

<script type="math/tex; mode=display">P_{ss\prime} = \mathbb{P}[S_{t+1}=s\prime|S_{t}=s]</script>

<p>And the State Transition Matrix is by</p>

<script type="math/tex; mode=display">% <![CDATA[
P =
\begin{Bmatrix}
P_{11} & ... & P_{1n} \\
\vdots & ... & \vdots \\
P_{n1} & ... & P_{nn}
\end{Bmatrix} %]]></script>

<p>From the above two concept, we can notice two things and these are also the constraint for MDP:</p>
<ul>
  <li>The state is finite (otherwise the definition of State Transition Matrix is problematic)</li>
  <li>The environment is fully observable, no hidden state exists</li>
</ul>

<p>We can obtain a definition for MP as a tuple $&lt;S, P&gt;$:</p>
<ul>
  <li>$S$ is a finite state set</li>
  <li>$P$ is a state transition matrix</li>
</ul>

<p>An example of MP:</p>

<p><img src="/assets/mdp.png" alt="Markov Process" /></p>

<h4 id="markov-reward-process">Markov Reward Process</h4>
<p>Markov Process combined with values, then we have Markov Reward Process (MRP), defined by a tuple $&lt;S, P, R, \gamma&gt;$:</p>
<ul>
  <li>$S$ is a finite state set</li>
  <li>$P$ is a state transition matrix</li>
  <li>$R$ is a reward function,
<script type="math/tex">R_{s} = \mathbb{E}[R_{t+1}|S_{t}=s]</script></li>
  <li>$\gamma$ is a discounting ratio</li>
</ul>

<p>As we have introduced reward, we can measure how many rewards we can get in each state. We define return $G_t$ as the discounted rewards we can get from timestamp $t$, and state value function $v(s)$ the expected return we can get starting from state $s$:</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=1}^{\infty}\gamma^{k}R_{t+k+1}</script>

<script type="math/tex; mode=display">v(s) = \mathbb{E}[G_t|S_t=s]</script>

<p>Note that the return is accumulated with discounting. This is important is:</p>
<ul>
  <li>Avoid infity loop that might exist in MDP (e.g. self loop)</li>
  <li>Model the uncertenity about the future</li>
  <li>From common sense, humman prefer immediate reward than long term ones</li>
</ul>

<p>We can breakdown the state value function into two parts, immediate and long term. Using recurse, we have the Bellman Equation for MRP:</p>

<script type="math/tex; mode=display">v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t=s]</script>

<p>By expanding the above expectation and using sum to replace expectation operator, we have:</p>

<script type="math/tex; mode=display">v(s) = R_s + \gamma\sum_{s^\prime\in S}P_{ss^\prime}v(s^\prime)</script>

<h4 id="markov-decision-process">Markov Decision Process</h4>
<p>Adding the actions we can make among the state, we finally have the defination for MDP, which is $&lt;S, A, P, R, \gamma&gt;$$:</p>
<ul>
  <li>$S$ is a finite state set</li>
  <li>$A$ is a finite action set</li>
  <li>$P$ is a state transition matrix,
<script type="math/tex">P_{ss^\prime}^a = \mathbb{P}[S_{t+1}=s^\prime|S_{t}=s, A_t=a]</script></li>
  <li>$R$ is a reward function,
<script type="math/tex">R_{s}^a = \mathbb{E}[R_{t+1}|S_{t}=s, A_t=a]</script></li>
  <li>$\gamma$ is a discounting ratio</li>
</ul>

<p>Notice the change on the state transition matrix, before we only have a single matrix, and now we have one for each action $a$ (we can think of in the pervious case, we have only single action). Under different action $a$, the transition probability can be different between the two state. We can now regard the new state transition matrix as a tensor with three dimension.</p>

<p>As we have actions to now, we need to make decsion how to take actions. A <strong>policy</strong> is a distribution over actions given a state:</p>

<script type="math/tex; mode=display">\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]</script>

<p>A policy fully determines how an agent would act, and it does not depend on the history. Similar to MRP, we have state value function for MDP as the expected return statring from $s$, following the policy $\pi$:</p>

<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s]</script>

<p>We can also define an action value function, which is the expected return we get starting from state $s$, taking action $a$ and following policy $\pi$:</p>

<script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a]</script>

<p>These two functions have relationship and can be transfromed to each other easily, to get state value function, we can summation over all the action value function for the action that state we could get, weight by the policy:</p>

<script type="math/tex; mode=display">v_\pi(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s, a)</script>

<p>And the action value function can be obtained by summation overall all state we can transation to, weighted by the state transition matrix:</p>

<script type="math/tex; mode=display">q_\pi(s, a) = R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a v(s^\prime)</script>

<p>Comebine the above equations, we can obtain the Bellman Exception Equation as:</p>

<script type="math/tex; mode=display">v_\pi(s) = \sum_{a\in A}\pi(a|s)(R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a v(s^\prime))</script>

<script type="math/tex; mode=display">q_\pi(s, a) = R_s^a + \gamma\sum_{s^\prime}P_{ss^\prime}^a \sum_{a\in A}\pi(a|s^\prime)q_{\pi}(s^\prime, a)</script>

<p>Given a MDP, if we want to solve it (to know what’s the best performance we can get, e.g. What’s the maximum rewards we can get in the terminate state), we need to find the optimal value function for it. As long as we have obtain the optimal value function, we can compose an optimal policy easily:</p>

<script type="math/tex; mode=display">% <![CDATA[
\pi_{*}(a|s) =
\begin{cases}
1,  & \text{if $a = argmax_{a\in A} q(s, a)$} \\
0, & \text{otherwise}
\end{cases} %]]></script>

<blockquote>
  <p>There exists an optimal policy $\pi_{*}$ that is better than or equal to all other policy for any MDP. All optimal policy achieve optimal state value function and optimal action value function</p>
</blockquote>

<p>Following this policy, we can change our Bellman Exception Equation to Bellman Optimality Equation:</p>

<script type="math/tex; mode=display">v_*(s)=max_{a}(R_s^a + \gamma\sum_{s^\prime\in S}P_{ss^\prime}^a v_*(s^\prime))</script>

<script type="math/tex; mode=display">q_*(s, a)=R_s^a + \gamma\sum_{s^\prime\in S} argmax_{a} P_{ss^\prime}^a v_*(s^\prime)</script>

<p>Bellman Optimality Equation is non-linear, there is no closed form solution for it. However, we can solve it by some iterative methods (will introduce in later lectures):</p>
<ul>
  <li>Policy Iteration</li>
  <li>Value Iteration</li>
  <li>Q-learning</li>
  <li>Sarsa</li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">Coding Monkey</h2>

    <div class="footer-col-1 column">
      <ul>
        <li>Yang Pei</li>
        <li><a href="mailto:pyemma1991@gmail.com">pyemma1991@gmail.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/pyemma">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">pyemma</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/pyemma">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">pyemma</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">A coding monkey who wants to become a coding machine</p>
    </div>

  </div>

</footer>

    </body>
</html>
