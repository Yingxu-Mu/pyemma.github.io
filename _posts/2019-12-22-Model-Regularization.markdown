---
layout: single 
title:  "[WIP] Simple Review of Model Regularization Techniques"
tags:
- machine learning
published: false
---

## Why do we need regularization

## Regularization common techniques

* Weight decay (L2 Regularization)

* Dropout

* Batch Normalization

* Mulittask Learning


### Reference
* Droput: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
* Batch Normalization: https://arxiv.org/pdf/1502.03167.pdf
* Batch Normalization Implementation: https://kevinzakka.github.io/2016/09/14/batch_normalization/
* Understanding Batch Normalization: https://papers.nips.cc/paper/7996-understanding-batch-normalization.pdf
* How does Batch Normalization Help Optimization: https://arxiv.org/pdf/1805.11604.pdf  